
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../assets/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.1.11">
    
    
      
        <title>yolov2 - oneflow-yolo-doc</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3754935a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="oneflow-yolo-doc" class="md-header__button md-logo" aria-label="oneflow-yolo-doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            oneflow-yolo-doc
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              yolov2
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../index.html" class="md-tabs__link">
      首页
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../tutorials/01_chapter/yolov5_network_structure_analysis.html" class="md-tabs__link">
        教程
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="01_yolo.html" class="md-tabs__link md-tabs__link--active">
        论文解读
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="oneflow-yolo-doc" class="md-nav__button md-logo" aria-label="oneflow-yolo-doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    oneflow-yolo-doc
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        首页
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        教程
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="教程" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          教程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/01_chapter/yolov5_network_structure_analysis.html" class="md-nav__link">
        YOLOv5 网络结构解析
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/02_chapter/how_to_prepare_yolov5_training_data.html" class="md-nav__link">
        如何准备yolov5模型训练数据
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/04_chapter/mosaic.html" class="md-nav__link">
        数据增强
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/rectangular_reasoning.html" class="md-nav__link">
        矩形推理
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/iou_in-depth_analysis.html" class="md-nav__link">
        IOU深入解析
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/map_analysis.html" class="md-nav__link">
        模型精确度评估
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      <label class="md-nav__link" for="__nav_3">
        论文解读
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="论文解读" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          论文解读
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="01_yolo.html" class="md-nav__link">
        yolov1
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          yolov2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="02_yolo.html" class="md-nav__link md-nav__link--active">
        yolov2
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1.引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2.更好
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3.更快
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4.更强
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5.总结
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="03_yolo.html" class="md-nav__link">
        yolov3
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="04_yolo.html" class="md-nav__link">
        yolov4
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="06_yolo.html" class="md-nav__link">
        yolov6
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    摘要
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1.引言
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2.更好
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3.更快
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4.更强
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    5.总结
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>yolov2</h1>
                
                <p>本文翻译自: https://arxiv.org/pdf/1612.08242.pdf</p>
<p><span class="arithmatex">\(YOLO9000\)</span> :
Better, Faster, Stronger</p>
<p>Joseph Redmon∗†, Ali Farhadi∗†</p>
<p>University of Washington∗, Allen Institute for AI†</p>
<p>http://pjreddie.com/yolo9000/</p>
<h2 id="_1">摘要</h2>
<p>&emsp;我们推出的 <span class="arithmatex">\(YOLO9000\)</span> 是一款先进的实时目标检测系统，可检测9000多种目标类别。首先，我们提出对  <span class="arithmatex">\(YOLO\)</span>  检测方法的各种改进，这些改进有独创的，也有的是来源于以前的研究。改进后的模型  <span class="arithmatex">\(YOLOv2\)</span>  在 <span class="arithmatex">\(PASCAL  VOC\)</span> 和  <span class="arithmatex">\(COCO\)</span> 等标准检测任务中处于技术领先地位。通过使用一种新颖的多尺度训练方法，同样的  <span class="arithmatex">\(YOLOv2\)</span>  模型可以以不同的尺寸运行，在速度和准确性之间提供了一个简单的折衷。在 <span class="arithmatex">\(67 FPS\)</span> 时，  <span class="arithmatex">\(YOLOv2\)</span>  在 <span class="arithmatex">\(VOC \ 2007\)</span> 上获得了76.8 <span class="arithmatex">\(mAP\)</span> 。在 <span class="arithmatex">\(40FPS\)</span> 时，  <span class="arithmatex">\(YOLOv2\)</span>  获得了78.6 <span class="arithmatex">\(mAP\)</span>，超越了采用 <span class="arithmatex">\(ResNet\)</span> 和 <span class="arithmatex">\(SSD的Faster \ R-CNN\)</span> 等先进的方法，同时运行速度仍然更快。最后我们提出一种联合训练目标检测和分类的方法。使用这种方法，我们在 <span class="arithmatex">\(COCO\)</span> 检测数据集和 <span class="arithmatex">\(ImageNet\)</span> 分类数据集上同时训练 <span class="arithmatex">\(YOLO9000\)</span> 。我们的联合训练使 <span class="arithmatex">\(YOLO9000\)</span> 能够预测未标注检测数据的目标类别，并且我们在 <span class="arithmatex">\(ImageNet\)</span> 检测任务上验证了我们的方法。  <span class="arithmatex">\(YOLO9000\)</span> 在 <span class="arithmatex">\(ImageNet\)</span> 检测验证集上获得19.7  <span class="arithmatex">\(mAP\)</span>  ，尽管200个类中只有44个具有检测数据。在 <span class="arithmatex">\(COCO\)</span> 上没有的156种类上， <span class="arithmatex">\(YOLO9000\)</span> 得到 16.0  <span class="arithmatex">\(mAP\)</span>  ，但是 <span class="arithmatex">\(YOLO\)</span> 可以检测超过200个种类;它预测超过9000 多种不同的目标类别，而且它仍然是实时运行的。</p>
<p><img src = "./02_yolo_imgs/图片01.png"></p>
<p>图1: <span class="arithmatex">\(YOLO9000\)</span> 。 <span class="arithmatex">\(YOLO9000\)</span> 可以实时检测各种各样的目标类别。</p>
<h2 id="1">1.引言</h2>
<p>&emsp;通用的目标检测应该快速，准确，并且能够识别各种各样的目标。自从引入神经网络，检测框架变得越来越快速和准确。但是，大多数检测方法仅限于检测一小部分目标。</p>
<p>&emsp;与分类和标记等其他任务的数据集相比，目前目标检测数据集是有限的。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像[3][10][2]。而分类数据集有数以百万计的图像，数十或数百万个类别[20][2]。</p>
<p>&emsp;我们希望检测的类别能够扩展到目标分类的级别。但是，标注检测图像要比标注分类或贴标签要昂贵得多（<em>标签通常是用户免费提供</em>) 。因此，我们不太可能在近期内看到与分类数据集相同规模的检测数据集。</p>
<p>&emsp;我们提出了一种新方法——通过利用我们已有的大量分类数据来扩大当前检测系统的范围。 我们的方法使用目标分类的分层视图，使得我们可以将不同的数据集组合在一起。</p>
<p>&emsp;我们还提出了一种联合训练算法，它允许我们在检测和分类数据上训练目标检测器。 我们的方法利用标记检测图像来学习精确定位目标，同时使用分类图像来增加词汇量和鲁棒性。</p>
<p>&emsp;我们使用这种方法训练 <span class="arithmatex">\(YOLO9000\)</span> b一种可以检测超过9000种不同的目标类别的实时目标检测器。 首先，我们改进YOLO基础检测系统，生成最先进的实时检测器  <span class="arithmatex">\(YOLOv2\)</span>  。 然后，采用我们的数据集组合方法和联合训练算法，使用来自 <span class="arithmatex">\(ImageNet\)</span> 的9000多个类以及 <span class="arithmatex">\(COCO\)</span> 的检测数据来训练模型。</p>
<p>我们所有代码和预训练模型都可在线获得：http://pjreddie.com/yolo9000/。</p>
<h2 id="2">2.更好</h2>
<p>&emsp;与最先进的检测系统相比， <span class="arithmatex">\(YOLO\)</span> 存在各种缺点。 <span class="arithmatex">\(YOLO\)</span> 与 <span class="arithmatex">\(Fast \ R-CNN\)</span> 的误差比较分析表明， <span class="arithmatex">\(YOLO\)</span> 产生了大量的定位错误。此外，与生成候选区域方法相比，<span class="arithmatex">\(YOLO\)</span> 召回率(<em>recall</em>)相对较低。 因此，我们主要关注改善召回率和定位，同时保持分类准确性。</p>
<p>&emsp;计算机视觉通常趋向于更大更深的网络[6] [18] [17]。 更好的性能通常取决于训练更大的网络或将多个模型组合在一起。 但是，对于  <span class="arithmatex">\(YOLOv2\)</span>  ，我们需要一个更精确的检测器，而且保持很快的速度。 我们不是要扩大网络，而是简化网络，然后让表征(<em>即目标特征</em>)更易于学习。 我们将以往工作中的各种创意与我们自己新颖的方法结合起来，以提高 <span class="arithmatex">\(YOLO\)</span> 的性能。 结果汇总见 表2。</p>
<p>&emsp;<strong>批量标准化</strong>（Batch Normalization）。批量标准化可以显着改善收敛性，而且不再需要其他形式的正则化[7]。 通过在 <span class="arithmatex">\(YOLO\)</span> 中的所有卷积层上添加批量标准化，可以在 <span class="arithmatex">\(mAP\)</span>  中获得 2％以上的改进。 批量标准化也有助于规范模型。 通过批量标准化，可以从模型中删除 <span class="arithmatex">\(dropout\)</span> 而不会发生过拟合。</p>
<p>&emsp;<strong>高分辨率分类器</strong> （High Resolution Classifier）。所有的最先进的检测方法都使用在 <span class="arithmatex">\(ImageNet\)</span> 上预先训练好的分类器[16]。 从AlexNet开始，大多数分类器用小于 <span class="arithmatex">\(256×256\)</span> 的图像作为输入[8]。 最初的 <span class="arithmatex">\(YOLO\)</span> 以 <span class="arithmatex">\(224×224\)</span> 的图像训练分类器网络，并将分辨率提高到 <span class="arithmatex">\(448\)</span> 以进行检测训练。 这意味着网络必须切换到目标检测的学习，同时能调整到新的输入分辨率。</p>
<p>&emsp;对于  <span class="arithmatex">\(YOLOv2\)</span>  ，我们首先以 <span class="arithmatex">\(448×448\)</span> 的全分辨率在 <span class="arithmatex">\(ImageNet\)</span> 上进行 <span class="arithmatex">\(10\)</span> 个迭代周期的微调。这给予网络一些时间，以调整其滤波器来更好地处理更高分辨率的输入。然后，我们再对该检测网络进行微调。 这个高分辨率的分类网络使 <span class="arithmatex">\(mAP\)</span>  增加了近4％。</p>
<p>&emsp; <strong>卷积与锚框</strong>。 <span class="arithmatex">\(YOLO\)</span> 直接使用卷积特征提取器顶部的全连接层来预测边界框的坐标。而 <span class="arithmatex">\(Fast \ R-CNN\)</span> 不是直接预测坐标，是使用手工选取的先验来预测边界框[15]。而不是直接预测坐标 <span class="arithmatex">\(Fast \ R-CNN\)</span> 预测边界框使用手工挑选的先验区域[15]。 Faster R-CNN中的候选区域生成网络（RPN）仅使用卷积层来预测锚框的偏移和置信度。由于预测层是卷积的，所以RPN可以在特征图中的每个位置预测这些偏移。使用预测偏移代替坐标，可以简化问题并使网络更易于学习。</p>
<p>&emsp;我们从 <span class="arithmatex">\(YOLO\)</span> 中移除全连接层，并使用锚框来预测边界框。 首先我们消除一个池化层，以使网络卷积层的输出具有更高的分辨率。 我们还缩小网络，使其在分辨率为 <span class="arithmatex">\(416X416\)</span> 的输入图像上运行，而不是 <span class="arithmatex">\(448×448\)</span> 。我们这样做是因为我们想要在特征图中有奇数个位置，从而有一个单一的中心单元格。目标，尤其是大的目标，往往占据图像的中心，所以最好在正中心拥有单独一个位置来预测这些目标，而不是在中心附近的四个位置。 <span class="arithmatex">\(YOLO\)</span> 的卷积层对图像进行了 <span class="arithmatex">\(32\)</span> 倍的采样，所以通过使用 <span class="arithmatex">\(416\)</span> 的输入图像，我们得到 <span class="arithmatex">\(13×13\)</span> 的输出特征图。</p>
<p>&emsp;当我们移动到锚框时，我们将类预测机制与空间位置分开处理，单独预测每个锚框的类及其目标。 遵循原来的 <span class="arithmatex">\(YOLO\)</span> 的做法，目标预测依然预测了真实标签框（ground truth box）和候选框的 <span class="arithmatex">\(IOU\)</span> ，而类别预测也是预测了当有目标存在时，该类别的条件概率。</p>
<p>&emsp;使用锚框，精度值会小幅下降。因为原始的 <span class="arithmatex">\(YOLO\)</span> 仅为每个图片预测98个框，但使用锚框后，我们的模型预测的框数超过 1000 个。 在没有锚框情况下，我们的中等模型将获得 69.5 的 <span class="arithmatex">\(mAP\)</span>  ，召回率为81％。 使用锚框后，我们的模型获得了<span class="arithmatex">\(69.2\)</span> 的 <span class="arithmatex">\(mAP\)</span>  ，召回率为88％。尽管 <span class="arithmatex">\(mAP\)</span>  减少，但召回率的增加意味着我们的模型有更大的改进空间。</p>
<p><img src = "./02_yolo_imgs/图片02.png"></p>
<p>图2：<span class="arithmatex">\(VOC\)</span> 和 $ <span class="arithmatex">\(COCO\)</span> $ 上的聚类框尺寸。我们在边界框的维上运行 <span class="arithmatex">\(k-means\)</span> 聚类，以获得我们模型的良好先验。左图显示了我们通过k的各种选择获得的平均 <span class="arithmatex">\(IOU\)</span> 。我们发现 <span class="arithmatex">\(k = 5\)</span> 为召回与模型的复杂性提供了良好的折中。右图显示了 <span class="arithmatex">\(VOC\)</span> 和 <span class="arithmatex">\(COCO\)</span> 的相对质心。这两种方案都喜欢更薄，更高的框，并且 <span class="arithmatex">\(COCO\)</span> 的尺寸的变化比 <span class="arithmatex">\(VOC\)</span> 更大。</p>
<p><span class="arithmatex">\(k-means\)</span>算法: <span class="arithmatex">\(K-means\)</span> 算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。</p>
<p>&emsp;<strong>维度聚类</strong>（Dimension Clusters）。当把锚框与YOLO一起使用时，我们会遇到两个问题。 首先是框的尺寸是手工挑选的。虽然网络可以通过学习适当地调整方框，但是如果我们从一开始就为网络选择更好的先验框，就可以让网络更容易学习到更好的检测结果。</p>
<p>&emsp;我们不用手工选择先验框，而是在训练集的边界框上运行k-means聚类，自动找到良好的先验框。 如果我们使用具有欧几里得距离的标准 <span class="arithmatex">\(k-means\)</span> ，那么较大的框比较小的框产生更多的误差。 然而，我们真正想要的是独立于框的大小的，能获得良好的 <span class="arithmatex">\(IOU\)</span> 分数的先验框。 因此对于距离度量我们使用:</p>
<p><center></p>
<p><span class="arithmatex">\(d(\text { box, centroid }) = 1-\operatorname{IOU}(\text { box }, \text { centroid })\)</span></p>
<p></center></p>
<p>&emsp;我们用不同的 <span class="arithmatex">\(k\)</span> 值运行 <span class="arithmatex">\(k-means\)</span> ，并绘制最接近质心的平均 <span class="arithmatex">\(IOU\)</span>（见图2）。为了在模型复杂度和高召回率之间的良好折衷，我们选择 <span class="arithmatex">\(k = 5\)</span>。聚类的质心与手工选取的锚框显着不同，它有更少的短且宽的框，而且有更多既长又窄的框。</p>
<p>&emsp;表1中，我们将聚类策略的先验框中心数和手工选取的锚框数在最接近的平均 <span class="arithmatex">\(IOU\)</span> 上进行比较。仅5个先验框中心的平均 <span class="arithmatex">\(IOU\)</span> 为61.0，其性能类似于9个锚框的60.9。 使用9个质心会得到更高的平均 <span class="arithmatex">\(IOU\)</span> 。这表明使用 <span class="arithmatex">\(k-means\)</span> 生成边界框可以更好地表示模型并使其更容易学习。</p>
<p><span class="arithmatex">\(\begin{array}{lcc}
\text { Box Generation } &amp; \# &amp; \text { Avg IOU } \\
\hline \text { Cluster SSE } &amp; 5 &amp; 58.7 \\
\text { Cluster IOU } &amp; 5 &amp; 61.0 \\
\text { Anchor Boxes [15] } &amp; 9 &amp; 60.9 \\
\text { Cluster IOU } &amp; 9 &amp; 67.2
\end{array}\)</span></p>
<p>表1： <span class="arithmatex">\(VOC \  2007\)</span> 最接近先验的框的平均 <span class="arithmatex">\(IOU\)</span>。 <span class="arithmatex">\(VOC \  2007\)</span> 上的目标的平均IOU与其最接近的，未经修改的使用不同生成方法的目标之间的平均 <span class="arithmatex">\(IOU\)</span> 。聚类得结果比使用手工选取的先验框结果要好得多。</p>
<p>&emsp; <strong>直接位置预测</strong>（Direct location prediction）。当在 <span class="arithmatex">\(YOLO\)</span> 中使用锚框时，我们会遇到第二个问题：模型不稳定，尤其是在早期迭代的过程中。 大多数不稳定来自于预测框的 <span class="arithmatex">\((x,y)\)</span> 位置。 在候选区域网络中，网络预测的 <span class="arithmatex">\(t_x,t_y\)</span>  ，和中心坐标 <span class="arithmatex">\((x,y)\)</span> 计算如下：</p>
<p><center></p>
<p><span class="arithmatex">\(\huge\begin{array}{l}
x=\left(t_{x} * w_{a}\right)-x_{a} \\
y=\left(t_{y} * h_{a}\right)-y_{a}
\end{array}\)</span></p>
<p></center></p>
<p>&emsp;例如，预测 <span class="arithmatex">\(t_x = 1\)</span> 会使该框向右移动锚框的宽度，而预测 <span class="arithmatex">\(t_x = -1\)</span> 会将其向左移动相同的宽度。</p>
<p>&emsp;这个公式是不受约束的，所以任何锚框都可以在图像中的任何一点结束，而不管锚框是在哪个位置预测的。随机初始化模型需要很长时间才能稳定到预测合理的偏移量。</p>
<p>&emsp;我们没有预测偏移，而是遵循 <span class="arithmatex">\(YOLO\)</span> 的方法，预测相对于网格单元位置的位置坐标。这使得真实值的界限在0到1之间。我们使用逻辑激活来限制网络的预测落在这个范围内。</p>
<p>&emsp;网络为特征图的输出的每个单元预测5个边界框。网络预测每个边界框的5个坐标<span class="arithmatex">\(t_x,t_y,t_w,t_h和t_o\)</span> 。如果单元格从图像的左上角偏移了,并且之前的边界框具有宽度和高度 <span class="arithmatex">\(p_w,p_h\)</span> 则预测对应于：</p>
<p><center></p>
<p><span class="arithmatex">\(\begin{aligned}
b_{x} &amp;=\sigma\left(t_{x}\right)+c_{x} \\
b_{y} &amp;=\sigma\left(t_{y}\right)+c_{y} \\
b_{w} &amp;=p_{w} e^{t_{w}} \\
b_{h} &amp;=p_{h} e^{t_{h}} \\
\operatorname{Pr}(\text { object }) * \operatorname{IOU}(b, \text { object }) &amp;=\sigma\left(t_{o}\right)
\end{aligned}\)</span></p>
<p></center></p>
<p>&emsp;由于我们限制了位置预测，使得参数化更容易学习，从而使网络更加稳定。使用维度集群以及直接预测边界框中心位置，可以使 <span class="arithmatex">\(YOLO\)</span> 比锚框的版本提高近5％。</p>
<p><img src = "./02_yolo_imgs/图片03.png"></p>
<p>图3：具有维度先验和位置预测的边界框。我们预测框的宽度和高度作为聚类质心的偏移量。我们使用sigmoid函数预测相对于滤波器应用位置的框的中心坐标。</p>
<p>&emsp;<strong>细粒度功能</strong>（Fine-Grained Features）。修改后的YOLO在 <span class="arithmatex">\(13×13\)</span> 特征图上预测检测结果。 虽然这对于大型物体是足够的，但使用更细粒度特征对定位较小物体有好处。Faster R-CNN和SSD都在网络中的各种特征图上运行网络，以获得多个分辨率。 我们采取不同的方法，只需添加一个直通层，以 <span class="arithmatex">\(26×26\)</span> 的分辨率从较早的层中提取特征。</p>
<p>&emsp;直通层将高分辨率特征与低分辨率特征连接起来，将相邻特征叠加到不同的通道中，而不是空间位置上，类似于 <span class="arithmatex">\(ResNet\)</span> 中的恒等映射。将 <span class="arithmatex">\(26×26×512\)</span> 的特征图变为 <span class="arithmatex">\(13×13×2048\)</span> 的特征图，然后就可以与原来的特征连接。我们的检测器运行在这张扩展的特征图的顶部，以便它可以访问细粒度的功能。这使性能提高了1％。</p>
<p>&emsp;<strong>多尺度训练</strong>（Multi-Scale Training）。原来的 <span class="arithmatex">\(YOLO\)</span> 使用 <span class="arithmatex">\(448×448\)</span> 的输入分辨率。通过添加锚框，我们将分辨率更改为 <span class="arithmatex">\(416×416\)</span> 。但是，由于我们的模型仅使用卷积层和池化层，因此可以实时调整大小。我们希望  <span class="arithmatex">\(YOLOv2\)</span>  能够在不同尺寸的图像上运行，因此我们可以将多尺度训练应到模型中。</p>
<p>&emsp;我们不需要修改输入图像大小，而是每隔几次迭代就改变一次网络。每 <span class="arithmatex">\(10\)</span> 个批次我们的网络会随机选择一个新的图像尺寸大小。由于我们的模型缩减了 <span class="arithmatex">\(32\)</span> 倍，所以我们从 <span class="arithmatex">\(32\)</span> 的倍数中抽取：<span class="arithmatex">\({320,352，…，608}\)</span> 。因此，最小的选项是 <span class="arithmatex">\(320×320\)</span> ，最大的是 <span class="arithmatex">\(608×608\)</span> 。我们调整网络的尺寸到那个维度并继续训练。</p>
<p>&emsp;这个机制迫使网络学习如何在各种输入维度上做好预测。这意味着同一个网络可以预测不同分辨率下的检测结果。网络在较小的尺寸下运行速度更快，因此  <span class="arithmatex">\(YOLOv2\)</span>  在速度和准确性之间提供了一个轻松的折中。</p>
<p>&emsp; 在低分辨率下，  <span class="arithmatex">\(YOLOv2\)</span>  作为一种便宜但相当准确的检测器工作。 在 <span class="arithmatex">\(288×288\)</span> 情况下，它的运行速度超过 90 FPS，而 <span class="arithmatex">\(mAP\)</span>  几乎与 <span class="arithmatex">\(Fast \ R-CNN\)</span> 一样好。这使其成为小型 <span class="arithmatex">\(GPU\)</span> ，高帧率视频或多视频流的理想选择。</p>
<p>&emsp;在高分辨率下，  <span class="arithmatex">\(YOLOv2\)</span>  是一款先进的检测器，在VOC2007上获得了78.6的 <span class="arithmatex">\(mAP\)</span>  ，同时仍以高于实时速度运行。请参阅表3，了解  <span class="arithmatex">\(YOLOv2\)</span>  与其他框架在 <span class="arithmatex">\(VOC \  2007\)</span> 上的比较 图4。</p>
<p><img src ="./02_yolo_imgs/图片04.png"></p>
<p>图4： <span class="arithmatex">\(VOC \  2007\)</span> 上的精度和速度</p>
<p>&emsp;<strong>进一步的实验</strong>（Further Experiments）。 我们在 <span class="arithmatex">\(VOC \ 2012\)</span> 上训练了 <span class="arithmatex">\(YOLOv2\)</span>  进行检测。表4 显示了  <span class="arithmatex">\(YOLOv2\)</span>  与其他最先进的检测系统的性能比较。   <span class="arithmatex">\(YOLOv2\)</span>  运行速度远高于对手，且精度达到 73.4 <span class="arithmatex">\(mAP\)</span>  。 我们还在 <span class="arithmatex">\(COCO\)</span> 上训练，并与表5中的其他方法进行比较。使用 <span class="arithmatex">\(VOC\)</span> 度量（ <span class="arithmatex">\(IOU = 0.5\)</span> ），  <span class="arithmatex">\(YOLOv2\)</span>  获得44.0  <span class="arithmatex">\(mAP\)</span>  ，与 <span class="arithmatex">\(SSD\)</span> 和 <span class="arithmatex">\(Faster \ R-CNN\)</span> 相当。</p>
<p><center></p>
<p><span class="arithmatex">\(\begin{array}{lrrr}
\text { Detection Frameworks } &amp; \text { Train } &amp; \text { mAP } &amp; \text { FPS } \\
\hline \text { Fast R-CNN [5] } &amp; 2007+2012 &amp; 70.0 &amp; 0.5 \\
\text { Faster R-CNN VGG-16[15] } &amp; 2007+2012 &amp; 73.2 &amp; 7 \\
\text { Faster R-CNN ResNet[6] } &amp; 2007+2012 &amp; 76.4 &amp; 5 \\
\text { YOLO [14] } &amp; 2007+2012 &amp; 63.4 &amp; 45 \\
\text { SSD300 [11] } &amp; 2007+2012 &amp; 74.3 &amp; 46 \\
\text { SSD500 [11] } &amp; 2007+2012 &amp; 76.8 &amp; 19 \\
\hline \text { YOLOv2 288 } \times 288 &amp; 2007+2012 &amp; 69.0 &amp; 91 \\
\text { YOLOv2 352 } \times 352 &amp; 2007+2012 &amp; 73.7 &amp; 81 \\
\text { YOLOv2 416 } \times 416 &amp; 2007+2012 &amp; 76.8 &amp; 67 \\
\text { YOLOv2 480 } \times 480 &amp; 2007+2012 &amp; 77.8 &amp; 59 \\
\text { YOLOv2 } 544 \times 544 &amp; 2007+2012 &amp; \mathbf{7 8 . 6} &amp; 40
\end{array}\)</span></p>
<p>表3： <span class="arithmatex">\(PA S C A L  \ VOC \ 2007\)</span> 的检测框架。  <span class="arithmatex">\(YOLOv2\)</span>  比以前的检测方法更快，更准确。它也可以以不同的分辨率运行，以便在速度和准确性之间轻松折衷。每个  <span class="arithmatex">\(YOLOv2\)</span>  项实际上都是具有相同权重的相同训练模型，只是以不同的大小进行评估。所有的时间的测试都运行在Geforce GTX Titan X（原始的，而不是Pascal模型）</p>
<p></center></p>
<h2 id="3">3.更快</h2>
<p>&emsp;我们希望检测结果准确，但我们也希望检测速度更快。 大多数用于检测的应用程序（如机器人或自动驾驶汽车）都依赖于低延迟预测。 为了最大限度地提高性能，我们将  <span class="arithmatex">\(YOLOv2\)</span>   设计从头到尾都非常快 。</p>
<p><center></p>
<p><img src = "02_yolo_imgs/table03.png"></p>
<p>表2：从 <span class="arithmatex">\(YOLO\)</span> 到  <span class="arithmatex">\(YOLOv2\)</span>  的路径。大多数列出的设计决策都会导致 <span class="arithmatex">\(MAP\)</span>  显着增加。有两个例外情况是：切换到带有锚框的全卷积网络和使用新网络。切换到锚框方法增加召回率，而不改变 <span class="arithmatex">\(mAP\)</span>  ，而使用新网络削减33％的计算。
</center></p>
<p><center>
<img src = "02_yolo_imgs/table04.png">
表4：PASCAL VOC2012测试检测结果。  <span class="arithmatex">\(YOLOv2\)</span>  与采用ResNet和SSD512的Faster R-CNN等先进检测器性能相当，速度提高2至10倍。
</center></p>
<p>&emsp;大多数检测框架依赖于VGG-16作为基本特征提取器[17]。 <span class="arithmatex">\(VGG-16\)</span> 是一个功能强大，准确的分类网络，但它有不必要的复杂度。 <span class="arithmatex">\(VGG-16\)</span> 的卷积层在一个 <span class="arithmatex">\(224×224\)</span> 分辨率单个图像上运行一次需要 <span class="arithmatex">\(306.90\)</span> 亿浮点运算。</p>
<p>&emsp; <span class="arithmatex">\(YOLO\)</span> 框架使用基于 <span class="arithmatex">\(Googlenet\)</span> 架构的自定义网络[19]。这个网络比 <span class="arithmatex">\(VGG-16\)</span> 更快，一次前向传播只要 <span class="arithmatex">\(85.2\)</span> 亿次运行。然而，它的准确性略低于 <span class="arithmatex">\(VGG-16\)</span>。在 <span class="arithmatex">\(ImageNet\)</span> 上，用 <span class="arithmatex">\(224×224\)</span> 的单张裁剪图像， <span class="arithmatex">\(YOLO\)</span> 的自定义模型的精度为88.0％而 <span class="arithmatex">\(VGG-16\)</span> 则为90.0％。</p>
<p>&emsp; <span class="arithmatex">\(Darknet-19\)</span> 。我们提出了一个新的分类模型作为  <span class="arithmatex">\(YOLOv2\)</span>  的基础。我们的模型建立在网络设计的先前工作以及该领域的常识上。与 <span class="arithmatex">\(VGG\)</span> 模型类似，我们大多使用 <span class="arithmatex">\(3×3\)</span> 滤波器，并且在池化层步骤后使用两倍的通道数[17]。按照Network in Network（NIN）的方法，我们使用全局平均池化来做预测，并使用1×1滤波器来压缩3×3卷积的特征表示[9]。我们使用批量归一化来稳定训练，加速收敛，并规范模型[7]。</p>
<p>&emsp;最终的模型叫做Darknet-19，它有19个卷积层和5个Maxpool层。 <span class="arithmatex">\(Darknet-19\)</span> 只需要55.8亿次操作来处理图像，但在 <span class="arithmatex">\(ImageNet\)</span> 上实现了72.9％的top-1精度和91.2％的top-5精度。</p>
<p>&emsp; <strong>分类训练</strong>（Training for classification）。我们使用 <span class="arithmatex">\(DarkNet\)</span> 神经网络框架，使用随机梯度下降，初始学习率为0.1，多项式速率衰减为4，权重衰减为0.0005，动量为0.9，在标准 <span class="arithmatex">\(ImageNet\)</span>  1000类别分类数据集上对网络进行160个迭代周期的训练[13]。在训练过程中，我们使用标准数据增强技巧，包括随机截取，旋转和改变色相，饱和度和曝光。</p>
<p>&emsp; 如上所述，在我们对 <span class="arithmatex">\(224×224\)</span> 图像进行初始训练之后，我们用更大的分辨率448对网络进行了微调。微调时，我们使用上述参数进行训练，但仅用10个周期，并且开始时的学习率为10-3。在这个更高的分辨率下，我们的网络实现了76.5％的 <span class="arithmatex">\(top-1\)</span> 精度和93.3％的 <span class="arithmatex">\(top-5\)</span> 精度。</p>
<p>&emsp;<strong>检测训练</strong>（Training for detection）。我们这样修改网络：去除最后一个卷积层，取而代之的是添加三个 <span class="arithmatex">\(3 × 3\)</span> 的卷积层，每个层有 <span class="arithmatex">\(1024\)</span> 个过滤器，然后在最后添加 <span class="arithmatex">\(1×1\)</span> 卷积层，该层的滤波器数量是检测需要的输出数量。 对于 <span class="arithmatex">\(VOC\)</span> ，我们预测 <span class="arithmatex">\(05\)</span> 个边界框，每个边界框有 5个坐标和20个类别，所以有125个滤波器。我们还添加了从最后的 <span class="arithmatex">\(3×3×512\)</span> 层到倒数第二层卷积层的直通层，以便我们的模型可以使用细粒度特征。</p>
<p>&emsp;我们训练网络160个迭代周期，初始学习率为 <span class="arithmatex">\(10^{-3}\)</span>，在60和90周期同时除以10。我们使用 <span class="arithmatex">\(0.0005\)</span> 的权值衰减和 0.9 的动量(<em>momentum</em>)。我们对 <span class="arithmatex">\(YOLO\)</span> 和 <span class="arithmatex">\(SSD\)</span> 进行类似的数据增强，随机裁剪，色彩修改等。我们在 <span class="arithmatex">\(COCO\)</span> 和 <span class="arithmatex">\(VOC\)</span> 使用相同的训练策略。</p>
<p><center></p>
<p><span class="arithmatex">\(\begin{array}{l|c|cccc|ccc|ccc|ccc} 
&amp; &amp; 0.5: 0.95 &amp; 0.5 &amp; 0.75 &amp; \mathrm{~S} &amp; \mathrm{M} &amp; \mathrm{L} &amp; 1 &amp; 10 &amp; 100 &amp; \mathrm{~S} &amp; \mathrm{M} &amp; \mathrm{L} \\
\hline \text { Fast R-CNN [5] } &amp; \text { train } &amp; 19.7 &amp; 35.9 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - \\
\text { Fast R-CNN[1] } &amp; \text { train } &amp; 20.5 &amp; 39.9 &amp; 19.4 &amp; 4.1 &amp; 20.0 &amp; 35.8 &amp; 21.3 &amp; 29.5 &amp; 30.1 &amp; 7.3 &amp; 32.1 &amp; 52 .0 \\
\text { Faster R-CNN[15] } &amp; \text { trainval } &amp; 21.9 &amp; 42.7 &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - &amp; - \\
\text { ION [1] } &amp; \text { train } &amp; 23.6 &amp; 43.2 &amp; 23.6 &amp; 6.4 &amp; 24.1 &amp; 38.3 &amp; 23.2 &amp; 32.7 &amp; 33.5 &amp; 10.1 &amp; 37.7 &amp; 53.6 \\
\text { Faster R-CNN[10] } &amp; \text { trainval } &amp; 24.2 &amp; 45.3 &amp; 23.5 &amp; 7.7 &amp; 26.4 &amp; 37.1 &amp; 23.8 &amp; 34.0 &amp; 34.6 &amp; 12.0 &amp; 38.5 &amp; 54.4 \\
\text { SSD300 [11] } &amp; \text { trainval35k } &amp; 23.2 &amp; 41.2 &amp; 23.4 &amp; 5.3 &amp; 23.2 &amp; 39.6 &amp; 22.5 &amp; 33.2 &amp; 35.3 &amp; 9.6 &amp; 37.6 &amp; 56.5 \\
\text { SSD512 [11] } &amp; \text { trainval35k } &amp; \mathbf{26.8} &amp; \mathbf{4 6 . 5} &amp; \mathbf{2 7 . 8} &amp; \mathbf{9 . 0} &amp; \mathbf{2 8 . 9} &amp; 41.9 &amp; \mathbf{2 4 . 8} &amp; 37.5 &amp; \mathbf{3 9 . 8} &amp; \mathbf{1 4 . 0} &amp; 43.5 &amp; 59.0 \\
\hline \text { YOLOv2 [11] } &amp; \text { trainval35k } &amp; 21.6 &amp; 44.0 &amp; 19.2 &amp; 5.0 &amp; 22.4 &amp; 35.5 &amp; 20.7 &amp; 31.6 &amp; 33.3 &amp; 9.8 &amp; 36.5 &amp; 54 .4
\end{array}\)</span></p>
<p>表5： <span class="arithmatex">\(COCO\)</span>  test-dev集上的结果，来源于论文[11]
</center></p>
<h2 id="4">4.更强</h2>
<p>&emsp;我们提出了一个联合训练分类和检测数据的机制。 我们的方法使用了用于检测的图像来学习检测特定信息，如边界框坐标预测和目标以及如何对常见目标进行分类。通过使用仅具有类标签的图像来扩展其可检测类别的数量。</p>
<p>&emsp;在训练期间，我们混合来自检测和分类数据集的图像。 当我们的网络看到标记为检测的图像时，可以根据完整的  <span class="arithmatex">\(YOLOv2\)</span>  损失函数进行反向传播。 当它看到分类图像时，只会反向传播分类部分的损失。</p>
<p><center></p>
<p><span class="arithmatex">\(\begin{array}{l|c|c|c}
\text { Type } &amp; \text { Filters } &amp; \text { Size/Stride } &amp; \text { Output } \\
\hline \text { Convolutional } &amp; 32 &amp; 3 \times 3 &amp; 224 \times 224 \\
\text { Maxpool } &amp; &amp; 2 \times 2 / 2 &amp; 112 \times 112 \\
\text { Convolutional } &amp; 64 &amp; 3 \times 3 &amp; 112 \times 112 \\
\text { Maxpool } &amp; &amp; 2 \times 2 / 2 &amp; 56 \times 56 \\
\text { Convolutional } &amp; 128 &amp; 3 \times 3 &amp; 56 \times 56 \\
\text { Convolutional } &amp; 64 &amp; 1 \times 1 &amp; 56 \times 56 \\
\text { Convolutional } &amp; 128 &amp; 3 \times 3 &amp; 56 \times 56 \\
\text { Maxpool } &amp; &amp; 2 \times 2 / 2 &amp; 28 \times 28 \\
\text { Convolutional } &amp; 256 &amp; 3 \times 3 &amp; 28 \times 28 \\
\text { Convolutional } &amp; 128 &amp; 1 \times 1 &amp; 28 \times 28 \\
\text { Convolutional } &amp; 256 &amp; 3 \times 3 &amp; 28 \times 28 \\
\text { Maxpool } &amp; &amp; 2 \times 2 / 2 &amp; 14 \times 14 \\
\text { Convolutional } &amp; 512 &amp; 3 \times 3 &amp; 14 \times 14 \\
\text { Convolutional } &amp; 256 &amp; 1 \times 1 &amp; 14 \times 14 \\
\text { Convolutional } &amp; 512 &amp; 3 \times 3 &amp; 14 \times 14 \\
\text { Convolutional } &amp; 256 &amp; 1 \times 1 &amp; 14 \times 14 \\
\text { Convolutional } &amp; 512 &amp; 3 \times 3 &amp; 14 \times 14 \\
\text { Maxpool } &amp; &amp; 2 \times 2 / 2 &amp; 7 \times 7 \\
\text { Convolutional } &amp; 1024 &amp; 3 \times 3 &amp; 7 \times 7 \\
\text { Convolutional } &amp; 512 &amp; 1 \times 1 &amp; 7 \times 7 \\
\text { Convolutional } &amp; 1024 &amp; 3 \times 3 &amp; 7 \times 7 \\
\text { Convolutional } &amp; 512 &amp; 1 \times 1 &amp; 7 \times 7 \\
\text { Convolutional } &amp; 1024 &amp; 3 \times 3 &amp; 7 \times 7 \\
\hline \hline \text { Convolutional } &amp; 1000 &amp; 1 \times 1 &amp; 7 \times 7 \\
\text { Avgpool } &amp; &amp; \text { Global } &amp; 1000 \\
\text { Softmax } &amp; &amp; &amp;
\end{array}\)</span></p>
<p>表6：Darknet-19
</center></p>
<p>&emsp;这种方法带来了一些难题。检测数据集只有常用的目标和通用的标签，如“狗”或“船”。分类数据集具有更广泛和更深入的标签范围。  <span class="arithmatex">\(ImageNet\)</span> 拥有多种犬种，包括Norfolk terrier，Yorkshire terrier和Bedlington terrier。如果我们想在两个数据集上进行训练，则需要采用一致的方式来合并这些标签。</p>
<p>&emsp;大多数分类方法使用涵盖所有可能类别的 <span class="arithmatex">\(softmax\)</span> 层来计算最终概率分布。使用 <span class="arithmatex">\(softmax\)</span> ，意味着类是相互排斥的。这给组合数据集带来了问题，例如，你不能用这个模型来组合 <span class="arithmatex">\(ImageNet\)</span> 和 <span class="arithmatex">\(COCO\)</span> ，因为类别 <span class="arithmatex">\(Norfolk \ terrier和dog\)</span> 不是互斥的。</p>
<p>&emsp;相反，我们可以使用多标签模型来组合不会互相排斥的数据集。这个方法忽略了我们所知道的关于数据的所有结构，例如所有的 <span class="arithmatex">\(COCO\)</span> 类都是相互独立的。</p>
<p>&emsp;<strong>分层分类</strong> （Hierarchical classification）。  <span class="arithmatex">\(ImageNet\)</span> 标签是从 <span class="arithmatex">\(WordNet\)</span> 中提取的，<span class="arithmatex">\(WordNet\)</span> 是一个构建概念及其相互关系的语言数据库[12]。 Norfolk terrier和Yorkshire terrier都是terrier的下义词，terrier是一种hunting dog，hunting dog是dog，dog是canine等。大多数分类的方法假设标签是一个扁平结构，但是对于组合数据集，结构正是我们所需要的。</p>
<p>&emsp; <span class="arithmatex">\(WordNet\)</span> 的结构是有向图，而不是树，因为语言很复杂。 例如，“狗”既是一种“犬”又是一种“家养动物”，它们都是 <span class="arithmatex">\(WordNet\)</span> 中的同义词。 我们不使用完整的图结构，而是通过从 <span class="arithmatex">\(ImageNet\)</span> 中的概念构建分层树来简化问题。</p>
<p>&emsp;WordNet的结构是有向图，而不是树，因为语言很复杂。例如，一只狗既是一种犬科动物，又是一种家养动物，它们都是WordNet中的同种动物。我们没有使用完整的图结构，而是通过从 <span class="arithmatex">\(ImageNet\)</span> 中的概念构建分层树来简化问题。</p>
<p>&emsp;为了构建这棵树，我们检查 <span class="arithmatex">\(ImageNet\)</span> 中的视觉名词，并查看它们通过 <span class="arithmatex">\(WordNet\)</span> 图到根节点的路径，在这种情况下是“物理目标”。 许多同义词只有在图上一条路径，所以首先我们将所有这些路径添加到我们的树中。 然后，我们反复检查我们留下的概念，并尽可能少地添加生成树的路径。 所以如果一个概念有两条通向根的路径，一条路径会为我们的树增加三条边，另一条路只增加一条边，我们选择较短的路径。</p>
<p>&emsp; 最终的结果是 <span class="arithmatex">\(WordTree\)</span> ，一个视觉概念的分层模型。为了使用  <span class="arithmatex">\(WordTree\)</span>  进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。例如，在terrier节点我们预测：</p>
<p><center></p>
<p><span class="arithmatex">\(\large\operatorname{Pr}  (Norfolk \ terrier|terrier) \\
 \operatorname{Pr}  (Yorkshire \ terrier|terrier) \\
 \operatorname{Pr}(  Bedlington \ terrier|terrier  )\)</span>
</center></p>
<p>&emsp;如果我们想要计算一个特定节点的绝对概率，我们只需沿着通过树到达根节点的路径，再乘以条件概率。所以如果我们想知道一张图片是否是Norfolk terrier，我们计算：</p>
<p><center></p>
<p><span class="arithmatex">\(\operatorname{Pr}(\text { Norfolk terrier }) = \operatorname{Pr}(\text { Norfolk terrier } \mid
\text { terrier })\)</span></p>
<p><span class="arithmatex">\(\quad * \operatorname{Pr}(\text { terrier } \mid \text { hunting dog })\)</span></p>
<p><span class="arithmatex">\(* \ldots * \\\)</span></p>
<p><span class="arithmatex">\(* \operatorname{Pr}(\text { mammal } \mid \operatorname{Pr}(\text { animal })\)</span></p>
<p><span class="arithmatex">\(* \operatorname{Pr}(\text { animal } \mid \text { physical object })\)</span></p>
<p></center></p>
<p>&emsp;为了实现分类，我们假定图像包含一个目标：<span class="arithmatex">\(P r(physical object) = 1\)</span>。</p>
<p>&emsp;为了验证这种方法，我们在使用1000类 <span class="arithmatex">\(ImageNet\)</span> 构建的 <span class="arithmatex">\(WordTree\)</span>  上训练 <span class="arithmatex">\(Darknet-19\)</span> 模型。 为了构建 <span class="arithmatex">\(WordTree1k\)</span> ，我们添加了所有中间节点，将标签空间从 <span class="arithmatex">\(1000\)</span> 扩展到 <span class="arithmatex">\(1369\)</span>。在训练过程中，我我们将真实标签向树上面传播，以便如果图像被标记为Norfolk terrier，则它也被标记为dog和mamal等。为了计算条件概率，我们的模型预测了1369个值的向量，并且我们计算了相同概念的下义词在所有同义词集上的softmax，见图5。</p>
<p>&emsp;使用与以前相同的训练参数，我们的分层Darknet-19达到了71.9％的 <span class="arithmatex">\(top-1\)</span> 精度和90.4％的 <span class="arithmatex">\(top-5\)</span> 精度。 尽管增加了369个附加概念，并且让我们的网络预测了树状结构，但我们的精度仅略有下降。 以这种方式进行分类也有若干好处。 在新的或未知的目标类别上，性能会优雅低降低。 例如，如果网络看到一张狗的照片，但不确定它是什么类型的狗，它仍然会以高置信度预测“dog”，只是在下义词会有较低的置信度。</p>
<p>&emsp;该方法也适用于检测。现在，我们不用假定每个图像都有一个目标物体，而是使用  <span class="arithmatex">\(YOLOv2\)</span>  的目标预测器给出P r（目标物体）的值。检测器预测边界框和概率树。我们遍历树，在每次分割中选取具有最高的置信度的路径，直到达到某个阈值，然后我们得到该目标的类别。</p>
<p>&emsp; <strong>数据集与 <span class="arithmatex">\(WordTree\)</span>  的组合</strong> (Dataset combination with WordTree)。我们可以使用 <span class="arithmatex">\(WordTree\)</span>  以可行的方式将多个数据集组合在一起。我们只需将数据集中的类别映射到树中的synsets即可。图6显示了一个使用 <span class="arithmatex">\(WordTree\)</span>  组合来自 <span class="arithmatex">\(ImageNet\)</span> 和 <span class="arithmatex">\(COCO\)</span> 的标签的示例。 WordNet非常多样化，因此我们可以将这种技术用于大多数数据集。</p>
<p><img src = "02_yolo_imgs/图片05.png"></p>
<p>图5：对 <span class="arithmatex">\(ImageNet\)</span> 与 <span class="arithmatex">\(WordTree\)</span>  的预测。大多数ImaNet模型使用一个大的softmax来预测概率分布。使用 <span class="arithmatex">\(WordTree\)</span>  ，我们通过共同的下位词执行多个softmax操作。</p>
<p>&emsp;<strong>联合分类和检测</strong>(Joint classification and detection)。现在我们可以使用 <span class="arithmatex">\(WordTree\)</span>  组合数据集，在分类和检测上训练联合模型。我们想要训练一个非常大规模的检测器，所以使用 <span class="arithmatex">\(COCO\)</span> 检测数据集和完整 <span class="arithmatex">\(ImageNet\)</span> 版本中的前9000类创建我们的组合数据集。我们还需要评估我们的方法，以便从 <span class="arithmatex">\(ImageNet\)</span> 检测挑战中添加任何尚未包含的类。该数据集的相应 <span class="arithmatex">\(WordTree\)</span>  具有9418个类。 <span class="arithmatex">\(ImageNet\)</span> 有更大的数据集，所以我们通过对 <span class="arithmatex">\(COCO\)</span> 进行过采样来平衡数据集，使得 <span class="arithmatex">\(ImageNet\)</span> 与 <span class="arithmatex">\(COCO\)</span> 的比例略大于 <span class="arithmatex">\(4:1\)</span>。</p>
<p>&emsp;我们使用上述的数据集训练 <span class="arithmatex">\(YOLO9000\)</span> 。 我们使用基本的  <span class="arithmatex">\(YOLOv2\)</span>  架构，但只有3个先验框而不是5个来限制输出大小。当我们的网络处理检测图像时，我们会像平常一样反向传播损失。对于分类损失，我们只是将损失反向传播到标签相应级别或更高的级别。 例如，如果标签是狗，我们不会将任何错误给树做进一步预测，如德国牧羊犬与黄金猎犬，因为我们没有这些信息。</p>
<p><img src = "02_yolo_imgs/图片06.png"></p>
<p>图6：使用 <span class="arithmatex">\(WordTree\)</span>  层次结构组合数据集。使用WordNet概念图，我们构建了视觉概念的分层树。然后，我们可以通过将数据集中的类映射到树中的synsets来合并数据集。出于说明目的，这是 <span class="arithmatex">\(WordTree\)</span>  的简化视图。</p>
<p>&emsp;当网络处理分类图像时，我们只是反向传播分类损失。要做到这一点，我们只需找到预测该类别最高概率的边界框，然后在预测的树上计算损失。我们还假设预测框与真实框的 <span class="arithmatex">\(IOU\)</span> 至少为0.3，并且基于这个假设我们反向传播目标损失。</p>
<p>&emsp;通过这种联合训练， <span class="arithmatex">\(YOLO9000\)</span> 学习使用 <span class="arithmatex">\(COCO\)</span> 中的检测数据来查找图像中的目标，并学习使用来自 <span class="arithmatex">\(ImageNet\)</span> 的数据对各种这些目标进行分类。</p>
<p>&emsp;我们在 <span class="arithmatex">\(ImageNet\)</span> 检测任务上评估 <span class="arithmatex">\(YOLO9000\)</span> 。  <span class="arithmatex">\(ImageNet\)</span> 的检测任务与 <span class="arithmatex">\(COCO\)</span> 共享44个目标类别，这意味着 <span class="arithmatex">\(YOLO9000\)</span> 看到的测试图像大多数是分类数据，而不是检测数据。  <span class="arithmatex">\(YOLO9000\)</span> 的总 <span class="arithmatex">\(mAP\)</span>  是19.7  <span class="arithmatex">\(mAP\)</span>  ，其中在不相交的156个目标类上， <span class="arithmatex">\(YOLO9000\)</span> 从未见过这些类的任何检测数据的标签，仍获得了16.0 <span class="arithmatex">\(mAP\)</span>  。这个 <span class="arithmatex">\(mAP\)</span>  高于DPM的结果，但 <span class="arithmatex">\(YOLO9000\)</span> 是在部分监督[4]的不同的数据集上训练的。而且它能同时检测9000个其他目标类别，所有的检测都是实时的。</p>
<p>&emsp;在分析 <span class="arithmatex">\(YOLO9000\)</span> 在 <span class="arithmatex">\(ImageNet\)</span> 上的表现时，我们发现它很好地学习了新的动物种类，但是在像服装和设备这样的学习类别中表现不佳。新动物更容易学习，因为目标预测可以从 <span class="arithmatex">\(COCO\)</span> 中的动物泛化的很好。相反， <span class="arithmatex">\(COCO\)</span> 没有任何类型的衣服的边界框标签，只针对人，因此 <span class="arithmatex">\(YOLO9000\)</span> 在分类“墨镜”或“泳裤”等类别上存在困难。</p>
<p><center></p>
<p><span class="arithmatex">\(\begin{array}{ll}
\text { diaper } &amp; 0.0 \\
\text { horizontal bar } &amp; 0.0 \\
\text { rubber eraser } &amp; 0.0 \\
\text { sunglasses } &amp; 0.0 \\
\text { swimming trunks } &amp; 0.0 \\
\ldots &amp; \\
\text { red panda } &amp; 50.7 \\
\text { fox } &amp; 52.1 \\
\text { koala bear } &amp; 54.3 \\
\text { tiger } &amp; 61.0 \\
\text { armadillo } &amp; 61.7
\end{array}\)</span></p>
<p>表7： <span class="arithmatex">\(ImageNet\)</span> 上的 <span class="arithmatex">\(YOLO9000\)</span> 最佳和最差类别。 156个弱监督类的AP最高和最低的类。  <span class="arithmatex">\(YOLO9000\)</span> 模型很好地预测各种各样的动物，但不擅长预测诸如服装或设备等的新类。
</center></p>
<h2 id="5">5.总结</h2>
<p>&emsp;我们介绍实时检测系统  <span class="arithmatex">\(YOLOv2\)</span>  和 <span class="arithmatex">\(YOLO9000\)</span> 。   <span class="arithmatex">\(YOLOv2\)</span>  在各种检测数据集中都是最先进的，并且比其他检测系统更快。此外，它可以在各种图像尺寸下运行，以提供速度和准确性之间的平滑折中。</p>
<p>&emsp;<span class="arithmatex">\(YOLO9000\)</span> 是一个通过联合优化检测和分类来检测超过9000个目标类别的实时框架。我们使用 <span class="arithmatex">\(WordTree\)</span>  将各种来源的数据和我们的联合优化技术相结合，在 <span class="arithmatex">\(ImageNet\)</span> 和 <span class="arithmatex">\(COCO\)</span> 上同时进行训练。  <span class="arithmatex">\(YOLO9000\)</span> 向缩小检测和分类之间的数据集大小的差距迈出了坚实的一步。</p>
<p>&emsp;我们的许多技术都是泛化到目标检测之外的领域。  <span class="arithmatex">\(ImageNet\)</span> 的 <span class="arithmatex">\(WordTree\)</span>  表示方法为图像分类提供了更丰富，更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将会很有用。像多尺度训练这样的训练技术可以为各种视觉任务提供帮助。</p>
<p>&emsp;对于未来的工作，我们希望使用类似的技术进行弱监督图像分割。我们还计划使用更强大的匹配策略来改善我们的检测结果，以在训练期间将弱标签分配给分类数据。计算机视觉拥有大量的标记数据。我们将继续寻找方法，将不同的数据来源和数据结构结合在一起，形成更强大的视觉世界模型。</p>
<h2 id="references">References</h2>
<ul>
<li>[1]   S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. arXiv preprint arXiv:1512.04143, 2015. 6</li>
<li>[2]   J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009. 1</li>
<li>[3]   M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A.  Zisserman. The pascal visual object classes (voc) chal-lenge. International journal of computer vision, 88(2):303–338, 2010. 1</li>
<li>[4]   P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Discriminatively trained deformable part models, release 4. http://people.cs.uchicago.edu/ pff/latent-release4/. 8</li>
<li>[5]   R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 4, 5, 6</li>
<li>[6]   K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-ing for image recognition. arXiv preprint arXiv:1512.03385, 2015. 2, 4, 5</li>
<li>[7]   S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 2, 5</li>
<li>[8]   A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. 2</li>
<li>[9]   M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. 5</li>
<li>[10]  T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-mon objects in context. In European Conference on Com-puter Vision, pages 740–755. Springer, 2014. 1, 6</li>
<li>[11]  W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. E. Reed. SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015. 4, 5, 6</li>
<li>[12]  G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. Introduction to wordnet: An on-line lexical database. International journal of lexicography, 3(4):235–244, 1990. 6</li>
<li>[13]  J. Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016. 5</li>
<li>[14]  J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. arXiv preprint arXiv:1506.02640, 2015. 4, 5</li>
<li>[15]  S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-wards real-time object detection with region proposal net-works. arXiv preprint arXiv:1506.01497, 2015. 2, 3, 4, 5, 6</li>
<li>[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. 2</li>
<li>[17] K. Simonyan and A. Zisserman. V ery deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 2, 5</li>
<li>[18] C. Szegedy, S. Ioffe, and V . V anhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016. 2 </li>
<li>[19] C. Szegedy, W. Liu, Y . Jia, P . Sermanet, S. Reed, D. Anguelov, D. Erhan, V . V anhoucke, and A. Rabinovich.Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 5 </li>
<li>[20] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016. 1</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="01_yolo.html" class="md-footer__link md-footer__link--prev" aria-label="上一页: yolov1" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                上一页
              </span>
              yolov1
            </div>
          </div>
        </a>
      
      
        
        <a href="03_yolo.html" class="md-footer__link md-footer__link--next" aria-label="下一页: yolov3" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                下一页
              </span>
              yolov3
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.top", "instant"], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.477d984a.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.ddd52ceb.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>