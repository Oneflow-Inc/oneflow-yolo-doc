
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../assets/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.1.11">
    
    
      
        <title>YOLOv4 - oneflow-yolo-doc</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3754935a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.f1a3b89f.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <script>function __prefix(e){return new URL("..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#abstract" class="md-skip">
          Ë∑≥ËΩ¨Ëá≥
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../index.html" title="oneflow-yolo-doc" class="md-header__button md-logo" aria-label="oneflow-yolo-doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            oneflow-yolo-doc
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              YOLOv4
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="ÊêúÁ¥¢" placeholder="ÊêúÁ¥¢" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Ê≠£Âú®ÂàùÂßãÂåñÊêúÁ¥¢ÂºïÊìé
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../index.html" class="md-tabs__link">
      È¶ñÈ°µüè†
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../tutorials/00_chapter/overview.html" class="md-tabs__link">
        YOLOv5ÊïôÁ®ãüìö
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="00_yolo_history.html" class="md-tabs__link md-tabs__link--active">
        ËÆ∫ÊñáËß£ËØª
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="oneflow-yolo-doc" class="md-nav__button md-logo" aria-label="oneflow-yolo-doc" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    oneflow-yolo-doc
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        È¶ñÈ°µüè†
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        YOLOv5ÊïôÁ®ãüìö
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="YOLOv5ÊïôÁ®ãüìö" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          YOLOv5ÊïôÁ®ãüìö
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/00_chapter/overview.html" class="md-nav__link">
        one-yolov5 ÁâπÁÇπËß£Êûê
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/01_chapter/yolov5_network_structure_analysis.html" class="md-nav__link">
        YOLOv5 ÁΩëÁªúÁªìÊûÑËß£Êûê
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/02_chapter/how_to_prepare_yolov5_training_data.html" class="md-nav__link">
        Â¶Ç‰ΩïÂáÜÂ§áYOLOv5Ê®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆ
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/03_chapter/quick_start.html" class="md-nav__link">
        Âø´ÈÄüÂºÄÂßã
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/03_chapter/model_train.html" class="md-nav__link">
        Ê®°ÂûãËÆ≠ÁªÉ
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/03_chapter/TTA.html" class="md-nav__link">
        Test-Time Augmentation (TTA)
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/04_chapter/mosaic.html" class="md-nav__link">
        Êï∞ÊçÆÂ¢ûÂº∫
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/rectangular_reasoning.html" class="md-nav__link">
        Áü©ÂΩ¢Êé®ÁêÜ
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/iou_in-depth_analysis.html" class="md-nav__link">
        IOUÊ∑±ÂÖ•Ëß£Êûê
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05_chapter/map_analysis.html" class="md-nav__link">
        Ê®°ÂûãÁ≤æÁ°ÆÂ∫¶ËØÑ‰º∞
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/06_chapter/export_onnx_tflite_tensorrt.html" class="md-nav__link">
        Ê®°ÂûãÂØºÂá∫
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      <label class="md-nav__link" for="__nav_3">
        ËÆ∫ÊñáËß£ËØª
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="ËÆ∫ÊñáËß£ËØª" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          ËÆ∫ÊñáËß£ËØª
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="00_yolo_history.html" class="md-nav__link">
        history
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="01_yolo.html" class="md-nav__link">
        YOLOv1
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="02_yolo.html" class="md-nav__link">
        YOLOv2
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="03_yolo.html" class="md-nav__link">
        YOLOv3
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          YOLOv4
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="04_yolo.html" class="md-nav__link md-nav__link--active">
        YOLOv4
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract ÊëòË¶Å
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction ÂºïË®Ä
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2. Related work Áõ∏ÂÖ≥Â∑•‰Ωú
  </a>
  
    <nav class="md-nav" aria-label="2. Related work Áõ∏ÂÖ≥Â∑•‰Ωú">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-object-detection-models" class="md-nav__link">
    2.1. Object detection models  ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-bag-of-freebies" class="md-nav__link">
    2.2. Bag of freebies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-bag-of-specials" class="md-nav__link">
    2.3. Bag of specials
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-methodology" class="md-nav__link">
    3. Methodology ÊñπÊ≥ïËÆ∫
  </a>
  
    <nav class="md-nav" aria-label="3. Methodology ÊñπÊ≥ïËÆ∫">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-selection-of-architecture" class="md-nav__link">
    3.1. Selection of architecture Êû∂ÊûÑÈÄâÊã©
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-selection-of-bof-and-bos" class="md-nav__link">
    3.2. Selection of BoF and BoS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-additional-improvements" class="md-nav__link">
    3.3. Additional improvements
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-yolov4" class="md-nav__link">
    3.4. YOLOv4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-experiments" class="md-nav__link">
    4. Experiments  ÂÆûÈ™å
  </a>
  
    <nav class="md-nav" aria-label="4. Experiments  ÂÆûÈ™å">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-experimental-setup" class="md-nav__link">
    4.1. Experimental setup ÂÆûÈ™åËÆæÁΩÆ
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-influence-of-different-features-on-classifier" class="md-nav__link">
    4.2. Influence of different features on Classifier ‰∏çÂêåÁâπÂæÅÂØπÂàÜÁ±ªÂô®ËÆ≠ÁªÉÁöÑÂΩ±Âìç
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-influence-of-different-features-on-detector" class="md-nav__link">
    4.3. Influence of different features on Detector
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-influence-of-different-backbones-and-pretrained-weightings-on-detector-training" class="md-nav__link">
    4.4. Influence of different backbones and pretrained weightings on Detector training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5results" class="md-nav__link">
    5.Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-conclusions" class="md-nav__link">
    6. Conclusions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-acknowledgements" class="md-nav__link">
    7. Acknowledgements
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="06_yolo.html" class="md-nav__link">
        YOLOv6
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    Abstract ÊëòË¶Å
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction ÂºïË®Ä
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-related-work" class="md-nav__link">
    2. Related work Áõ∏ÂÖ≥Â∑•‰Ωú
  </a>
  
    <nav class="md-nav" aria-label="2. Related work Áõ∏ÂÖ≥Â∑•‰Ωú">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-object-detection-models" class="md-nav__link">
    2.1. Object detection models  ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-bag-of-freebies" class="md-nav__link">
    2.2. Bag of freebies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-bag-of-specials" class="md-nav__link">
    2.3. Bag of specials
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-methodology" class="md-nav__link">
    3. Methodology ÊñπÊ≥ïËÆ∫
  </a>
  
    <nav class="md-nav" aria-label="3. Methodology ÊñπÊ≥ïËÆ∫">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-selection-of-architecture" class="md-nav__link">
    3.1. Selection of architecture Êû∂ÊûÑÈÄâÊã©
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-selection-of-bof-and-bos" class="md-nav__link">
    3.2. Selection of BoF and BoS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-additional-improvements" class="md-nav__link">
    3.3. Additional improvements
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-yolov4" class="md-nav__link">
    3.4. YOLOv4
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-experiments" class="md-nav__link">
    4. Experiments  ÂÆûÈ™å
  </a>
  
    <nav class="md-nav" aria-label="4. Experiments  ÂÆûÈ™å">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-experimental-setup" class="md-nav__link">
    4.1. Experimental setup ÂÆûÈ™åËÆæÁΩÆ
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-influence-of-different-features-on-classifier" class="md-nav__link">
    4.2. Influence of different features on Classifier ‰∏çÂêåÁâπÂæÅÂØπÂàÜÁ±ªÂô®ËÆ≠ÁªÉÁöÑÂΩ±Âìç
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-influence-of-different-features-on-detector" class="md-nav__link">
    4.3. Influence of different features on Detector
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-influence-of-different-backbones-and-pretrained-weightings-on-detector-training" class="md-nav__link">
    4.4. Influence of different backbones and pretrained weightings on Detector training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5results" class="md-nav__link">
    5.Results
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-conclusions" class="md-nav__link">
    6. Conclusions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-acknowledgements" class="md-nav__link">
    7. Acknowledgements
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>YOLOv4</h1>
                
                <p>[2004.10934] YOLOv4: Optimal Speed and Accuracy of Object Detection (arxiv.org)</p>
<h2 id="abstract">Abstract ÊëòË¶Å</h2>
<p>&emsp;There are a huge number of features which are said to
improve Convolutional Neural Network (CNN) accuracy.
Practical testing of combinations of such features on large
datasets, and theoretical justification of the result, is re-
quired. Some features operate on certain models exclusively
and for certain problems exclusively, or only for small-scale
datasets; while some features, such as batch-normalization
and residual-connections, are applicable to the majority of
models, tasks, and datasets. We assume that such universal
features include Weighted-Residual-Connections (WRC),
Cross-Stage-Partial-connections (CSP), Cross mini-Batch
Normalization (CmBN), Self-adversarial-training (SAT)
and Mish-activation. We use new features: WRC, CSP,
CmBN, SAT, Mish activation, Mosaic data augmentation,
CmBN, DropBlock regularization, and CIoU loss, and com-
bine some of them to achieve state-of-the-art results: 43.5%
AP (65.7% AP50) for the MS COCO dataset at a real-
time speed of ‚àº65 FPS on Tesla V100. Source code is at
https://github.com/AlexeyAB/darknet.</p>
<p>ÊçÆËØ¥ÊúâÂ§ßÈáèÁâπÂæÅÂèØ‰ª•ÊèêÈ´òÂç∑ÁßØÁ•ûÁªèÁΩëÁªú (CNN) ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ ÈúÄË¶ÅÂú®Â§ßÂûãÊï∞ÊçÆÈõÜ‰∏äÂØπËøô‰∫õÁâπÂæÅÁöÑÁªÑÂêàËøõË°åÂÆûÈôÖÊµãËØïÔºåÂπ∂ÂØπÁªìÊûúËøõË°åÁêÜËÆ∫ËÆ∫ËØÅ„ÄÇ Êüê‰∫õÁâπÂæÅ‰∏ìÈó®ÈíàÂØπÊüê‰∫õÊ®°ÂûãÔºå‰∏ìÈó®ÈíàÂØπÊüê‰∫õÈóÆÈ¢òÔºåÊàñ‰ªÖÈíàÂØπÂ∞èËßÑÊ®°Êï∞ÊçÆÈõÜÔºõ ËÄå‰∏Ä‰∫õÁâπÊÄßÔºåÂ¶ÇÊâπÈáèÂΩí‰∏ÄÂåñÂíåÊÆãÂ∑ÆËøûÊé•ÔºåÈÄÇÁî®‰∫éÂ§ßÂ§öÊï∞Ê®°Âûã„ÄÅ‰ªªÂä°ÂíåÊï∞ÊçÆÈõÜ„ÄÇÊàë‰ª¨ÂÅáËÆæËøô‰∫õÈÄöÁî®ÔºàuniversalÔºâÁâπÂæÅÂåÖÊã¨Âä†ÊùÉÊÆãÂ∑ÆËøûÊé• (WRC)„ÄÅË∑®Èò∂ÊÆµÈÉ®ÂàÜËøûÊé• (CSP)„ÄÅ‰∫§ÂèâÂ∞èÊâπÈáèÂΩí‰∏ÄÂåñ (CmBN)„ÄÅËá™ÊàëÂØπÊäóËÆ≠ÁªÉ (SAT) Âíå Mish ÊøÄÊ¥ª„ÄÇ Êàë‰ª¨‰ΩøÁî®Êñ∞ÂäüËÉΩÔºöWRC„ÄÅCSP„ÄÅCmBN„ÄÅSAT„ÄÅMish ÊøÄÊ¥ª„ÄÅMosaic Êï∞ÊçÆÂ¢ûÂº∫„ÄÅDropBlock Ê≠£ÂàôÂåñÂíå CIoU ÊçüÂ§±ÔºåÂπ∂ÁªìÂêàÂÖ∂‰∏≠ÁöÑ‰∏Ä‰∫õÊù•ÂÆûÁé∞ÊúÄÂÖàËøõÁöÑÁªìÊûúÔºöÂú® Tesla V100 ‰∏ä‰ª• ~65 FPS ÁöÑÂÆûÊó∂ÈÄüÂ∫¶Áî®‰∫é MS COCO Êï∞ÊçÆÈõÜÔºåÁªìÊûú‰∏∫43.5% APÔºà65.7 % AP50) „ÄÇ Ê∫ê‰ª£Á†Å‰Ωç‰∫é https://github.com/AlexeyAB/darknet„ÄÇ</p>
<h2 id="introduction">Introduction ÂºïË®Ä</h2>
<p>&emsp;The majority of CNN-based object detectors are largely
applicable only for recommendation systems. For example,
searching for free parking spaces via urban video cameras
is executed by slow accurate models, whereas car collision
warning is related to fast inaccurate models. Improving
the real-time object detector accuracy enables using them
not only for hint generating recommendation systems, but
also for stand-alone process management and human input
reduction. Real-time object detector operation on conven-
tional Graphics Processing Units (GPU) allows their mass
usage at an affordable price. The most accurate modern
neural networks do not operate in real time and require large
number of GPUs for training with a large mini-batch-size.
We address such problems through creating a CNN that op-
erates in real-time on a conventional GPU, and for which
training requires only one conventional GPU.</p>
<p>&emsp;Â§ßÂ§öÊï∞Âü∫‰∫é CNN ÁöÑÂØπË±°Ê£ÄÊµãÂô®Âú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏ä‰ªÖÈÄÇÁî®‰∫éÊé®ËçêÁ≥ªÁªüÔºàrecommendation systemÔºâ„ÄÇ ‰æãÂ¶ÇÔºåÈÄöËøáÂüéÂ∏ÇÊëÑÂÉèÊú∫ÊêúÁ¥¢ÂÖçË¥πÂÅúËΩ¶‰ΩçÊòØÁî±ÊÖ¢ÈÄüÂáÜÁ°ÆÊ®°ÂûãÊâßË°åÁöÑÔºåËÄåÊ±ΩËΩ¶Á¢∞ÊíûË≠¶Âëä‰∏éÂø´ÈÄü‰∏çÂáÜÁ°ÆÊ®°ÂûãÊúâÂÖ≥„ÄÇ ÊèêÈ´òÂÆûÊó∂ÁõÆÊ†áÊ£ÄÊµãÂô®ÁöÑÂáÜÁ°ÆÊÄß‰∏ç‰ªÖÂèØ‰ª•Â∞ÜÂÆÉ‰ª¨Áî®‰∫éÊèêÁ§∫ÁîüÊàêÊé®ËçêÁ≥ªÁªüÔºåËøòÂèØ‰ª•Áî®‰∫éÁã¨Á´ãÊµÅÁ®ãÁÆ°ÁêÜÔºàstand-alone process managementÔºâÂíåÂáèÂ∞ë‰∫∫Â∑•ËæìÂÖ•„ÄÇ ‰º†ÁªüÂõæÂΩ¢Â§ÑÁêÜÂçïÂÖÉ (GPU) ‰∏äÁöÑÂÆûÊó∂ÁõÆÊ†áÊ£ÄÊµãÂô®Êìç‰ΩúÂÖÅËÆ∏‰ª•ÂÆûÊÉ†ÁöÑ‰ª∑Ê†ºÂ§ßËßÑÊ®°‰ΩøÁî®„ÄÇ ÊúÄÁ≤æÁ°ÆÁöÑÁé∞‰ª£Á•ûÁªèÁΩëÁªúÊó†Ê≥ïÂÆûÊó∂ËøêË°åÔºåÂπ∂‰∏îÈúÄË¶ÅÂ§ßÈáèÁöÑ GPU ËøõË°åÂ§ßÂûãÂ∞èÂûãÊâπÂ§ÑÁêÜÂ§ßÂ∞èÁöÑËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÈÄöËøáÂàõÂª∫‰∏Ä‰∏™Âú®‰º†Áªü GPU ‰∏äÂÆûÊó∂ËøêË°åÁöÑ CNN Êù•Ëß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºå‰∏∫Ê≠§ËÆ≠ÁªÉÂè™ÈúÄË¶Å‰∏Ä‰∏™Â∏∏ËßÑ GPU„ÄÇ</p>
<p><img src = "04_yolo_imgs/ÂõæÁâá01.png"></p>
<p>Figure 1: Comparison of the proposed YOLOv4 and other
state-of-the-art object detectors. YOLOv4 runs twice faster
than EfficientDet with comparable performance. Improves
YOLOv3‚Äôs AP and FPS by 10% and 12%, respectively.</p>
<p>Âõæ1ÔºöÂØπYOLOv4ÂíåÂÖ∂‰ªñÊúÄÂÖàËøõÁöÑÁõÆÊ†áÊ£ÄÊµãÂô®ËøõË°åÊØîËæÉ„ÄÇÂÖ∑ÊúâÂêåÁ≠âÁöÑÊÄßËÉΩÊÉÖÂÜµ‰∏ãÔºåYOLOv4ÁöÑÈÄüÂ∫¶ÊòØ EfficientDet ÁöÑ‰∏§ÂÄç„ÄÇÂπ∂‰∏î YOLOv4 Â∞ÜYOLOv3 ÁöÑ AP Âíå FPS ÂàÜÂà´ÊèêÈ´ò‰∫Ü 10% Âíå 12%„ÄÇ</p>
<p>&emsp;The main goal of this work is designing a fast operating
speed of an object detector in production systems and opti-
mization for parallel computations, rather than the low com-
putation volume theoretical indicator (BFLOP). We hope
that the designed object can be easily trained and used. For
example, anyone who uses a conventional GPU to train and
test can achieve real-time, high quality, and convincing ob-
ject detection results, as the YOLOv4 results shown in Fig-
ure 1. Our contributions are summarized as follows:</p>
<p>&emsp;ËøôÈ°πÂ∑•‰ΩúÁöÑ‰∏ªË¶ÅÁõÆÊ†áÊòØËÆæËÆ°Áîü‰∫ßÁ≥ªÁªü‰∏≠ËøêË°åÈÄüÂ∫¶ËæÉÂø´ÁöÑÁõÆÊ†áÊ£ÄÊµãÂô®ÔºåÂπ∂‰ºòÂåñÂπ∂Ë°åËÆ°ÁÆóÔºåËÄå‰∏çÊòØ‰ΩéËÆ°ÁÆóÈáèÁêÜËÆ∫ÊåáÊ†á ÔºàBFLOPÔºâ„ÄÇÊàë‰ª¨Â∏åÊúõËÆæËÆ°ÁöÑÊ£ÄÊµãÂô®ËÉΩÂ§üËΩªÊùæËÆ≠ÁªÉÂíå‰ΩøÁî®„ÄÇ‰æãÂ¶ÇÔºå‰ΩøÁî®‰ªª‰Ωï‰º†Áªü GPU ËøõË°åËÆ≠ÁªÉÂíåÊµãËØïÁöÑ‰∫∫ÈÉΩÂèØ‰ª•Ëé∑ÂæóÂÆûÊó∂„ÄÅÈ´òË¥®ÈáèÂíå‰ª§‰∫∫‰ø°ÊúçÁöÑÁõÆÊ†áÊ£ÄÊµãÁªìÊûúÔºåÂ¶ÇÂõæ 1 ÊâÄÁ§∫ÁöÑ YOLOv4 ÁªìÊûúÊâÄÁ§∫„ÄÇÊàë‰ª¨ÁöÑË¥°ÁåÆÊÄªÁªìÂ¶Ç‰∏ãÔºö</p>
<ul>
<li>1Ôºâ We develope an efficient and powerful object detection
model. It makes everyone can use a 1080 Ti or 2080 Ti
GPU to train a super fast and accurate object detector.</li>
<li>
<p>1ÔºâÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™È´òÊïàËÄåÂº∫Â§ßÁöÑÂØπË±°Ê£ÄÊµãÊ®°Âûã„ÄÇ ÂÆÉ‰ΩøÊØè‰∏™‰∫∫ÈÉΩÂèØ‰ª•‰ΩøÁî® 1080 Ti Êàñ 2080 Ti GPU Êù•ËÆ≠ÁªÉ‰∏Ä‰∏™Ë∂ÖÂø´ÈÄüÂíåÂáÜÁ°ÆÁöÑÁõÆÊ†á Ê£ÄÊµãÂô®„ÄÇ</p>
</li>
<li>
<p>2ÔºâWe verify the influence of state-of-the-art Bag-of-
Freebies and Bag-of-Specials methods of object detec-
tion during the detector training.</p>
</li>
<li>
<p>2ÔºâÂú®Ê£ÄÊµãÂô®ËÆ≠ÁªÉÊúüÈó¥ÔºåÊàë‰ª¨È™åËØÅ‰∫ÜÊúÄÂÖàËøõÁöÑ Bag-of-Freebies Âíå Bag-of-Specials ÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÁöÑÂΩ±Âìç„ÄÇ</p>
</li>
<li>
<p>3ÔºâWe modify state-of-the-art methods and make them
more effecient and suitable for single GPU training,
including CBN [89], PAN [49], SAM [85], etc.</p>
</li>
<li>
<p>3ÔºâÊàë‰ª¨‰øÆÊîπ‰∫ÜÊúÄÂÖàËøõÁöÑÊñπÊ≥ïÔºå‰ΩøÂÖ∂Êõ¥ÊúâÊïàÔºåÊõ¥ÈÄÇÂêàÂçï GPU ËÆ≠ÁªÉÔºåÊñπÊ≥ïÂåÖÊã¨ CBN [89]„ÄÅPAN [49]„ÄÅSAM [85] Á≠â„ÄÇ</p>
</li>
</ul>
<p><img src = "04_yolo_imgs/ÂõæÁâá02.png"></p>
<p><center> Âõæ2ÔºöÁõÆÊ†áÊ£ÄÊµãÂô® </center></p>
<h2 id="2-related-work">2. Related work Áõ∏ÂÖ≥Â∑•‰Ωú</h2>
<h3 id="21-object-detection-models">2.1. Object detection models  ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã</h3>
<p>&emsp;A modern detector is usually composed of two parts,a backbone which is pre-trained on ImageNet and a head which is used to predict classes and bounding boxes of objects.For those detectors running on GPU platform, their backbone could be VGG [68], ResNet [26], ResNeXt [86],
or DenseNet [30]. For those detectors running on CPU platform, their backbone could be SqueezeNet [31], MobileNet
[28, 66, 27, 74], or ShuffleNet [97, 53]. As to the head part,
it is usually categorized into two kinds, i.e., one-stage object
detector and two-stage object detector. The most representative two-stage object detector is the R-CNN [19] series,
including fast R-CNN [18], faster R-CNN [64], R-FCN [9],
and Libra R-CNN [58]. It is also possible to make a two-
stage object detector an anchor-free object detector, such as
RepPoints [87]. As for one-stage object detector, the most
representative models are YOLO [61, 62, 63], SSD [50],
and RetinaNet [45]. In recent years, anchor-free one-stage
object detectors are developed. The detectors of this sort are
CenterNet [13], CornerNet [37, 38], FCOS [78], etc. Object
detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. We
can call it the neck of an object detector. Usually, a neck
is composed of several bottom-up paths and several topdown paths. Networks equipped with this mechanism include Feature Pyramid Network (FPN) [44], Path Aggregation Network (PAN) [49], BiFPN [77], and NAS-FPN [17].</p>
<p>&emsp;Áé∞‰ª£Ê£ÄÊµãÂô®ÈÄöÂ∏∏Áî±‰∏§ÈÉ®ÂàÜÁªÑÊàêÔºå‰∏Ä‰∏™ÊòØÂú® ImageNet ‰∏äÈ¢ÑÂÖàËÆ≠ÁªÉÁöÑÈ™®Âπ≤ÁΩëÔºåÂè¶‰∏Ä‰∏™ÊòØÁî®‰∫éÈ¢ÑÊµãÁâ©‰ΩìÁöÑÁ±ªÂíåËæπÁïåÊ°ÜÁöÑÂ§¥ÈÉ®„ÄÇÂØπ‰∫éÂú® GPU Âπ≥Âè∞‰∏äËøêË°åÁöÑÊ£ÄÊµãÂô®ÔºåÂÖ∂‰∏ªÂπ≤ÂèØ‰ª•ÊòØ VGG [68]„ÄÅResNet [26]„ÄÅResNeXt [86]ÊàñDenseNet [30]„ÄÇÂØπ‰∫éÂú® CPU Âπ≥Âè∞‰∏äËøêË°åÁöÑÊ£ÄÊµãÂô®ÔºåÂÖ∂‰∏ªÂπ≤ÂèØ‰ª•ÊòØSqueezeNet [31], MobileNet [28, 66, 27, 74], Êàñ ShuffleNet [97, 53].„ÄÇËá≥‰∫éÂ§¥ÈÉ®ÈÉ®ÂàÜÔºåÈÄöÂ∏∏ÂàÜ‰∏∫‰∏§Á±ª Ôºå Âç≥ÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®Âíå‰∏§Èò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô® „ÄÇÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑ‰∏§Á∫ßÁõÆÊ†áÊ£ÄÊµãÂô®ÊòØR-CNN[19]Á≥ªÂàóÔºåÂåÖÊã¨fast R-CNN [18], faster R-CNN [64], R-FCN [9], Âíå Libra R-CNN [58] „ÄÇ‰πüÂèØ‰ª•‰Ωø‰∏§Èò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®Êàê‰∏∫Êó†ÈîöÁõÆÊ†áÊ£ÄÊµãÂô®ÔºåÂ¶Ç RepPoints [87]„ÄÇËá≥‰∫éÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®ÔºåÊúÄÂÖ∑‰ª£Ë°®ÊÄßÁöÑÂûãÂè∑ÊòØYOLO[61„ÄÅ62„ÄÅ63]„ÄÅSSD[50]ÂíåRetinaNet[45]„ÄÇËøëÂπ¥Êù•ÔºåÁ†îÂà∂‰∫ÜÊó†ÈîöÂºèÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®„ÄÇÊ≠§Á±ªÊ£ÄÊµãÂô®Êúâ CenterNet [13]„ÄÅCornerNet [37„ÄÅ 38]„ÄÅFCOS [78]Á≠â„ÄÇËøëÂπ¥Êù•ÁõÆÊ†áÊ£ÄÊµãÂô®ÁöÑÁ†îÂèëÈÄöÂ∏∏ÊòØÂú®È™®Âπ≤ÂíåÂ§¥ÈÉ®‰πãÈó¥ÁöÑÊ∑ªÂä†‰∏Ä‰∫õÂ±ÇÔºåËøô‰∫õÂ±ÇÈÄöÂ∏∏Áî®‰∫éÊî∂ÈõÜ‰∏çÂêåÈò∂ÊÆµÁöÑÁâπÂæÅÂõæ„ÄÇÊàë‰ª¨ÂèØ‰ª•Áß∞ÂÆÉ‰∏∫ÁõÆÊ†áÊ£ÄÊµãÂô®ÁöÑËÑñÂ≠ê„ÄÇÈÄöÂ∏∏ÔºåÈ¢àÈÉ®Áî±Âá†‰∏™Ëá™‰∏ãËÄå‰∏äÁöÑË∑ØÂæÑÂíåÂá†‰∏™Ëá™‰∏äËÄå‰∏ãÁöÑË∑ØÂæÑÁªÑÊàê„ÄÇÈÖçÂ§áÊ≠§Êú∫Âà∂ÁöÑÁΩëÁªúÂåÖÊã¨ÁâπÂæÅÈáëÂ≠óÂ°îÁΩëÁªú ÔºàFPNÔºâ [44]„ÄÅË∑ØÂæÑËÅöÂêàÁΩëÁªú ÔºàPANÔºâ [49]„ÄÅBiFPN [77]Âíå NAS-FPN [17]„ÄÇ</p>
<p>&emsp;In addition to the above models, some researchers put their emphasis on directly building a new backbone (DetNet [43], DetNAS [7]) or a new whole model (SpineNet [12], HitDetector [20]) for object detection.</p>
<p>&emsp;Èô§‰∫Ü‰∏äËø∞Ê®°ÂûãÂ§ñÔºå‰∏Ä‰∫õÁ†îÁ©∂‰∫∫ÂëòËøòÊääÈáçÁÇπÁõ¥Êé•ÊîæÂú®ÊûÑÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ‰∏ªÂπ≤ÔºàDetNet [43]ÔºåDetNAS [7]ÔºâÊàñÊñ∞ÁöÑÂÆåÊï¥Ê®°ÂûãÔºàSpineNet [12]ÔºåHitDetector [20]ÔºâÁî®‰∫éÁõÆÊ†áÊ£ÄÊµã„ÄÇ</p>
<p>&emsp;To sum up, an ordinary object detector is composed of several parts:</p>
<p>&emsp;Áªº‰∏äÊâÄËø∞Ôºå‰∏Ä‰∏™ÊôÆÈÄöÁöÑÁõÆÊ†áÊ£ÄÊµãÂô®Áî±Â¶Ç‰∏ãÈÉ®ÂàÜÁªÑÊàêÔºö</p>
<ul>
<li>Input: Image, Patches, Image Pyramid</li>

<li>Backbones: VGG16 [68], ResNet-50 [26], SpineNet [12], EfficientNet-B0/B7 [75], CSPResNeXt50 [81], CSPDarknet53 [81] </li>

<li> Neck:
    <ul>
    <li> Additional blocks: SPP [25], ASPP [5], RFB [47], SAM [85]</li>
    <li>Path-aggregation blocks: FPN [44], PAN [49], NAS-FPN [17], Fully-connected FPN, BiFPN [77], ASFF [48], SFAM [98] </li>
    </ul>
</li>
<li>Heads:
    <ul>
        <li>Dense Prediction (one-stage):
            <ul type = "disc">
                <li>RPN [64], SSD [50], YOLO [61], RetinaNet [45] (anchor based)</li>
                <li> CornerNet [37], CenterNet [13], MatrixNet [60], FCOS [78] (anchor free) </li>
            </ul>
        </li>
        <li> Sparse Prediction (two-stage):
            <ul type = "disc">
                <li> Faster R-CNN [64], R-FCN [9], Mask R-CNN [23] (anchor based)</li>
                <li> RepPoints [87] (anchor free)</li>
            </ul>
        </li>
    </ul>
</li>
</ul>

<h3 id="22-bag-of-freebies">2.2. Bag of freebies</h3>
<p>&emsp;Usually, a conventional object detector is trained offline. 
Therefore, researchers always like to take this advantage and develop better training methods which can make the object detector receive better accuracy without increasing the inference cost. 
We call these methods that only change the training strategy or only increase the training
cost as ‚Äúbag of freebies.‚Äù What is often adopted by object
detection methods and meets the definition of bag of freebies is data augmentation. The purpose of data augmentation is to increase the variability of the input images, so that the designed object detection model has higher robustness
to the images obtained from different environments. 
For examples, photometric distortions and geometric distortions
are two commonly used data augmentation method and they
definitely benefit the object detection task. In dealing with
photometric distortion, we adjust the brightness, contrast,
hue, saturation, and noise of an image. For geometric distortion, we add random scaling, cropping, flipping, and rotating.</p>
<p>&emsp;ÈÄöÂ∏∏Ôºå‰º†ÁªüÁöÑÁâ©‰ΩìÊ£ÄÊµãÂô®ÊòØÁ¶ªÁ∫øËÆ≠ÁªÉÁöÑ„ÄÇ Âõ†Ê≠§ÔºåÁ†îÁ©∂‰∫∫ÂëòÊÄªÊòØÂñúÊ¨¢Âà©Áî®Ëøô‰∏Ä‰ºòÂäøÔºåÂºÄÂèëÊõ¥Â•ΩÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºå‰ΩøÁõÆÊ†áÊ£ÄÊµãÂô®Âú®‰∏çÂ¢ûÂä†Êé®ÁêÜÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãËé∑ÂæóÊõ¥Â•ΩÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨Â∞ÜËøô‰∫õÂè™‰ºöÊîπÂèòËÆ≠ÁªÉÁ≠ñÁï•ÊàñÂè™‰ºöÂ¢ûÂä†ËÆ≠ÁªÉÊàêÊú¨ÁöÑÊñπÊ≥ïÁß∞‰∏∫ ‚Äúbag of freebies‚Äù„ÄÇÁõÆÊ†áÊ£ÄÊµãÊñπÊ≥ïÁªèÂ∏∏ÈááÁî®‰∏îÁ¨¶Âêàbag of freebies ÂÆö‰πâÁöÑÊòØÊï∞ÊçÆÂ¢ûÂº∫Ôºàdata augmentationÔºâ„ÄÇÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÁõÆÁöÑÊòØÂ¢ûÂä†ËæìÂÖ•ÂõæÂÉèÁöÑÂèØÂèòÊÄßÔºàvariabilityÔºâÔºå‰ΩøËÆæËÆ°ÁöÑÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÂØπ‰∏çÂêåÁéØÂ¢É‰∏ãËé∑ÂæóÁöÑÂõæÂÉèÂÖ∑ÊúâÊõ¥È´òÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂÖâÂ∫¶Â§±ÁúüÔºàphotometric distortionÔºâÂíåÂá†‰ΩïÂ§±ÁúüÔºàgeometric distortionÔºâÊòØ‰∏§ÁßçÂ∏∏Áî®ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂÆÉ‰ª¨ÁªùÂØπÊúâÂà©‰∫éÁõÆÊ†áÊ£ÄÊµã‰ªªÂä°„ÄÇÂú®Â§ÑÁêÜÂÖâÂ∫¶Â§±ÁúüÊó∂ÔºåÊàë‰ª¨Ë∞ÉÊï¥ÂõæÂÉèÁöÑ‰∫ÆÂ∫¶ÔºàbrightnessÔºâ„ÄÅÂØπÊØîÂ∫¶ÔºàcontrastÔºâ„ÄÅËâ≤Ë∞ÉÔºàhueÔºâ„ÄÅÈ•±ÂíåÂ∫¶ÔºàsaturationÔºâÂíåÂô™Â£∞ÔºànoiseÔºâ„ÄÇ ÂØπ‰∫éÂá†‰ΩïÂ§±ÁúüÔºåÊàë‰ª¨Ê∑ªÂä†‰∫ÜÈöèÊú∫Áº©ÊîæÔºàrandom scaleÔºâ„ÄÅË£ÅÂâ™ÔºàcropÔºâ„ÄÅÁøªËΩ¨ÔºàflipÔºâÂíåÊóãËΩ¨ÔºàrotateÔºâ„ÄÇ</p>
<p>&emsp;The data augmentation methods mentioned above are all
pixel-wise adjustments, and all original pixel information in
the adjusted area is retained. In addition, some researchers
engaged in data augmentation put their emphasis on sim-
ulating object occlusion issues. They have achieved good
results in image classification and object detection. For ex-
ample, random erase [100] and CutOut [11] can randomly
select the rectangle region in an image and fill in a random
or complementary value of zero. As for hide-and-seek [69]
and grid mask [6], they randomly or evenly select multiple
rectangle regions in an image and replace them to all ze-
ros. If similar concepts are applied to feature maps, there
are DropOut [71], DropConnect [80], and DropBlock [16]
methods. In addition, some researchers have proposed the
methods of using multiple images together to perform data
augmentation. For example, MixUp [92] uses two images
to multiply and superimpose with different coefficient ra-
tios, and then adjusts the label with these superimposed ra-
tios. As for CutMix [91], it is to cover the cropped image
to rectangle region of other images, and adjusts the label
according to the size of the mix area. In addition to the
above mentioned methods, style transfer GAN [15] is also
used for data augmentation, and such usage can effectively
reduce the texture bias learned by CNN.</p>
<p>&emsp;‰∏äÈù¢ÊèêÂà∞ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÈÉΩÊòØÈÄêÂÉèÁ¥†Ôºàpixel-wiseÔºâË∞ÉÊï¥ÁöÑÔºåÂπ∂‰∏î‰øùÁïô‰∫ÜË∞ÉÊï¥Âå∫ÂüüÂÜÖÁöÑÊâÄÊúâÂéüÂßãÂÉèÁ¥†‰ø°ÊÅØ„ÄÇ Ê≠§Â§ñÔºå‰∏Ä‰∫õ‰ªé‰∫ãÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÁ†îÁ©∂‰∫∫ÂëòÂ∞ÜÈáçÁÇπÊîæÂú®Ê®°ÊãüÂØπË±°ÈÅÆÊå°ÔºàocclusionÔºâÈóÆÈ¢ò‰∏ä„ÄÇ‰ªñ‰ª¨Âú®ÂõæÂÉèÂàÜÁ±ªÂíåÁõÆÊ†áÊ£ÄÊµãÊñπÈù¢ÂèñÂæó‰∫ÜÂæàÂ•ΩÁöÑÊïàÊûú„ÄÇ ‰æãÂ¶ÇÔºåÈöèÊú∫Êì¶Èô§[100]ÂíåCutOut[11]ÂèØ‰ª•ÈöèÊú∫ÈÄâÊã©ÂõæÂÉè‰∏≠ÁöÑÁü©ÂΩ¢Âå∫ÂüüÂπ∂Â°´ÂÖÖÈöèÊú∫ÂÄºÊàñ‰∫íË°•ÂÄºÈõ∂Ôºàcomplementary value of zeroÔºâ„ÄÇËá≥‰∫éÊçâËø∑ËóèÔºàhide-and-seekÔºâ [69] ÂíåÁΩëÊ†ºËíôÁâàÔºàgrid maskÔºâ [6]ÔºåÂÆÉ‰ª¨ÈöèÊú∫ÊàñÂùáÂåÄÂú∞ÈÄâÊã©ÂõæÂÉè‰∏≠ÁöÑÂ§ö‰∏™Áü©ÂΩ¢Âå∫ÂüüÂπ∂Â∞ÜÂÆÉ‰ª¨ÂÖ®ÊõøÊç¢‰∏∫Èõ∂„ÄÇ Â¶ÇÊûúÂ∞ÜÁ±ª‰ººÁöÑÊ¶ÇÂøµÂ∫îÁî®‰∫éÁâπÂæÅÂõæÔºåÂàôÊúâ DropOut [71]„ÄÅDropConnect [80] Âíå DropBlock [16] ÊñπÊ≥ï„ÄÇ Ê≠§Â§ñÔºå‰∏Ä‰∫õÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫ÜÂ∞ÜÂ§ö‰∏™ÂõæÂÉè‰∏ÄËµ∑‰ΩøÁî®Êù•ËøõË°åÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊñπÊ≥ï„ÄÇ ‰æãÂ¶ÇÔºåMixUp [92] ‰ΩøÁî®‰∏§ÂπÖÂõæÂÉèÊù•‰Ωø‰∏çÂêåÁöÑÁ≥ªÊï∞ÊØî‰æãÔºàcoefficient ratioÔºâÁõ∏‰πòÂè†Âä†ÔºàsuperimposeÔºâÔºåÁÑ∂ÂêéÁî®Ëøô‰∫õÂè†Âä†ÊØî‰æãË∞ÉÊï¥Ê†áÁ≠æ„ÄÇËá≥‰∫éCutMix [91]ÔºåÂ∞±ÊòØÂ∞ÜË£ÅÂâ™ÂêéÁöÑÂõæÂÉèË¶ÜÁõñÂà∞ÂÖ∂‰ªñÂõæÂÉèÁöÑÁü©ÂΩ¢Âå∫ÂüüÔºåÂπ∂Ê†πÊçÆÊ∑∑ÂêàÂå∫ÂüüÁöÑÂ§ßÂ∞èË∞ÉÊï¥Ê†áÁ≠æ„ÄÇ Èô§‰∫Ü‰∏äËø∞ÊñπÊ≥ïÂ§ñÔºåÈ£éÊ†ºËøÅÁßª GAN [15] ‰πüË¢´Áî®‰∫éÊï∞ÊçÆÂ¢ûÂº∫ÔºåËøôÁßçÁî®Ê≥ïÂèØ‰ª•ÊúâÊïàÂáèÂ∞ë CNN Â≠¶‰π†Âà∞ÁöÑÁ∫πÁêÜÔºàtextureÔºâÂÅèÂ∑Æ„ÄÇ</p>
<p>&emsp;Different from the various approaches proposed above,
some other bag of freebies methods are dedicated to solving
the problem that the semantic distribution in the dataset may
have bias. In dealing with the problem of semantic distribution bias, a very important issue is that there is a problem
of data imbalance between different classes, and this problem is often solved by hard negative example mining [72]
or online hard example mining [67] in two-stage object detector. But the example mining method is not applicable to one-stage object detector, because this kind of detector belongs to the dense prediction architecture. Therefore Lin
et al. [45] proposed focal loss to deal with the problem
of data imbalance existing between various classes. Another very important issue is that it is difficult to express the
relationship of the degree of association between different
categories with the one-hot hard representation. This representation scheme is often used when executing labeling.
The label smoothing proposed in [73] is to convert hard label into soft label for training, which can make model more
robust. In order to obtain a better soft label, Islam et al. [33]
introduced the concept of knowledge distillation to design
the label refinement network.</p>
<p>&emsp;‰∏é‰∏äÈù¢ÊèêÂá∫ÁöÑÂêÑÁßçÊñπÊ≥ï‰∏çÂêåÔºåÂÖ∂‰ªñ‰∏Ä‰∫õbag of freebiesÊñπÊ≥ï‰∏ìÈó®Áî®‰∫éËß£ÂÜ≥Êï∞ÊçÆÈõÜ‰∏≠ËØ≠‰πâÂàÜÂ∏ÉÔºàsemantic distributionÔºâÂèØËÉΩÂ≠òÂú®ÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇÂú®Â§ÑÁêÜËØ≠‰πâÂàÜÂ∏ÉÂÅèÂ∑ÆÈóÆÈ¢òÊó∂Ôºå‰∏Ä‰∏™ÈùûÂ∏∏ÈáçË¶ÅÁöÑÈóÆÈ¢òÊòØ‰∏çÂêåÁ±ª‰πãÈó¥Â≠òÂú®Êï∞ÊçÆ‰∏çÂπ≥Ë°°Ôºàdata imbalanceÔºâÁöÑÈóÆÈ¢ò„ÄÇ Ëøô‰∏™ÈóÆÈ¢òÈÄöÂ∏∏ÈÄöËøá‰∏§Èò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®‰∏≠ÁöÑÁ°¨Âèç‰æãÊåñÊéòÔºàhard negative example miningÔºâ[72]ÊàñÂú®Á∫øÁ°¨Á§∫‰æãÊåñÊéòÔºàonline hard example miningÔºâ[67]Êù•Ëß£ÂÜ≥„ÄÇ ‰ΩÜÊòØÁ§∫‰æãÊåñÊéòÊñπÊ≥ï‰∏çÈÄÇÁî®‰∫éÂçïÈò∂ÊÆµÁõÆÊ†áÊ£ÄÊµãÂô®ÔºåÂõ†‰∏∫ËøôÁßçÊ£ÄÊµãÂô®Â±û‰∫éÂØÜÈõÜÈ¢ÑÊµãÊû∂ÊûÑ„ÄÇÂõ†Ê≠§ Lin Á≠â‰∫∫ [45] ÊèêÂá∫‰∫Üfocal loss Êù•Â§ÑÁêÜÂêÑ‰∏™Á±ª‰πãÈó¥Â≠òÂú®ÁöÑÊï∞ÊçÆ‰∏çÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇ Âè¶‰∏Ä‰∏™ÈùûÂ∏∏ÈáçË¶ÅÁöÑÈóÆÈ¢òÊòØÔºå‰ΩøÁî®one-hot hardË°®Á§∫ÂæàÈöæË°®Ëææ‰∏çÂêåÁ±ªÂà´‰πãÈó¥ÁöÑÂÖ≥ËÅîÁ®ãÂ∫¶ÁöÑÂÖ≥Á≥ª„ÄÇ ËøôÁßçË°®Á§∫ÊñπÊ°àÂú®ËøõË°åÊ†áÊ≥®Êó∂ÁªèÂ∏∏‰ΩøÁî®„ÄÇ[73]‰∏≠ÊèêÂá∫ÁöÑÊ†áÁ≠æÂπ≥ÊªëÔºàlabel smoothingÔºâÊòØÂ∞ÜÁ°¨Ê†áÁ≠æÔºàhard labelÔºâËΩ¨Êç¢‰∏∫ËΩØÊ†áÁ≠æÔºàsoft labelÔºâËøõË°åËÆ≠ÁªÉÔºåÂèØ‰ª•‰ΩøÊ®°ÂûãÊõ¥Âä†È≤ÅÊ£í„ÄÇ ‰∏∫‰∫ÜËé∑ÂæóÊõ¥Â•ΩÁöÑËΩØÊ†áÁ≠æÔºåIslamÁ≠â[33]ÂºïÂÖ•‰∫ÜÁü•ËØÜËí∏È¶èÔºàknowledge distillationÔºâÁöÑÊ¶ÇÂøµÊù•ËÆæËÆ°Ê†áÁ≠æÁªÜÂåñÁΩëÁªúÔºàlabel refinement networkÔºâ„ÄÇ</p>
<p>&emsp;The last bag of freebies is the objective function of
Bounding Box (BBox) regression. The traditional object
detector usually uses Mean Square Error (MSE) to directly perform regression on the center point coordinates
and height and width of the BBox, i.e.,  <span class="arithmatex">\({x_{center}, y_{center},
w, h}\)</span> , or the upper left point and the lower right point,
i.e., <span class="arithmatex">\({x_{top-left}, y_{top-left}, x_{bottom-right}, y_{bottom-right} }\)</span> . As
for anchor-based method, it is to estimate the corresponding offset, 
for example 
<span class="arithmatex">\({x_{center-offset}, y_{center-offset},w_{offset}, h_{offset}}\)</span>
and<br />
<span class="arithmatex">\({x_{top-left-offset}, y_{top-left-offset},x_{bottom-right-offset}, y_{bottom-right-offset}}\)</span> .
However, to directly estimate the coordinate values of each point of the
BBox is to treat these points as independent variables, but
in fact does not consider the integrity of the object itself. In
order to make this issue processed better, some researchers
recently proposed IoU loss [90], which puts the coverage of
predicted BBox area and ground truth BBox area into consideration. The IoU loss computing process will trigger the
calculation of the four coordinate points of the BBox by executing IoU with the ground truth, and then connecting the
generated results into a whole code. Because IoU is a scale
invariant representation, it can solve the problem that when
traditional methods calculate the <span class="arithmatex">\(l_1\)</span> or <span class="arithmatex">\(l_2\)</span> loss of 
<span class="arithmatex">\({x, y, w,h}\)</span> , the loss will increase with the scale. Recently, some
researchers have continued to improve IoU loss. For example, GIoU loss [65] is to include the shape and orientation
of object in addition to the coverage area. They proposed to
find the smallest area BBox that can simultaneously cover
the predicted BBox and ground truth BBox, and use this BBox as the denominator to replace the denominator originally used in IoU loss. As for DIoU loss [99], it additionally considers the distance of the center of an object, and CIoU loss [99], on the other hand simultaneously considers the
overlapping area, the distance between center points, and the aspect ratio. CIoU can achieve better convergence speed and accuracy on the BBox regression problem.</p>
<p>&emsp;ÊúÄÂêé‰∏Ä‰∫õ bag of freebies ÊòØËæπÁïåÊ°Ü (BBox) ÂõûÂΩíÁöÑÁõÆÊ†áÂáΩÊï∞„ÄÇ ‰º†ÁªüÁöÑÁõÆÊ†áÊ£ÄÊµãÂô®ÈÄöÂ∏∏‰ΩøÁî®ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÁõ¥Êé•ÂØπBBoxÁöÑ‰∏≠ÂøÉÁÇπÂùêÊ†áÂíåÈ´òÂ∫¶ÂíåÂÆΩÂ∫¶ËøõË°åÂõûÂΩíÔºåÂç≥
<span class="arithmatex">\({x_{center}, y_{center},w, h}\)</span>ÔºåÊàñÂ∑¶‰∏äÁÇπÂíåÂè≥‰∏ãÁÇπÔºåÂç≥ <span class="arithmatex">\({x_{top-left}, y_{top-left}, x_{bottom-right}, y_{bottom-right} }\)</span> „ÄÇÂØπ‰∫éanchor-basedÊñπÊ≥ïÔºåÂ∞±ÊòØ‰º∞ËÆ°ÂØπÂ∫îÁöÑoffsetÔºå‰æãÂ¶Ç <span class="arithmatex">\({x_{center-offset}, y_{center-offset},w_{offset}, h_{offset}}\)</span> Âíå <span class="arithmatex">\({x_{top-left-offset}, y_{top-left-offset},x_{bottom-right-offset}, y_{bottom-right-offset}}\)</span> „ÄÇ‰ΩÜÊòØÔºåÁõ¥Êé•‰º∞ËÆ°BBoxÊØè‰∏™ÁÇπÁöÑÂùêÊ†áÂÄºÔºåÂ∞±ÊòØÊääËøô‰∫õÁÇπÂΩìÊàêËá™ÂèòÈáèÔºàindependent variableÔºâÔºåÂÆûÈôÖ‰∏äÂπ∂Ê≤°ÊúâËÄÉËôëÂØπË±°Êú¨Ë∫´ÁöÑÂÆåÊï¥ÊÄß„ÄÇ ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞Â§ÑÁêÜËøô‰∏™ÈóÆÈ¢òÔºåÊúÄËøëÊúâÁ†îÁ©∂‰∫∫ÂëòÊèêÂá∫‰∫Ü IoU loss [90]ÔºåÂÆÉËÄÉËôë‰∫ÜÈ¢ÑÊµã BBox Âå∫ÂüüÂíåÁúüÂÆûBBox Âå∫ÂüüÁöÑË¶ÜÁõñËåÉÂõ¥„ÄÇIoU ÊçüÂ§±ËÆ°ÁÆóËøáÁ®ãÈÄöËøá‰ΩøÁî®ÁúüÂÆûÂÄºËÆ°ÁÆóIoU Êù•Ëß¶ÂèëBBox Âõõ‰∏™ÂùêÊ†áÁÇπÁöÑËÆ°ÁÆóÔºåÁÑ∂ÂêéÂ∞ÜÁîüÊàêÁöÑÁªìÊûúËøûÊé•Êàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ‰ª£Á†Å„ÄÇÁî±‰∫éIoUÊòØÊØî‰æã‰∏çÂèòÁöÑË°®Á§∫ÔºåÂèØ‰ª•Ëß£ÂÜ≥‰º†ÁªüÊñπÊ≥ïËÆ°ÁÆó <span class="arithmatex">\({x,y,w,h}\)</span> ÁöÑ <span class="arithmatex">\(l_1\)</span> Êàñ <span class="arithmatex">\(l_2\)</span> ÊçüÂ§±Êó∂ÔºåÊçüÂ§±‰ºöÈöèÁùÄÊØî‰æãÂ¢ûÂä†ÁöÑÈóÆÈ¢ò„ÄÇÊúÄËøëÔºå‰∏Ä‰∫õÁ†îÁ©∂‰∫∫ÂëòÁªßÁª≠ÊîπËøõ IoU ÊçüÂ§±„ÄÇ‰æãÂ¶ÇÔºåGIoU loss[65]Èô§‰∫ÜË¶ÜÁõñÂå∫ÂüüÂ§ñÔºåËøòÂåÖÊã¨Áâ©‰ΩìÁöÑÂΩ¢Áä∂ÂíåÊñπÂêëÔºàorientationÔºâ„ÄÇ‰ªñ‰ª¨ÊèêÂá∫ÂØªÊâæÂèØÂêåÊó∂Ë¶ÜÁõñÈ¢ÑÊµãBBoxÂíåÁúüÂÆûBBoxÁöÑÊúÄÂ∞èÈù¢ÁßØBBoxÔºåÂπ∂Áî®Ëøô‰∏™BBox‰Ωú‰∏∫ÂàÜÊØçÔºàdenominatorÔºâÊù•‰ª£ÊõøÂéüÊù•Âú®IoU loss‰∏≠‰ΩøÁî®ÁöÑÂàÜÊØç„ÄÇËá≥‰∫éDIoU loss [99]ÔºåÂÆÉÈ¢ùÂ§ñËÄÉËôë‰∫ÜÁâ©‰Ωì‰∏≠ÂøÉÁöÑË∑ùÁ¶ªÔºåËÄåCIoU loss [99]ÔºåÂè¶‰∏ÄÊñπÈù¢ÂêåÊó∂ËÄÉËôë‰∫ÜÈáçÂè†Âå∫Âüü„ÄÅ‰∏≠ÂøÉÁÇπ‰πãÈó¥ÁöÑË∑ùÁ¶ªÂíåÂÆΩÈ´òÊØî„ÄÇCIoU Âú® BBox ÂõûÂΩíÈóÆÈ¢ò‰∏äÂèØ‰ª•ËææÂà∞Êõ¥Â•ΩÁöÑÊî∂ÊïõÈÄüÂ∫¶ÂíåÁ≤æÂ∫¶„ÄÇ</p>
<h3 id="23-bag-of-specials">2.3. Bag of specials</h3>
<p>&emsp;For those plugin modules and post-processing methods
that only increase the inference cost by a small amount
but can significantly improve the accuracy of object detection, we call them ‚Äúbag of specials‚Äù. Generally speaking,
these plugin modules are for enhancing certain attributes in a model, such as enlarging receptive field, introducing attention mechanism, or strengthening feature integration capability, etc., and post-processing is a method for screening model prediction results.</p>
<p>&emsp;ÂØπ‰∫éÈÇ£‰∫õÂè™Â¢ûÂä†Â∞ëÈáèÊé®ÁêÜÊàêÊú¨‰ΩÜÂèØ‰ª•ÊòæÁùÄÊèêÈ´òÁõÆÊ†áÊ£ÄÊµãÁ≤æÂ∫¶ÁöÑÊèí‰ª∂Ê®°ÂùóÔºàplugin moduleÔºâÂíåÂêéÂ§ÑÁêÜÔºàpost-processingÔºâÊñπÊ≥ïÔºåÊàë‰ª¨Áß∞‰πã‰∏∫‚Äúbag of specials‚Äù„ÄÇ ‰∏ÄËà¨Êù•ËØ¥ÔºåËøô‰∫õÊèí‰ª∂Ê®°ÂùóÊòØ‰∏∫‰∫ÜÂ¢ûÂº∫Ê®°Âûã‰∏≠ÁöÑÊüê‰∫õÂ±ûÊÄßÔºàattributeÔºâÔºåÊØîÂ¶ÇÊâ©Â§ßÊÑüÂèóÈáéÔºàreceptive fieldÔºâ„ÄÅÂºïÂÖ•Ê≥®ÊÑèÂäõÊú∫Âà∂Ôºàattention mechanismÔºâ„ÄÅÊàñËÄÖÂä†Âº∫ÁâπÂæÅÊï¥ÂêàÔºàintegrationÔºâËÉΩÂäõÁ≠âÔºåÂêéÂ§ÑÁêÜÊòØ‰∏ÄÁßçÁ≠õÈÄâÔºàscreenÔºâÊ®°ÂûãÈ¢ÑÊµãÁªìÊûúÁöÑÊñπÊ≥ï„ÄÇ</p>
<p>&emsp;Common modules that can be used to enhance receptive field are SPP [25], ASPP [5], and RFB [47]. The
SPP module was originated from Spatial Pyramid Matching (SPM) [39], and SPMs original method was to split feature map into several d √ó d equal blocks, where d can be <span class="arithmatex">\({1, 2, 3, ...}\)</span>, thus forming spatial pyramid, and then extracting bag-of-word features. SPP integrates SPM into CNN
and use max-pooling operation instead of bag-of-word operation. Since the SPP module proposed by He et al. [25]
will output one dimensional feature vector, it is infeasible to
be applied in Fully Convolutional Network (FCN). Thus in
the design of YOLOv3 [63], Redmon and Farhadi improve
SPP module to the concatenation of max-pooling outputs
with kernel size <span class="arithmatex">\(k √ó k\)</span> , where <span class="arithmatex">\(k = {1, 5, 9, 13}\)</span> , and stride
equals to 1. Under this design, a relatively large k √ó k maxpooling effectively increase the receptive field of backbone
feature. After adding the improved version of SPP module,
YOLOv3-608 upgrades AP50 by 2.7% on the MS COCO
object detection task at the cost of 0.5% extra computation.
The difference in operation between ASPP [5] module and
improved SPP module is mainly from the original k√ók kernel size, max-pooling of stride equals to 1 to several <span class="arithmatex">\(3 √ó 3\)</span>
kernel size, dilated ratio equals to k, and stride equals to 1
in dilated convolution operation. RFB module is to use several dilated convolutions of k√ók kernel, dilated ratio equals
to k, and stride equals to 1 to obtain a more comprehensive
spatial coverage than ASPP . RFB [47] only costs 7% extra
inference time to increase the AP50 of SSD on MS COCO
by 5.7%.</p>
<p>&emsp;ÂèØÁî®‰∫éÂ¢ûÂº∫ÊÑüÂèóÈáéÁöÑÂ∏∏ËßÅÊ®°ÂùóÊúâ SPP [25]„ÄÅASPP [5] Âíå RFB [47]„ÄÇSPPÊ®°ÂùóËµ∑Ê∫ê‰∫éSpatial Pyramid MatchingÔºàSPMÔºâ[39]ÔºåSPMÁöÑÂéüÂßãÊñπÊ≥ïÊòØÂ∞ÜÁâπÂæÅÂõæÂàÜÂâ≤ÊàêÂá†‰∏™ <span class="arithmatex">\(d √ó d\)</span> Áõ∏Á≠âÁöÑÂùóÔºåÂÖ∂‰∏≠ <span class="arithmatex">\(d\)</span> ÂèØ‰ª•ÊòØ <span class="arithmatex">\({1, 2, 3, ...}\)</span> ÔºåÂõ†Ê≠§ÂΩ¢ÊàêÁ©∫Èó¥ÈáëÂ≠óÂ°îÔºåÁÑ∂ÂêéÊèêÂèñËØçË¢ãÔºàbag-of-wordÔºâÁâπÂæÅ„ÄÇSPP Â∞Ü SPM ÈõÜÊàêÂà∞ CNN ‰∏≠Âπ∂‰ΩøÁî®ÊúÄÂ§ßÊ±†ÂåñÊìç‰ΩúËÄå‰∏çÊòØËØçË¢ãÊìç‰Ωú„ÄÇÁî±‰∫éHeÁ≠â‰∫∫[25]ÊèêÂá∫ÁöÑSPPÊ®°Âùó‰ºöËæìÂá∫‰∏ÄÁª¥ÁâπÂæÅÂêëÈáèÔºåÂõ†Ê≠§‰∏çÈÄÇÁî®‰∫éÂÖ®Âç∑ÁßØÁΩëÁªúÔºàFCNÔºâ„ÄÇÂõ†Ê≠§ÔºåÂú® YOLOv3 [63] ÁöÑËÆæËÆ°‰∏≠ÔºåRedmon Âíå Farhadi Â∞Ü SPP Ê®°ÂùóÊîπËøõ‰∏∫ ÂÜÖÊ†∏Â§ßÂ∞è‰∏∫ <span class="arithmatex">\(k √ó k\)</span> ÁöÑÊúÄÂ§ßÊ±†ÂåñËæìÂá∫ÁöÑ‰∏≤ËÅîÔºåÂÖ∂‰∏≠  <span class="arithmatex">\(k = {1, 5, 9, 13}\)</span> ÔºåÊ≠•ÂπÖÁ≠â‰∫é 1„ÄÇ Âú®ËøôÁßçËÆæËÆ°‰∏ãÔºåÁõ∏ÂØπËæÉÂ§ßÁöÑ <span class="arithmatex">\(k √ó k\)</span> ÊúÄÂ§ßÊ±†ÂåñÊúâÊïàÂú∞Â¢ûÂä†‰∫Ü‰∏ªÂπ≤ÔºàbackboneÔºâÁâπÂæÅÁöÑÊÑüÂèóÈáé„ÄÇÊ∑ªÂä†ÊîπËøõÁâàSPPÊ®°ÂùóÂêéÔºåYOLOv3-608Âú®MS COCOÁâ©‰ΩìÊ£ÄÊµã‰ªªÂä°‰∏ä‰ª•0.5%ÁöÑÈ¢ùÂ§ñËÆ°ÁÆó‰∏∫‰ª£‰ª∑Â∞ÜAP50ÊèêÂçá‰∫Ü2.7%„ÄÇASPP [5] Ê®°ÂùóÂíåÊîπËøõÁöÑ SPP Ê®°ÂùóÂú®Êìç‰Ωú‰∏äÁöÑÂå∫Âà´‰∏ªË¶ÅÊòØÂéüÂßãÁöÑ <span class="arithmatex">\(k√ók\)</span> Ê†∏Â§ßÂ∞èÔºåÊúÄÂ§ßÊ±†ÂåñÁöÑÊ≠•ÂπÖÁ≠â‰∫é1 Âà∞Âá†‰∏™ <span class="arithmatex">\(3√ó3\)</span> Ê†∏Â§ßÂ∞èÔºåÊâ©Âº†ÊØîÔºàdilated ratioÔºâÁ≠â‰∫é kÔºåÂú®Êâ©Âº†Âç∑ÁßØÔºàdilated convolutionÔºâÊìç‰Ωú‰∏≠Ê≠•ÂπÖÁ≠â‰∫é 1„ÄÇRFBÊ®°ÂùóÊòØ‰ΩøÁî®Âá†‰∏™k√ókÊ†∏ÁöÑÊâ©Âº†Âç∑ÁßØÔºåÊâ©Âº†ÊØîÁ≠â‰∫ékÔºåÊ≠•ÂπÖÁ≠â‰∫é1Ôºå‰ª•Ëé∑ÂæóÊØîASPPÊõ¥ÂÖ®Èù¢ÁöÑÁ©∫Èó¥Ë¶ÜÁõñ„ÄÇRFB [47] ‰ªÖËä±Ë¥π 7% ÁöÑÈ¢ùÂ§ñÊé®ÁêÜÊó∂Èó¥Âç≥ÂèØÂØπ MS COCO ‰∏ä SSD ÁöÑ AP50 ÊèêÈ´ò 5.7%„ÄÇ</p>
<p>&emsp;The attention module that is often used in object detection is mainly divided into channel-wise attention and point-wise attention, and the representatives of these two attention models are Squeeze-and-Excitation (SE) [29] and Spatial Attention Module (SAM) [85], respectively. Although
SE module can improve the power of ResNet50 in the ImageNet image classification task 1% top-1 accuracy at the
cost of only increasing the computational effort by 2%, but
on a GPU usually it will increase the inference time by
about 10%, so it is more appropriate to be used in mobile
devices. 
But for SAM, it only needs to pay 0.1% extra calculation and it can improve ResNet50-SE 0.5% top-1 accuracy on the ImageNet image classification task. Best of all,
it does not affect the speed of inference on the GPU at all.</p>
<p>&emsp;ÁõÆÊ†áÊ£ÄÊµã‰∏≠ÁªèÂ∏∏‰ΩøÁî®ÁöÑÊ≥®ÊÑèÂäõÊ®°ÂùóÔºàattention moduleÔºâ‰∏ªË¶ÅÂàÜ‰∏∫ÈÄêÈÄöÈÅìÊ≥®ÊÑèÂäõÔºàchannel-wise attentionÔºâÂíåÈÄêÁÇπÊ≥®ÊÑèÂäõÔºàpoint-wise attentionÔºâÔºåËøô‰∏§ÁßçÊ≥®ÊÑèÂäõÊ®°ÂûãÁöÑ‰ª£Ë°®ÂàÜÂà´ÊòØSqueeze-and-ExcitationÔºàSEÔºâ[29]ÂíåSpatial Attention ModuleÔºàSAMÔºâ[85]„ÄÇËôΩÁÑ∂ SE Ê®°ÂùóÂèØ‰ª•Â∞Ü ResNet50 Âú® ImageNet ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÊèêÈ´ò 1% <span class="arithmatex">\(top-1\)</span> ÂáÜÁ°ÆÁéáÔºå‰ª£‰ª∑ÊòØÂè™Â¢ûÂä† 2% ÁöÑËÆ°ÁÆóÈáèÔºå‰ΩÜÂú® GPU ‰∏äÈÄöÂ∏∏‰ºöÂ¢ûÂä†Â§ßÁ∫¶ 10% ÁöÑÊé®ÁêÜÊó∂Èó¥Ôºå ÊâÄ‰ª•Êõ¥ÈÄÇÂêàÁî®Âú®ÁßªÂä®ËÆæÂ§á‰∏ä„ÄÇ‰ΩÜÂØπ‰∫é SAM Êù•ËØ¥ÔºåÂÆÉÂè™ÈúÄË¶ÅÈ¢ùÂ§ñ‰ªòÂá∫ 0.1% ÁöÑËÆ°ÁÆóÔºåÂ∞±ÂèØ‰ª•Â∞Ü ResNet50-SE Âú® ImageNet ÂõæÂÉèÂàÜÁ±ª‰ªªÂä°‰∏äÁöÑ <span class="arithmatex">\(top-1\)</span> ÂáÜÁ°ÆÁéáÊèêÈ´ò 0.5%„ÄÇ ÊúÄÈáçË¶ÅÁöÑÊòØÔºåÂÆÉ‰∏ç‰ºöÂΩ±Âìç GPU ‰∏äÁöÑÊé®ÁêÜÈÄüÂ∫¶„ÄÇ</p>
<p>&emsp;In terms of feature integration, the early practice is to use
skip connection [51] or hyper-column [22] to integrate low-
level physical feature to high-level semantic feature. Since
multi-scale prediction methods such as FPN have become
popular, many lightweight modules that integrate different
feature pyramid have been proposed. The modules of this
sort include SFAM [98], ASFF [48], and BiFPN [77]. The
main idea of SFAM is to use SE module to execute channel-
wise level re-weighting on multi-scale concatenated feature
maps. As for ASFF, it uses softmax as point-wise level re-
weighting and then adds feature maps of different scales.
In BiFPN, the multi-input weighted residual connections is
proposed to execute scale-wise level re-weighting, and then
add feature maps of different scales.</p>
<p>&emsp;Âú®ÁâπÂæÅÊï¥ÂêàÔºàfeature integrationÔºâÊñπÈù¢ÔºåÊó©ÊúüÁöÑÂÅöÊ≥ïÊòØ‰ΩøÁî®Ë∑≥ËøáËøûÊé•Ôºàskip connectionÔºâ[51]ÊàñË∂ÖÂàóÔºàhyper-columnÔºâ[22]Â∞Ü‰ΩéÁ∫ßÁâ©ÁêÜÁâπÂæÅÊï¥ÂêàÂà∞È´òÁ∫ßËØ≠‰πâÁâπÂæÅÔºàsemantic featureÔºâ„ÄÇ ÈöèÁùÄFPNÁ≠âÂ§öÊØî‰æãÔºàmulti-scaleÔºâÈ¢ÑÊµãÊñπÊ≥ïÁöÑÊµÅË°åÔºåËÆ∏Â§öÈõÜÊàê‰∏çÂêåÁâπÂæÅÈáëÂ≠óÂ°îÁöÑËΩªÈáèÁ∫ßÊ®°ÂùóË¢´ÊèêÂá∫„ÄÇ ËøôÁ±ªÊ®°ÂùóÂåÖÊã¨ SFAM [98]„ÄÅASFF [48] Âíå BiFPN [77]„ÄÇ SFAM ÁöÑ‰∏ªË¶ÅÊÄùÊÉ≥ÊòØ‰ΩøÁî® SE Ê®°ÂùóÂú®Â§öÊØî‰æãÁ∫ßËÅîÔºàmulti-scale concatenatedÔºâÁâπÂæÅÂõæ‰∏äÊâßË°åÈÄêÈÄöÈÅìÊ∞¥Âπ≥ÁöÑÈáçÊñ∞Âä†ÊùÉÔºàchannel-wise level re-weightingÔºâ„ÄÇ Ëá≥‰∫éASFFÔºåÂÆÉ‰ΩøÁî®softmax‰Ωú‰∏∫ÈÄêÁÇπÁ∫ßÈáçÂä†ÊùÉÔºàpoint-wise level re-weightingÔºâÔºåÁÑ∂ÂêéÊ∑ªÂä†‰∏çÂêåÊØî‰æãÁöÑÁâπÂæÅÂõæ„ÄÇ Âú® BiFPN ‰∏≠ÔºåÊèêÂá∫‰∫Ü‰ΩøÁî®Â§öËæìÂÖ•Âä†ÊùÉÊÆãÂ∑ÆËøûÊé•Ôºàmulti-input weighted residual connectionsÔºâÊù•ÊâßË°åÊåâÂ∞∫Â∫¶Á∫ßÂà´ÈáçÊñ∞Âä†ÊùÉÔºàscale-wise level re-weightingÔºâÔºåÁÑ∂ÂêéÊ∑ªÂä†‰∏çÂêåÂ∞∫Â∫¶ÁöÑÁâπÂæÅÂõæ„ÄÇ</p>
<p>&emsp;In the research of deep learning, some people put their
focus on searching for good activation function. A good
activation function can make the gradient more efficiently
propagated, and at the same time it will not cause too
much extra computational cost. In 2010, Nair and Hin-
ton [56] propose ReLU to substantially solve the gradient
vanish problem which is frequently encountered in tradi-
tional tanh and sigmoid activation function. Subsequently,
LReLU [54], PReLU [24], ReLU6 [28], Scaled Exponential
Linear Unit (SELU) [35], Swish [59], hard-Swish [27], and
Mish [55], etc., which are also used to solve the gradient
vanish problem, have been proposed. The main purpose of
LReLU and PReLU is to solve the problem that the gradi-
ent of ReLU is zero when the output is less than zero. As
for ReLU6 and hard-Swish, they are specially designed for
quantization networks. For self-normalizing a neural net-
work, the SELU activation function is proposed to satisfy
the goal. One thing to be noted is that both Swish and Mish
are continuously differentiable activation function.</p>
<p>&emsp;Âú®Ê∑±Â∫¶Â≠¶‰π†ÁöÑÁ†îÁ©∂‰∏≠ÔºåÊúâ‰∫∫ÊääÈáçÁÇπÊîæÂú®ÂØªÊâæÂ•ΩÁöÑÊøÄÊ¥ªÂáΩÊï∞‰∏ä„ÄÇ‰∏Ä‰∏™Â•ΩÁöÑÊøÄÊ¥ªÂáΩÊï∞ÂèØ‰ª•ËÆ©Ê¢ØÂ∫¶Êõ¥ÊúâÊïàÂú∞‰º†Êí≠ÔºåÂêåÊó∂‰∏ç‰ºöÈÄ†ÊàêÂ§™Â§öÈ¢ùÂ§ñÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇ2010 Âπ¥ÔºåNair Âíå Hinton [56] ÊèêÂá∫ ReLU Êù•ÂÆûË¥®ÊÄßÂú∞Ëß£ÂÜ≥‰º†Áªü tanh Âíå sigmoid ÊøÄÊ¥ªÂáΩÊï∞‰∏≠ÁªèÂ∏∏ÈÅáÂà∞ÁöÑÊ¢ØÂ∫¶Ê∂àÂ§±Ôºàgradient vanishÔºâÈóÆÈ¢ò„ÄÇÈöèÂêéÔºåLReLU [54]„ÄÅPReLU [24]„ÄÅReLU6 [28]„ÄÅScaled Exponential Linear Unit (SELU) [35]„ÄÅSwish [59]„ÄÅhard-Swish [27] Âíå Mish [55] Á≠âË¢´ÊèêÂá∫Ôºå‰πüÁî®‰∫éËß£ÂÜ≥Ê¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢ò„ÄÇLReLUÂíåPReLUÁöÑ‰∏ªË¶ÅÁõÆÁöÑÊòØËß£ÂÜ≥ËæìÂá∫Â∞è‰∫éÈõ∂Êó∂ReLUÊ¢ØÂ∫¶‰∏∫Èõ∂ÁöÑÈóÆÈ¢ò„ÄÇËá≥‰∫é ReLU6 Âíå hard-SwishÔºåÂÆÉ‰ª¨ÊòØ‰∏ìÈó®‰∏∫ÈáèÂåñÔºàquantizationÔºâÁΩëÁªúËÆæËÆ°ÁöÑ„ÄÇ‰∏∫‰∫ÜËá™ÂΩí‰∏ÄÂåñÔºàself-normalizeÔºâÁ•ûÁªèÁΩëÁªúÔºåSELU ÊøÄÊ¥ªÂáΩÊï∞Ë¢´ÊèêÂá∫‰ª•ÂÆûÁé∞ËØ•ÁõÆÊ†á„ÄÇÈúÄË¶ÅÊ≥®ÊÑèÁöÑ‰∏Ä‰ª∂‰∫ãÊòØ Swish Âíå Mish ÈÉΩÊòØËøûÁª≠ÂèØÂæÆÔºàcontinuously differentiableÔºâÁöÑÊøÄÊ¥ªÂáΩÊï∞„ÄÇ</p>
<p>&emsp;The post-processing method commonly used in deep-
learning-based object detection is NMS, which can be used
to filter those BBoxes that badly predict the same ob-
ject, and only retain the candidate BBoxes with higher re-
sponse. The way NMS tries to improve is consistent with
the method of optimizing an objective function. The orig-
inal method proposed by NMS does not consider the con-
text information, so Girshick et al. [19] added classification
confidence score in R-CNN as a reference, and according to
the order of confidence score, greedy NMS was performed
in the order of high score to low score. As for soft NMS [1],
it considers the problem that the occlusion of an object may
cause the degradation of confidence score in greedy NMS
with IoU score. The DIoU NMS [99] developers way of
thinking is to add the information of the center point dis-
tance to the BBox screening process on the basis of soft
NMS. It is worth mentioning that, since none of above post-
processing methods directly refer to the captured image fea-
tures, post-processing is no longer required in the subse-
quent development of an anchor-free method.</p>
<p>&emsp;Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁâ©‰ΩìÊ£ÄÊµãÂ∏∏Áî®ÁöÑÂêéÂ§ÑÁêÜÔºàpost-processingÔºâÊñπÊ≥ïÊòØNMSÔºåÂÆÉÂèØ‰ª•Áî®Êù•ËøáÊª§ÈÇ£‰∫õÂØπÂêå‰∏ÄÁâ©‰ΩìÈ¢ÑÊµã‰∏çÂ•ΩÁöÑBBoxÔºåÂè™‰øùÁïôÂìçÂ∫îËæÉÈ´òÁöÑÂÄôÈÄâBBox„ÄÇNMS Â∞ùËØïÊîπËøõÁöÑÊñπÂºè‰∏é‰ºòÂåñÁõÆÊ†áÂáΩÊï∞ÁöÑÊñπÊ≥ï‰∏ÄËá¥„ÄÇ NMS ÊèêÂá∫ÁöÑÂéüÂßãÊñπÊ≥ïÊ≤°ÊúâËÄÉËôë‰∏ä‰∏ãÊñáÔºàcontextÔºâ‰ø°ÊÅØÔºåÂõ†Ê≠§ Girshick Á≠â‰∫∫ [19] Âú® R-CNN ‰∏≠Ê∑ªÂä†‰∫ÜÂàÜÁ±ªÁΩÆ‰ø°Â∫¶Ôºàclassification confidence scoreÔºâÂàÜÊï∞‰Ωú‰∏∫ÂèÇËÄÉÔºåÂπ∂ÊåâÁÖßÁΩÆ‰ø°Â∫¶ÂæóÂàÜÁöÑÈ°∫Â∫èÔºå‰ªéÈ´òÂàÜÂà∞‰ΩéÂàÜÁöÑÈ°∫Â∫èËøõË°åË¥™Â©™ÔºàgreedyÔºâNMS„ÄÇÂØπ‰∫é soft NMS [1]ÔºåÂÆÉËÄÉËôë‰∫ÜÂú®ÂÖ∑Êúâ IoU ÂàÜÊï∞ÁöÑ greedy NMS ‰∏≠ÂØπË±°ÁöÑÈÅÆÊå°ÔºàocclusionÔºâÂèØËÉΩÂØºËá¥ÁΩÆ‰ø°Â∫¶‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇ DIoU NMS [99] ÂºÄÂèëËÄÖÁöÑÊÄùË∑ØÊòØÂú®ËΩØ NMS ÁöÑÂü∫Á°Ä‰∏äÔºåÂú® BBox Á≠õÈÄâÔºàscreenÔºâËøáÁ®ã‰∏≠Âä†ÂÖ•‰∏≠ÂøÉÁÇπË∑ùÁ¶ª‰ø°ÊÅØ„ÄÇ ÂÄºÂæó‰∏ÄÊèêÁöÑÊòØÔºåÁî±‰∫é‰∏äËø∞ÂêéÂ§ÑÁêÜÊñπÊ≥ïÈÉΩÊ≤°ÊúâÁõ¥Êé•ÂèÇËÄÉÊçïËé∑ÁöÑÂõæÂÉèÁâπÂæÅÔºåÂõ†Ê≠§Âú®ÂêéÁª≠ÂºÄÂèëanchor-freeÊñπÊ≥ïÊó∂‰∏çÂÜçÈúÄË¶ÅËøõË°åÂêéÂ§ÑÁêÜ„ÄÇ</p>
<h2 id="3-methodology">3. Methodology ÊñπÊ≥ïËÆ∫</h2>
<p>&emsp;The basic aim is fast operating speed of neural network,
in production systems and optimization for parallel compu-
tations, rather than the low computation volume theoreti-
cal indicator (BFLOP). We present two options of real-time
neural networks:</p>
<p>&emsp;Âü∫Êú¨ÁõÆÊ†áÊòØÁ•ûÁªèÁΩëÁªúÂú®Áîü‰∫ßÁ≥ªÁªü‰∏≠ÁöÑÂø´ÈÄüËøêË°åÂíåÂπ∂Ë°åËÆ°ÁÆóÔºàparallel computationÔºâÁöÑ‰ºòÂåñÔºåËÄå‰∏çÊòØ‰ΩéËÆ°ÁÆóÈáèÁêÜËÆ∫ÊåáÊ†áÔºàlow computation volume theoretical indicatorÔºâÔºàBFLOPÔºâ„ÄÇ Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏§ÁßçÂÆûÊó∂Á•ûÁªèÁΩëÁªúÈÄâÈ°πÔºö</p>
<ul>
<li>
<p>For GPU we use a small number of groups (1 - 8) in
convolutional layers: CSPResNeXt50 / CSPDarknet53 </p>
</li>
<li>
<p>ÂØπ‰∫é GPUÔºåÊàë‰ª¨Âú®Âç∑ÁßØÂ±Ç‰∏≠‰ΩøÁî®Â∞ëÈáèÁªÑ (1 - 8)ÔºöCSPResNeXt50 / CSPDarknet53</p>
</li>
<li>
<p>For VPU - we use grouped-convolution, but we refrain from using Squeeze-and-excitement (SE) blocks specifically this includes the following models:
EfficientNet-lite / MixNet [76] / GhostNet [21] / Mo-bileNetV3</p>
</li>
<li>ÂØπ‰∫é VPU - Êàë‰ª¨‰ΩøÁî®ÂàÜÁªÑÂç∑ÁßØÔºå‰ΩÜÊàë‰ª¨ÈÅøÂÖç‰ΩøÁî® Squeeze-and-excitement (SE) Âùó , - ÂÖ∑‰ΩìÂåÖÊã¨Ê®°ÂûãÔºöEfficientNet-lite / MixNet [76] / GhostNet [21] / MobileNetV3</li>
</ul>
<h3 id="31-selection-of-architecture">3.1. Selection of architecture Êû∂ÊûÑÈÄâÊã©</h3>
<p>&emsp;Our objective is to find the optimal balance among the in-
put network resolution, the convolutional layer number, the 
parameter number <span class="arithmatex">\((filter size^2 * filters * channel / groups)\)</span> ,
and the number of layer outputs (filters). For instance, our
numerous studies demonstrate that the CSPResNext50 is
considerably better compared to CSPDarknet53 in terms
of object classification on the ILSVRC2012 (ImageNet)
dataset [10]. However, conversely, the CSPDarknet53 is better compared to CSPResNext50 in terms of detecting ob-
jects on the MS COCO dataset [46].</p>
<p>&emsp;Êàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂú®ËæìÂÖ•ÁΩëÁªúÂàÜËæ®Áéá„ÄÅÂç∑ÁßØÂ±ÇÊï∞„ÄÅÂèÇÊï∞Êï∞Èáè <span class="arithmatex">\((filter size^2 * filters * channel / groups)\)</span> ÂíåÂ±ÇËæìÂá∫Êï∞ÔºàËøáÊª§Âô®Ôºâ‰πãÈó¥ÊâæÂà∞ÊúÄ‰Ω≥Âπ≥Ë°°„ÄÇ ‰æãÂ¶ÇÔºåÊàë‰ª¨ÁöÑÂ§ßÈáèÁ†îÁ©∂Ë°®ÊòéÔºåÂú® ILSVRC2012 (ImageNet) Êï∞ÊçÆÈõÜ [10] ‰∏äÁöÑÂØπË±°ÂàÜÁ±ªÊñπÈù¢ÔºåCSPResNext50 ‰∏é CSPDarknet53 Áõ∏ÊØîË¶ÅÂ•ΩÂæóÂ§ö„ÄÇ ÁÑ∂ËÄåÔºåÁõ∏ÂèçÔºåÂú® MS COCO Êï∞ÊçÆÈõÜ‰∏äËøõË°åÁõÆÊ†áÊ£ÄÊµãÔºåCSPDarknet53 ÊØî CSPResNext50 Êõ¥Â•Ω [46]„ÄÇ</p>
<p>&emsp;The next objective is to select additional blocks for increasing the receptive field and the best method of parameter aggregation from different backbone levels for different
detector levels: e.g. FPN, PAN, ASFF, BiFPN.</p>
<p>&emsp;‰∏ã‰∏Ä‰∏™ÁõÆÊ†áÊòØÈÄâÊã©È¢ùÂ§ñÁöÑÂùóÊù•Â¢ûÂä†ÊÑüÂèóÈáéÔºå‰ª•Âèä‰ªé‰∏çÂêåÁöÑ‰∏ªÂπ≤Á∫ßÂà´‰∏∫‰∏çÂêåÁöÑÊ£ÄÊµãÂô®Á∫ßÂà´ÈÄâÊã©ÂèÇÊï∞ËÅöÂêàÔºàparameter aggregationÔºâÁöÑÊúÄ‰Ω≥ÊñπÊ≥ïÔºö‰æãÂ¶Ç FPN„ÄÅPAN„ÄÅASFF„ÄÅBiFPN„ÄÇ</p>
<p>&emsp;A reference model which is optimal for classification is
not always optimal for a detector. In contrast to the classifier, the detector requires the following:</p>
<p>&emsp;ÂØπ‰∫éÂàÜÁ±ªËÄåË®ÄÊúÄ‰Ω≥ÁöÑÂèÇËÄÉÊ®°ÂûãÂØπ‰∫éÊ£ÄÊµãÂô®ËÄåË®ÄÂπ∂‰∏çÊÄªÊòØÊúÄ‰Ω≥ÁöÑ„ÄÇ ‰∏éÂàÜÁ±ªÂô®Áõ∏ÊØîÔºåÊ£ÄÊµãÂô®ÈúÄË¶Å‰ª•‰∏ãÂÜÖÂÆπÔºö</p>
<ul>
<li>
<p>Higher input network size (resolution) ‚Äì for detecting
multiple small-sized objects</p>
</li>
<li>
<p>Êõ¥È´òÁöÑËæìÂÖ•ÁΩëÁªúÂ∞∫ÂØ∏ÔºàÂàÜËæ®ÁéáÔºâ‚Äî‚Äî Áî®‰∫éÊ£ÄÊµãÂ§ö‰∏™Â∞èÂ∞∫ÂØ∏Áâ©‰Ωì</p>
</li>
<li>
<p>More layers ‚Äì for a higher receptive field to cover the
increased size of input network</p>
</li>
<li>
<p>Êõ¥Â§öÂ±Ç‚Äî‚ÄîÁî®‰∫éÊõ¥È´òÁöÑÊÑüÂèóÈáé‰ª•Ë¶ÜÁõñÂ¢ûÂä†ÁöÑËæìÂÖ•ÁΩëÁªúÂ§ßÂ∞è</p>
</li>
<li>
<p>More parameters ‚Äì for greater capacity of a model to
detect multiple objects of different sizes in a single im-
age</p>
</li>
<li>
<p>Êõ¥Â§öÂèÇÊï∞‚Äî‚Äî‰ΩøÊ®°ÂûãÊúâÊõ¥Â§ßÁöÑËÉΩÂäõÂú®Âçï‰∏™ÂõæÂÉè‰∏≠Ê£ÄÊµãÂ§ö‰∏™‰∏çÂêåÂ§ßÂ∞èÁöÑÂØπË±°</p>
</li>
</ul>
<p><img src = "04_yolo_imgs/table01.png"></p>
<p><span class="arithmatex">\(\text { Ë°® 1: Áî®‰∫éÂõæÂÉèÂàÜÁ±ªÁöÑÁ•ûÁªèÁΩëÁªúÂèÇÊï∞„ÄÇ }\)</span></p>
<p>&emsp;Hypothetically speaking, we can assume that a model
with a larger receptive field size (with a larger number of
convolutional layers <span class="arithmatex">\(3 √ó 3\)</span> ) and a larger number of parameters should be selected as the backbone. Table 1 shows the
information of CSPResNeXt50, CSPDarknet53, and EfficientNet B3. The CSPResNext50 contains only 16 convolutional layers <span class="arithmatex">\(3 √ó 3\)</span> , a <span class="arithmatex">\(425 √ó 425\)</span> receptive field and 20.6
M parameters, while CSPDarknet53 contains 29 convolu-
tional layers <span class="arithmatex">\(3 √ó 3\)</span> , a <span class="arithmatex">\(725 √ó 725\)</span> receptive field and 27.6
M parameters. This theoretical justification, together with
our numerous experiments, show that CSPDarknet53 neural network is the optimal model of the two as the backbone
for a detector.</p>
<p>&emsp;ÂÅáËÆæÂú∞ËØ¥ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅáËÆæÂ∫îËØ•ÈÄâÊã©ÂÖ∑ÊúâÊõ¥Â§ßÊÑüÂèóÈáéÂ§ßÂ∞èÔºàÂÖ∑ÊúâÊõ¥Â§öÂç∑ÁßØÂ±Ç 3 √ó 3ÔºâÂíåÊõ¥Â§öÂèÇÊï∞ÁöÑÊ®°Âûã‰Ωú‰∏∫‰∏ªÂπ≤„ÄÇ Ë°® 1 ÊòæÁ§∫‰∫Ü CSPResNeXt50„ÄÅCSPDarknet53 Âíå EfficientNet B3 ÁöÑ‰ø°ÊÅØ„ÄÇ CSPResNext50 ‰ªÖÂåÖÂê´ 16 ‰∏™ 3 √ó 3 Âç∑ÁßØÂ±Ç„ÄÅ425 √ó 425 ÊÑüÂèóÈáéÂíå 20.6 M ÂèÇÊï∞ÔºåËÄå CSPDarknet53 ÂåÖÂê´ 29 ‰∏™Âç∑ÁßØÂ±Ç 3 √ó 3„ÄÅ725 √ó 725 ÊÑüÂèóÈáéÂíå 27.6 M ÂèÇÊï∞„ÄÇ Ëøô‰∏™ÁêÜËÆ∫ËØÅÊòéÔºåÂä†‰∏äÊàë‰ª¨ÁöÑÂ§ßÈáèÂÆûÈ™åÔºåË°®Êòé CSPDarknet53 Á•ûÁªèÁΩëÁªúÊòØ‰∏§ËÄÖÁöÑÊúÄ‰Ω≥Ê®°ÂûãÔºå‰Ωú‰∏∫Ê£ÄÊµãÂô®ÁöÑ‰∏ªÂπ≤„ÄÇ</p>
<p>&emsp;The influence of the receptive field with different sizes is
summarized as follows:</p>
<p>&emsp;‰∏çÂêåÂ§ßÂ∞èÁöÑÊÑüÂèóÈáéÁöÑÂΩ±ÂìçÊÄªÁªìÂ¶Ç‰∏ã:</p>
<ul>
<li>Up to the object size - allows viewing the entire object</li>
<li>ÊúÄÂ§öÂà∞ÂØπË±°Â§ßÂ∞è ‚Äì ÂÖÅËÆ∏Êü•ÁúãÊï¥‰∏™ÂØπË±°</li>
<li>Up to network size - allows viewing the context around the object</li>
<li>ÊúÄÂ§öÁΩëÁªúÂ§ßÂ∞è ‚Äì ÂÖÅËÆ∏Êü•ÁúãÂØπË±°Âë®Âõ¥ÁöÑ‰∏ä‰∏ãÊñá</li>
<li>Exceeding the network size - increases the number of connections between the image point and the final activation</li>
<li>Ë∂ÖËøáÁΩëÁªúÂ§ßÂ∞è ‚Äì Â¢ûÂä†ÂõæÂÉèÁÇπÂíåÊúÄÁªàÊøÄÊ¥ª‰πãÈó¥ÁöÑËøûÊé•Êï∞</li>
</ul>
<p>&emsp; We add the SPP block over the CSPDarknet53, since it 
significantly increases the receptive field, separates out 
the most significant context features and causes almost no reduction of the network operation speed. We use PANet as
the method of parameter aggregation from different backbone levels for different detector levels, instead of the FPN
used in YOLOv3.</p>
<p>&emsp;Êàë‰ª¨Âú® CSPDarknet53 ‰∏äÊ∑ªÂä†‰∫Ü SPP ÂùóÔºåÂõ†‰∏∫ÂÆÉÊòæÁùÄÂ¢ûÂä†‰∫ÜÊÑüÂèóÈáéÔºåÂàÜÁ¶ªÂá∫ÊúÄÈáçË¶ÅÁöÑ‰∏ä‰∏ãÊñáÁâπÂæÅÔºåÂπ∂‰∏îÂá†‰πé‰∏ç‰ºöÈôç‰ΩéÁΩëÁªúËøêË°åÈÄüÂ∫¶„ÄÇ Êàë‰ª¨‰ΩøÁî® PANet ‰Ωú‰∏∫Êù•Ëá™‰∏çÂêå‰∏ªÂπ≤Á∫ßÂà´ÁöÑÂØπ‰∏çÂêåÊ£ÄÊµãÂô®Á∫ßÂà´ÁöÑÂèÇÊï∞ËÅöÂêàÊñπÊ≥ïÔºåËÄå‰∏çÊòØ YOLOv3 ‰∏≠‰ΩøÁî®ÁöÑ FPN„ÄÇ</p>
<p>&emsp;Finally, we choose CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv3
(anchor based) head as the architecture of YOLOv4.</p>
<p>&emsp;ÊúÄÂêéÔºåÊàë‰ª¨ÈÄâÊã© CSPDarknet53 ‰∏ªÂπ≤„ÄÅSPP ÈôÑÂä†Ê®°Âùó„ÄÅPANet Ë∑ØÂæÑËÅöÂêàÈ¢àÈÉ®Âíå YOLOv3Ôºà(anchor basedÔºâÂ§¥ÈÉ®‰Ωú‰∏∫ YOLOv4 ÁöÑÊû∂ÊûÑ„ÄÇ</p>
<p>&emsp;n the future we plan to expand significantly the content
of Bag of Freebies (BoF) for the detector, which theoreti-
cally can address some problems and increase the detector
accuracy, and sequentially check the influence of each fea-
ture in an experimental fashion.</p>
<p>&emsp;Êú™Êù•Êàë‰ª¨ËÆ°ÂàíÂ§ßÂäõÊâ©Â±ïÊ£ÄÊµãÂô®ÁöÑBag of Freebies (BoF) ÂÜÖÂÆπÔºåÁêÜËÆ∫‰∏äÂèØ‰ª•Ëß£ÂÜ≥‰∏Ä‰∫õÈóÆÈ¢òÂπ∂ÊèêÈ´òÊ£ÄÊµãÂô®ÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂‰ª•ÂÆûÈ™åÊñπÂºè‰æùÊ¨°Ê£ÄÊü•ÊØè‰∏™ÁâπÂæÅÁöÑÂΩ±Âìç„ÄÇ</p>
<p>&emsp;We do not use Cross-GPU Batch Normalization (CGBN
or SyncBN) or expensive specialized devices. This al-
lows anyone to reproduce our state-of-the-art outcomes on
a conventional graphic processor e.g. GTX 1080Ti or RTX
2080Ti.</p>
<p>&emsp;Êàë‰ª¨‰∏ç‰ΩøÁî®Ë∑® GPUÔºàCross-GPUÔºâÊâπÈáèÂΩí‰∏ÄÂåñÔºàCGBN Êàñ SyncBNÔºâÊàñÊòÇË¥µÁöÑ‰∏ìÁî®ËÆæÂ§á„ÄÇ ËøôÂÖÅËÆ∏‰ªª‰Ωï‰∫∫Âú®‰º†ÁªüÂõæÂΩ¢Â§ÑÁêÜÂô®‰∏äÈáçÁé∞Êàë‰ª¨ÊúÄÂÖàËøõÁöÑÁªìÊûúÔºå‰æãÂ¶Ç GTX 1080Ti Êàñ RTX 2080Ti„ÄÇ</p>
<h3 id="32-selection-of-bof-and-bos">3.2. Selection of BoF and BoS</h3>
<p>&emsp;For improving the object detection training, a CNN usu-
ally uses the following:</p>
<p>&emsp;‰∏∫‰∫ÜÊîπËøõÁõÆÊ†áÊ£ÄÊµãËÆ≠ÁªÉÔºåCNN ÈÄöÂ∏∏‰ΩøÁî®‰ª•‰∏ãÂÜÖÂÆπÔºö</p>
<ul>
<li>Activations: ReLU, leaky-ReLU, parametric-ReLU,
ReLU6, SELU, Swish, or Mish</li>
<li>ÊøÄÊ¥ªÔºöReLU„ÄÅleaky-ReLU„ÄÅÂèÇÊï∞Âåñ ReLUÔºàparametric-ReLUÔºâ„ÄÅReLU6„ÄÅSELU„ÄÅSwish Êàñ Mish</li>
<li>Bounding box regression loss: MSE, IoU, GIoU,CIoU, DIoU</li>
<li>ËæπÁïåÊ°ÜÂõûÂΩíÊçüÂ§±ÔºöMSE„ÄÅIoU„ÄÅGIoU„ÄÅCIoU„ÄÅDIoU</li>
<li>Data augmentation: CutOut, MixUp, CutMix</li>
<li>Êï∞ÊçÆÂ¢ûÂº∫ÔºöCutOut„ÄÅMixUp„ÄÅCutMix</li>
<li>Regularization method: DropOut, DropPath [36],Spatial DropOut [79], or DropBlock</li>
<li>Ê≠£ÂàôÂåñÊñπÊ≥ïÔºöDropOut„ÄÅDropPath [36]„ÄÅSpatial DropOut [79] Êàñ DropBlock</li>
<li>Normalization of the network activations by their mean and variance: Batch Normalization (BN) [32],
Cross-GPU Batch Normalization (CGBN or SyncBN)[93], Filter Response Normalization (FRN) [70], or Cross-Iteration Batch Normalization (CBN) [89]</li>
<li>ÈÄöËøáÂùáÂÄºÂíåÊñπÂ∑ÆÂØπÁΩëÁªúÊøÄÊ¥ªËøõË°åÂΩí‰∏ÄÂåñÔºöÊâπÂΩí‰∏ÄÂåñ (BN) [32]„ÄÅCross-GPU ÊâπÂΩí‰∏ÄÂåñÔºàCGBN Êàñ SyncBNÔºâ[93]„ÄÅÊª§Ê≥¢Âô®ÂìçÂ∫îÔºàFilter ResponseÔºâÂΩí‰∏ÄÂåñ (FRN) [70] Êàñ‰∫§ÂèâËø≠‰ª£ÔºàCross-IterationÔºâÊâπÂΩí‰∏ÄÂåñ (CBN) [89]</li>
<li>Skip-connections: Residual connections, Weighted
residual connections, Multi-input weighted residual
connections, or Cross stage partial connections (CSP)</li>
<li>Ë∑≥ËøáËøûÊé•ÔºàSkip-connectionsÔºâÔºöÊÆãÂ∑ÆËøûÊé•ÔºàResidual connectionÔºâ„ÄÅÂä†ÊùÉÊÆãÂ∑ÆËøûÊé•„ÄÅÂ§öËæìÂÖ•ÔºàMulti-inputÔºâÂä†ÊùÉÊÆãÂ∑ÆËøûÊé•ÊàñË∑®Èò∂ÊÆµÈÉ®ÂàÜËøûÊé•ÔºàCross stage partial connectionsÔºâ (CSP)</li>
</ul>
<p>&emsp;As for training activation function, since PReLU and
SELU are more difficult to train, and ReLU6 is specifically
designed for quantization network, we therefore remove the
above activation functions from the candidate list. In the
method of reqularization, the people who published Drop-
Block have compared their method with other methods in
detail, and their regularization method has won a lot. There-
fore, we did not hesitate to choose DropBlock as our reg-
ularization method. As for the selection of normalization
method, since we focus on a training strategy that uses only
one GPU, syncBN is not considered.</p>
<p>&emsp;Ëá≥‰∫éËÆ≠ÁªÉÊøÄÊ¥ªÂáΩÊï∞ÔºåÁî±‰∫é PReLU Âíå SELU Êõ¥ÈöæËÆ≠ÁªÉÔºåËÄå ReLU6 ÊòØ‰∏ìÈó®‰∏∫ÈáèÂåñÔºàquantizationÔºâÁΩëÁªúËÆæËÆ°ÁöÑÔºåÂõ†Ê≠§Êàë‰ª¨‰ªéÂÄôÈÄâÂàóË°®‰∏≠Âà†Èô§‰∫Ü‰∏äËø∞ÊøÄÊ¥ªÂáΩÊï∞„ÄÇ Âú®Ê≠£ÂàôÂåñÁöÑÊñπÊ≥ï‰∏äÔºåÂèëË°®Drop-BlockÁöÑ‰∫∫ËØ¶ÁªÜÂØπÊØî‰∫Ü‰ªñ‰ª¨ÁöÑÊñπÊ≥ïÂíåÂÖ∂‰ªñÊñπÊ≥ïÔºå‰ªñ‰ª¨ÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ïËµ¢Âæó‰∫ÜÂæàÂ§ö„ÄÇ Âõ†Ê≠§ÔºåÊàë‰ª¨ÊØ´‰∏çÁäπË±´Âú∞ÈÄâÊã© DropBlock ‰Ωú‰∏∫Êàë‰ª¨ÁöÑÊ≠£ÂàôÂåñÊñπÊ≥ï„ÄÇ Ëá≥‰∫éÂΩí‰∏ÄÂåñÊñπÊ≥ïÁöÑÈÄâÊã©ÔºåÁî±‰∫éÊàë‰ª¨‰∏ìÊ≥®‰∫é‰ªÖ‰ΩøÁî®‰∏Ä‰∏™ GPU ÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÂõ†Ê≠§Ê≤°ÊúâËÄÉËôë syncBN„ÄÇ</p>
<h3 id="33-additional-improvements">3.3. Additional improvements</h3>
<p>&emsp;In order to make the designed detector more suitable for
training on single GPU, we made additional design and im-
provement as follows:</p>
<p>&emsp;‰∏∫‰∫Ü‰ΩøÊâÄËÆæËÆ°ÁöÑÊ£ÄÊµãÂô®Êõ¥ÈÄÇÂêàÂú®Âçï‰∏ÄGPU‰∏äËÆ≠ÁªÉÔºåÊàë‰ª¨ÂÅö‰∫ÜÂ¶Ç‰∏ãÈ¢ùÂ§ñÁöÑËÆæËÆ°ÂíåÊîπËøõ:</p>
<ul>
<li>We introduce a new method of data augmentation Mosaic, and Self-Adversarial Training (SA T)</li>
<li>Êàë‰ª¨ÂºïÂÖ•‰∫ÜÊï∞ÊçÆÂ¢ûÂº∫È©¨ËµõÂÖãÔºàMosaicÔºâÂíåËá™ÊàëÂØπÊäóËÆ≠ÁªÉ (SAT) ÁöÑÊñ∞ÊñπÊ≥ï</li>
<li>We select optimal hyper-parameters while applying genetic algorithms</li>
<li>Êàë‰ª¨Âú®Â∫îÁî®ÈÅó‰º†ÁÆóÊ≥ïÔºàgenetic algorithmÔºâÁöÑÂêåÊó∂ÈÄâÊã©ÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞</li>
<li>We modify some exsiting methods to make our design suitble for efficient training and detection - modified
SAM, modified PAN, and Cross mini-Batch Normalization (CmBN)</li>
<li>Êàë‰ª¨‰øÆÊîπ‰∫Ü‰∏Ä‰∫õÁé∞ÊúâÁöÑÊñπÊ≥ïÔºå‰ΩøÊàë‰ª¨ÁöÑËÆæËÆ°ÈÄÇÂêàÈ´òÊïàÁöÑËÆ≠ÁªÉÂíåÊ£ÄÊµã‚Äî‚Äî‰øÆÊîπÁöÑ SAM„ÄÅ‰øÆÊîπÁöÑ PAN Âíå‰∫§ÂèâÂ∞èÊâπÈáèÂΩí‰∏ÄÂåñ (CmBN)</li>
</ul>
<p>&emsp;Mosaic represents a new data augmentation method that
mixes 4 training images. Thus 4 different contexts are mixed, while CutMix mixes only 2 input images. This al-
lows detection of objects outside their normal context. In
addition, batch normalization calculates activation statistics
from 4 different images on each layer. This significantly
reduces the need for a large mini-batch size.</p>
<p>&emsp;Mosaic ‰ª£Ë°®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ïÔºåÂÆÉÊ∑∑Âêà‰∫Ü 4 ‰∏™ËÆ≠ÁªÉÂõæÂÉè„ÄÇ Âõ†Ê≠§Ê∑∑Âêà‰∫Ü 4 ‰∏™‰∏çÂêåÁöÑ‰∏ä‰∏ãÊñáÔºåËÄå CutMix ‰ªÖÊ∑∑Âêà‰∫Ü 2 ‰∏™ËæìÂÖ•ÂõæÂÉè„ÄÇ ËøôÂÖÅËÆ∏Ê£ÄÊµãÊ≠£Â∏∏‰∏ä‰∏ãÊñá‰πãÂ§ñÁöÑÂØπË±°„ÄÇ Ê≠§Â§ñÔºåÊâπÈáèÂΩí‰∏ÄÂåñËÆ°ÁÆóÊØèÂ±Ç 4 ‰∏™‰∏çÂêåÂõæÂÉèÁöÑÊøÄÊ¥ªÁªüËÆ°Êï∞ÊçÆ„ÄÇ ËøôÊòæÁùÄÂáèÂ∞ë‰∫ÜÂØπÂ§ßÂ∞∫ÂØ∏ mini-batch ÁöÑÈúÄÊ±Ç„ÄÇ</p>
<p><img src = "04_yolo_imgs/ÂõæÁâá03.png"></p>
<p>Figure 3: Mosaic represents a new method of data augmentation.</p>
<p>Âõæ3:È©¨ËµõÂÖãË°®Á§∫‰∏ÄÁßçÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ</p>
<p>&emsp;Self-Adversarial Training (SAT) also represents a new
data augmentation technique that operates in 2 forward
backward stages. In the 1st stage the neural network alters
the original image instead of the network weights. In this
way the neural network executes an adversarial attack on it-
self, altering the original image to create the deception that
there is no desired object on the image. In the 2nd stage, the
neural network is trained to detect an object on this modified
image in the normal way.</p>
<p>&emsp; Ëá™ÊàëÂØπÊäóËÆ≠ÁªÉ (SAT) ‰πü‰ª£Ë°®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÔºåÂÆÉÂú® 2 ‰∏™ÂâçÂêëÂíåÂêéÂêëÈò∂ÊÆµ‰∏≠ËøêË°å„ÄÇ Âú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÁ•ûÁªèÁΩëÁªúÊîπÂèòÂéüÂßãÂõæÂÉèËÄå‰∏çÊòØÁΩëÁªúÊùÉÈáç„ÄÇ
ÈÄöËøáËøôÁßçÊñπÂºèÔºåÁ•ûÁªèÁΩëÁªúÂØπËá™Ë∫´ÊâßË°åÂØπÊäóÊÄßÊîªÂáªÔºåÊîπÂèòÂéüÂßãÂõæÂÉè ‰ª•Âà∂ÈÄ†ÂõæÂÉè‰∏äÊ≤°ÊúâÊâÄÈúÄÂØπË±°ÁöÑÊ¨∫È™óÔºàdeceptionÔºâ„ÄÇ Âú®Á¨¨‰∫åÈò∂ÊÆµÔºåËÆ≠ÁªÉÁ•ûÁªèÁΩëÁªú‰ª•Ê≠£Â∏∏ÊñπÂºèÊ£ÄÊµãÊ≠§‰øÆÊîπÂõæÂÉè‰∏äÁöÑÂØπË±°„ÄÇ</p>
<p><img src = "04_yolo_imgs/ÂõæÁâá04.png"></p>
<p>Figure 4: Cross mini-Batch Normalization.</p>
<p>Âõæ 4Ôºö‰∫§ÂèâÂ∞èÊâπÈáèÂΩí‰∏ÄÂåñ„ÄÇ</p>
<p>&emsp;CmBN represents a CBN modified version, as shown
in Figure 4, defined as Cross mini-Batch Normalization
(CmBN). This collects statistics only between mini-batches
within a single batch.</p>
<p>&emsp; CmBN Ë°®Á§∫ CBN ‰øÆÊîπÁâàÊú¨ÔºåÂ¶ÇÂõæ 4 ÊâÄÁ§∫ÔºåÂÆö‰πâ‰∏∫‰∫§ÂèâÂ∞èÊâπÈáèÂΩí‰∏ÄÂåñ (CmBN)„ÄÇ Ëøô‰ªÖÂú®Âçï‰∏™ÊâπÊ¨°ÂÜÖÁöÑÂ∞èÊâπÊ¨°‰πãÈó¥Êî∂ÈõÜÁªüËÆ°‰ø°ÊÅØ„ÄÇ</p>
<p>&emsp; We modify SAM from spatial-wise attention to point-
wise attention, and replace shortcut connection of PAN to
concatenation, as shown in Figure 5 and Figure 6, respec-
tively.
&emsp; Êàë‰ª¨Â∞Ü SAM ‰ªéÊåâÁ©∫Èó¥Ê≥®ÊÑèÔºàspatial-wise attentionÔºâ‰øÆÊîπ‰∏∫ÈÄêÁÇπÊ≥®ÊÑèÔºàpoint-wise attentionÔºâÔºåÂπ∂Â∞Ü PAN ÁöÑÂø´Êç∑ËøûÊé•Ôºàshortcut connectionÔºâÊõøÊç¢‰∏∫‰∏≤ËÅîÔºàconcatenationÔºâÔºåÂàÜÂà´Â¶ÇÂõæ 5 ÂíåÂõæ 6 ÊâÄÁ§∫„ÄÇ</p>
<p><img src = "04_yolo_imgs/ÂõæÁâá05.png"></p>
<h3 id="34-yolov4">3.4. YOLOv4</h3>
<p>In this section, we shall elaborate the details of YOLOv4.</p>
<p>Âú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜËØ¶ÁªÜÈòêËø∞YOLOv4ÁöÑÁªÜËäÇ„ÄÇ</p>
<p>YOLOv4 consists of:</p>
<p>YOLOv4 ÂåÖÊã¨Ôºö</p>
<ul>
<li>Backbone: CSPDarknet53 [81]</li>
<li>Neck: SPP [25], PAN [49]</li>
<li>Head: YOLOv3 [63]</li>
</ul>
<p>YOLO v4 uses:</p>
<p>YOLO v4 ‰ΩøÁî®:</p>
<ul>
<li>Bag of Freebies (BoF) for backbone: CutMix and
Mosaic data augmentation, DropBlock regularization,
Class label smoothing</li>
<li>È™®Âπ≤ÁöÑBoFÔºöCutMix Âíå Mosaic Êï∞ÊçÆÂ¢ûÂº∫„ÄÅDropBlock Ê≠£ÂàôÂåñ„ÄÅÁ±ªÊ†áÁ≠æÂπ≥ÊªëÔºàClass label smoothingÔºâ</li>
<li>Bag of Specials (BoS) for backbone: Mish activation,
Cross-stage partial connections (CSP), Multiinput
weighted residual connections (MiWRC)</li>
<li>È™®Âπ≤ÁöÑBoSÔºöMish ÊøÄÊ¥ª„ÄÅË∑®Èò∂ÊÆµÈÉ®ÂàÜËøûÊé• (CSP)„ÄÅÂ§öËæìÂÖ•Âä†ÊùÉÊÆãÂ∑ÆËøûÊé• (MiWRC)</li>
<li>Bag of Freebies (BoF) for detector: CIoU-loss,
CmBN, DropBlock regularization, Mosaic data augmentation,
Self-Adversarial Training, Eliminate grid
sensitivity, Using multiple anchors for a single ground
truth, Cosine annealing scheduler [52], Optimal hyperparameters,
Random training shapes</li>
<li>Ê£ÄÊµãÂô®ÁöÑBoFÔºöCIoU-loss„ÄÅCmBN„ÄÅDropBlock Ê≠£ÂàôÂåñ„ÄÅMosaic Êï∞ÊçÆÂ¢ûÂº∫„ÄÅËá™ÊàëÂØπÊäóËÆ≠ÁªÉÔºàSATÔºâ„ÄÅÊ∂àÈô§ÁΩëÊ†ºÊïèÊÑüÊÄßÔºàEliminate grid sensitivityÔºâ„ÄÅ‰ΩøÁî®Â§ö‰∏™ÈîöÁÇπÔºàmultiple anchorsÔºâËé∑ÂèñÂçï‰∏™ÁúüÂÆûÂÄº„ÄÅ‰ΩôÂº¶ÈÄÄÁÅ´Ë∞ÉÂ∫¶Á®ãÂ∫èÔºàCosine annealing schedulerÔºâ [52]„ÄÅÊúÄ‰ºòË∂ÖÂèÇÊï∞ÔºåÈöèÊú∫ËÆ≠ÁªÉÂΩ¢Áä∂</li>
<li>Bag of Specials (BoS) for detector: Mish activation,
SPP-block, SAM-block, PAN path-aggregation block,DIoU-NMS</li>
<li>Áî®‰∫éÊ£ÄÊµãÂô®ÁöÑÁâπÊúâÂåÖ ÔºàBoSÔºâÔºö ËØØÂå∫ÊøÄÊ¥ª„ÄÅ SPP Âùó„ÄÅ SAM Âùó„ÄÅ PAN Ë∑ØÂæÑËÅöÂêàÂùó„ÄÅ DIoU-NMS</li>
</ul>
<h2 id="4-experiments">4. Experiments  ÂÆûÈ™å</h2>
<p>&emsp;We test the influence of different training improvement techniques on accuracy of the classifier on ImageNet
(ILSVRC 2012 val) dataset, and then on the accuracy of the
detector on MS COCO (test-dev 2017) dataset.</p>
<p>&emsp;Êàë‰ª¨Âú® ImageNet (ILSVRC 2012 val) Êï∞ÊçÆÈõÜ‰∏äÊµãËØï‰∫Ü‰∏çÂêåËÆ≠ÁªÉÊîπËøõÊäÄÊúØÂØπÂàÜÁ±ªÂô®ÂáÜÁ°ÆÊÄßÁöÑÂΩ±ÂìçÔºåÁÑ∂ÂêéÂú® MS COCO (test-dev 2017) Êï∞ÊçÆÈõÜ‰∏äÊµãËØï‰∫ÜÊ£ÄÊµãÂô®ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ</p>
<h3 id="41-experimental-setup">4.1. Experimental setup ÂÆûÈ™åËÆæÁΩÆ</h3>
<p>&emsp;In ImageNet image classification experiments, the default
hyper-parameters are as follows: the training steps is
8,000,000; the batch size and the mini-batch size are 128
and 32, respectively; the polynomial decay learning rate
scheduling strategy is adopted with initial learning rate 0.1;
the warm-up steps is 1000; the momentum and weight decay
are respectively set as 0.9 and 0.005. All of our BoS
experiments use the same hyper-parameter as the default
setting, and in the BoF experiments, we add an additional
50% training steps. In the BoF experiments, we verify
MixUp, CutMix, Mosaic, Bluring data augmentation, and
label smoothing regularization methods. In the BoS experiments,
we compared the effects of LReLU, Swish, and Mish
activation function. All experiments are trained with a 1080
Ti or 2080 Ti GPU.</p>
<p>&emsp;Âú® ImageNet ÂõæÂÉèÂàÜÁ±ªÂÆûÈ™å‰∏≠ÔºåÈªòËÆ§Ë∂ÖÂèÇÊï∞Â¶Ç‰∏ãÔºöËÆ≠ÁªÉÊ≠•Êï∞‰∏∫ 8,000,000Ôºõ Â§ßÊâπÈáèÂíåÂ∞èÊâπÈáèÂ§ßÂ∞èÂàÜÂà´‰∏∫ 128 Âíå 32Ôºõ ÈááÁî®Â§öÈ°πÂºèË°∞ÂáèÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Á≠ñÁï•Ôºàpolynomial decay learning rate scheduling strategyÔºâÔºåÂàùÂßãÂ≠¶‰π†Áéá0.1Ôºõ È¢ÑÁÉ≠Ôºàwarm-upÔºâÊ≠•Êï∞‰∏∫1000Ôºõ Âä®ÈáèË°∞ÂáèÂíåÊùÉÈáçË°∞ÂáèÂàÜÂà´ËÆæÁΩÆ‰∏∫ 0.9 Âíå 0.005„ÄÇÊàë‰ª¨ÊâÄÊúâÁöÑ BoS ÂÆûÈ™åÈÉΩ‰ΩøÁî®‰∏éÈªòËÆ§ËÆæÁΩÆÁõ∏ÂêåÁöÑË∂ÖÂèÇÊï∞ÔºåÂπ∂‰∏îÂú® BoF ÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨Ê∑ªÂä†‰∫ÜÈ¢ùÂ§ñÁöÑ 50% ËÆ≠ÁªÉÊ≠•Êï∞„ÄÇ Âú® BoF ÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨È™åËØÅ‰∫Ü MixUp„ÄÅCutMix„ÄÅMosaic„ÄÅBluring Êï∞ÊçÆÂ¢ûÂº∫ÂíåÊ†áÁ≠æÂπ≥ÊªëÊ≠£ÂàôÂåñÔºàlabel smoothing regularizationÔºâÊñπÊ≥ï„ÄÇ Âú® BoS ÂÆûÈ™å‰∏≠ÔºåÊàë‰ª¨ÊØîËæÉ‰∫Ü LReLU„ÄÅSwish Âíå Mish ÊøÄÊ¥ªÂáΩÊï∞ÁöÑÊïàÊûú„ÄÇ ÊâÄÊúâÂÆûÈ™åÂùá‰ΩøÁî® 1080 Ti Êàñ 2080 Ti GPU ËøõË°åËÆ≠ÁªÉ„ÄÇ</p>
<p>&emsp;In MS COCO object detection experiments, the default hyper-parameters are as follows: the training steps is
500,500; the step decay learning rate scheduling strategy is
adopted with initial learning rate 0.01 and multiply with a
factor 0.1 at the 400,000 steps and the 450,000 steps, respectively; The momentum and weight decay are respec-
tively set as 0.9 and 0.0005. All architectures use a single GPU to execute multi-scale training in the batch size
of 64 while mini-batch size is 8 or 4 depend on the architectures and GPU memory limitation. Except for us-
ing genetic algorithm for hyper-parameter search experiments, all other experiments use default setting. Genetic
algorithm used YOLOv3-SPP to train with GIoU loss and
search 300 epochs for min-val 5k sets. We adopt searched
learning rate 0.00261, momentum 0.949, IoU threshold for
assigning ground truth 0.213, and loss normalizer 0.07 for
genetic algorithm experiments. We have verified a large
number of BoF, including grid sensitivity elimination, mosaic data augmentation, IoU threshold, genetic algorithm,
class label smoothing, cross mini-batch normalization, self-adversarial training, cosine annealing scheduler, dynamic
mini-batch size, DropBlock, Optimized Anchors, different
kind of IoU losses. We also conduct experiments on various
BoS, including Mish, SPP, SAM, RFB, BiFPN, and Gaus-sian YOLO [8]. For all experiments, we only use one GPU
for training, so techniques such as syncBN that optimizes
multiple GPUs are not used.</p>
<p>&emsp;Âú® MS COCO Êï∞ÊçÆÈõÜ‰∏äÁõÆÊ†áÊ£ÄÊµãÂÆûÈ™å‰∏≠ÔºåÈªòËÆ§Ë∂ÖÂèÇÊï∞Â¶Ç‰∏ãÔºöËÆ≠ÁªÉÊ≠•Êï∞‰∏∫ 500,500Ôºõ ÈááÁî®Ê≠•ÈïøË°∞ÂáèÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Á≠ñÁï•Ôºàstep decay learning rate scheduling strategyÔºâÔºåÂàùÂßãÂ≠¶‰π†Áéá0.01ÔºåÂàÜÂà´Âú®40‰∏áÊ≠•Âíå45‰∏áÊ≠•‰πò‰ª•Âõ†Â≠ê0.1Ôºõ Âä®ÈáèÂíåÊùÉÈáçË°∞ÂáèÂàÜÂà´ËÆæÁΩÆ‰∏∫ 0.9 Âíå 0.0005„ÄÇÊâÄÊúâÊû∂ÊûÑÔºàarchitectureÔºâÈÉΩ‰ΩøÁî®Âçï‰∏™ GPU ‰ª• 64 ÁöÑÊâπÈáèÂ§ßÂ∞èÊâßË°åÂ§öÊØî‰æãÔºàmulti-scaleÔºâËÆ≠ÁªÉÔºåËÄåÂ∞èÊâπÈáèÂ§ßÂ∞è‰∏∫ 8 Êàñ 4ÔºåÂÖ∑‰ΩìÂèñÂÜ≥‰∫éÊû∂ÊûÑÂíå GPU ÂÜÖÂ≠òÈôêÂà∂„ÄÇ Èô§‰∫Ü‰ΩøÁî®ÈÅó‰º†ÁÆóÊ≥ïËøõË°åË∂ÖÂèÇÊï∞ÊêúÁ¥¢ÂÆûÈ™åÂ§ñÔºåÂÖ∂‰ªñÊâÄÊúâÂÆûÈ™åÂùá‰ΩøÁî®ÈªòËÆ§ËÆæÁΩÆ„ÄÇ ÈÅó‰º†ÁÆóÊ≥ï‰ΩøÁî® YOLOv3-SPP ËÆ≠ÁªÉ GIoUÊçüÂ§±Âπ∂ÊêúÁ¥¢ 300 ‰∏™ epoch ‰ª•Ëé∑Âèñmin-val 5k ÈõÜ„ÄÇ Êàë‰ª¨ÈááÁî®ÊêúÁ¥¢Â≠¶‰π†Áéá 0.00261ÔºåÂä®Èáè 0.949ÔºåÂàÜÈÖçÁúüÂÆûÂÄºÁöÑ IoU ÈòàÂÄº 0.213ÔºåÈÅó‰º†ÁÆóÊ≥ïÂÆûÈ™åÁöÑÊçüÂ§±ÂΩí‰∏ÄÂåñÂô® 0.07„ÄÇ
Êàë‰ª¨Â∑≤ÁªèÈ™åËØÅ‰∫ÜÂ§ßÈáèÁöÑ BoFÔºåÂåÖÊã¨ÁΩëÊ†ºÊïèÊÑüÊÄßÊ∂àÈô§Ôºàgrid sensitivity eliminationÔºâ„ÄÅÈ©¨ËµõÂÖãÊï∞ÊçÆÂ¢ûÂº∫„ÄÅIoU ÈòàÂÄº„ÄÅÈÅó‰º†ÁÆóÊ≥ï„ÄÅÁ±ªÊ†áÁ≠æÂπ≥Êªë„ÄÅ‰∫§ÂèâÂ∞èÊâπÈáèÂΩí‰∏ÄÂåñ (CmBN)„ÄÅËá™ÂØπÊäóËÆ≠ÁªÉ„ÄÅ‰ΩôÂº¶ÈÄÄÁÅ´Ë∞ÉÂ∫¶Âô®„ÄÅÂä®ÊÄÅÂ∞èÊâπÈáèÂ§ßÂ∞è„ÄÅDropBlock , ‰ºòÂåñÁöÑÈîöÁÇπÔºàOptimized AnchorÔºâÔºå‰∏çÂêåÁßçÁ±ªÁöÑ IoU ÊçüÂ§±„ÄÇ Êàë‰ª¨ËøòÂØπÂêÑÁßç BoS ËøõË°å‰∫ÜÂÆûÈ™åÔºåÂåÖÊã¨ Mish„ÄÅSPP„ÄÅSAM„ÄÅRFB„ÄÅBiFPN ÂíåGaussian YOLO [8]„ÄÇ ÂØπ‰∫éÊâÄÊúâÂÆûÈ™åÔºåÊàë‰ª¨Âè™‰ΩøÁî®‰∏Ä‰∏™ GPU ËøõË°åËÆ≠ÁªÉÔºåÂõ†Ê≠§Ê≤°Êúâ‰ΩøÁî®‰ºòÂåñÂ§ö‰∏™ GPU ÁöÑÂêåÊ≠•BNÔºàsyncBNÔºâ Á≠âÊäÄÊúØ„ÄÇ</p>
<h3 id="42-influence-of-different-features-on-classifier">4.2. Influence of different features on Classifier ‰∏çÂêåÁâπÂæÅÂØπÂàÜÁ±ªÂô®ËÆ≠ÁªÉÁöÑÂΩ±Âìç</h3>
<p>&emsp;First, we study the influence of different features on
classifier training; specifically, the influence of Class la-
bel smoothing, the influence of different data augmentation
techniques, bilateral blurring, MixUp, CutMix and Mosaic,
as shown in Fugure 7, and the influence of different activa-
tions, such as Leaky-ReLU (by default), Swish, and Mish.</p>
<p>&emsp;È¶ñÂÖàÔºåÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏çÂêåÁâπÂæÅÂØπÂàÜÁ±ªÂô®ËÆ≠ÁªÉÁöÑÂΩ±ÂìçÔºõ ÂÖ∑‰ΩìÊù•ËØ¥ÔºåClass label smoothingÁöÑÂΩ±ÂìçÔºå‰∏çÂêåÊï∞ÊçÆÂ¢ûÂº∫ÊäÄÊúØÁöÑÂΩ±ÂìçÔºåÂèåËæπÊ®°Á≥äÔºàbilateral blurringÔºâÔºåMixUpÔºåCutMixÂíåMosaicÔºåÂ¶ÇFugure 7ÊâÄÁ§∫Ôºå‰ª•Âèä‰∏çÂêåÊøÄÊ¥ªÁöÑÂΩ±ÂìçÔºå‰æãÂ¶ÇLeaky-ReLUÔºàÈªòËÆ§ÔºâÔºåSwish ÔºåÂíåMish„ÄÇ</p>
<p><img src = "04_yolo_imgs/ÂõæÁâá07.png"></p>
<p>Figure 7: V arious method of data augmentation.</p>
<p>Âõæ7:ÂêÑÁßçÊï∞ÊçÆÂ¢ûÂº∫ÊñπÊ≥ï„ÄÇ</p>
<p>&emsp;In our experiments, as illustrated in Table 2, the classifier‚Äôs accuracy is improved by introducing the features
such as: CutMix and Mosaic data augmentation, Class label smoothing, and Mish activation. As a result, our BoF-backbone (Bag of Freebies) for classifier training includes
the following: CutMix and Mosaic data augmentation and
Class label smoothing. In addition we use Mish activation
as a complementary option, as shown in Table 2 and Table</p>
<p>&emsp; Âú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠ÔºåÂ¶ÇË°® 2 ÊâÄÁ§∫ÔºåÈÄöËøáÂºïÂÖ•‰ª•‰∏ãÁâπÂæÅÊù•ÊèêÈ´òÂàÜÁ±ªÂô®ÁöÑÂáÜÁ°ÆÊÄßÔºöCutMix Âíå Mosaic Êï∞ÊçÆÂ¢ûÂº∫„ÄÅÁ±ªÊ†áÁ≠æÂπ≥ÊªëÂíå Mish ÊøÄÊ¥ª„ÄÇ Âõ†Ê≠§ÔºåÊàë‰ª¨Áî®‰∫éÂàÜÁ±ªÂô®ËÆ≠ÁªÉÁöÑ BoF-backboneÔºàBag of FreebiesÔºâÂåÖÊã¨‰ª•‰∏ãÂÜÖÂÆπÔºöCutMix Âíå Mosaic Êï∞ÊçÆÂ¢ûÂº∫‰ª•ÂèäÁ±ªÊ†áÁ≠æÂπ≥Êªë„ÄÇ Ê≠§Â§ñÔºåÊàë‰ª¨‰ΩøÁî® Mish ÊøÄÊ¥ª‰Ωú‰∏∫Ë°•ÂÖÖÈÄâÈ°πÔºàcomplementary optionÔºâÔºåÂ¶ÇË°® 2 ÂíåË°® 3 ÊâÄÁ§∫„ÄÇ</p>
<p>Table 2: Influence of BoF and Mish on the CSPResNeXt-50 classifier accuracy.</p>
<p>Ë°®2:BoFÂíåMishÂØπCSPResNeXt-50ÂàÜÁ±ªÂáÜÁ°ÆÁéáÁöÑÂΩ±Âìç„ÄÇ</p>
<p><img src = "04_yolo_imgs/table02.png"></p>
<p>Table 3: Influence of BoF and Mish on the CSPDarknet-53 classifier accuracy.</p>
<p>Ë°®3:BoFÂíåMishÂØπCSPDarknet-53ÂàÜÁ±ªÂáÜÁ°ÆÁéáÁöÑÂΩ±Âìç„ÄÇ</p>
<p><img src = "04_yolo_imgs/table03.png"></p>
<h3 id="43-influence-of-different-features-on-detector">4.3. Influence of different features on Detector</h3>
<p>&emsp;Further study concerns the influence of different Bag-of-Freebies (BoF-detector) on the detector training accuracy,
as shown in Table 4. We significantly expand the BoF list
through studying different features that increase the detector
accuracy without affecting FPS:</p>
<p>&emsp;Ëøõ‰∏ÄÊ≠•ÁöÑÁ†îÁ©∂Ê∂âÂèä‰∏çÂêåÁöÑÂÖçË¥πË¢ã(BOFÊé¢ÊµãÂô®)ÂØπÊé¢ÊµãÂô®ËÆ≠ÁªÉÁ≤æÂ∫¶ÁöÑÂΩ±ÂìçÔºåÂ¶ÇË°®4ÊâÄÁ§∫„ÄÇÊàë‰ª¨ÈÄöËøáÁ†îÁ©∂Âú®‰∏çÂΩ±ÂìçFPSÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òÊé¢ÊµãÂô®Á≤æÂ∫¶ÁöÑ‰∏çÂêåÁâπÊÄßÔºåÊòæËëóÊâ©Â±ï‰∫ÜBOFÂàóË°®Ôºö</p>
<ul>
<li>S: Eliminate grid sensitivity the equation  <span class="arithmatex">\(b_x = œÉ(t_x )+ c_x , b_y = œÉ(t_y )+c_y\)</span> , where <span class="arithmatex">\(c_x\)</span> and <span class="arithmatex">\(c_y\)</span> are always whole numbers, is used in YOLOv3 for evaluating the object coordinates, therefore, extremely high <span class="arithmatex">\(t_x\)</span> absolute values are required for the <span class="arithmatex">\(b_x\)</span> value approaching the <span class="arithmatex">\(c_x \ or \ c_{ x} + 1\)</span> values. We solve this problem through multiplying the sigmoid by a factor exceeding 1.0, so eliminating the effect of grid on which the object is undetectable.</li>
<li>
<p>SÔºö Ê∂àÈô§ÁΩëÊ†ºÁÅµÊïèÂ∫¶ÊñπÁ®ã <span class="arithmatex">\(b_x = œÉ(t_x )+ c_x , b_y = œÉ(t_y )+c_y\)</span> Ôºå ÂÖ∂‰∏≠  <span class="arithmatex">\(c_x\)</span> Âíå <span class="arithmatex">\(c_y\)</span>  ÂßãÁªà‰∏∫Êï¥Êï∞Ôºå Âú® YOLOv3 ‰∏≠‰ΩøÁî®Áî®‰∫éËØÑ‰º∞ÁõÆÊ†áÂùêÊ†áÔºå Âõ†Ê≠§ÔºåÊé•Ëøë <span class="arithmatex">\(c_x \ or \ c_{ x} + 1\)</span>  ÂÄºÁöÑ <span class="arithmatex">\(b_x\)</span> ÂÄºÈúÄË¶ÅÊûÅÈ´òÁöÑ <span class="arithmatex">\(t_x\)</span> ÁªùÂØπÂÄº„ÄÇÊàë‰ª¨ÈÄöËøáÂ∞Ü sigmoid ‰πò‰ª•Ë∂ÖËøá 1.0 ÁöÑÂõ†Â≠êÊù•Ëß£ÂÜ≥Ê≠§ÈóÆÈ¢òÔºå‰ªéËÄåÊ∂àÈô§‰∫ÜÂØπË±°Êó†Ê≥ïÊ£ÄÊµãÂà∞ÁöÑÁΩëÊ†ºÁöÑÂΩ±Âìç„ÄÇ</p>
</li>
<li>
<p>M: Mosaic data augmentation ‚Äì using the 4-image mosaic during training instead of single image</p>
</li>
<li>MÔºöÈ©¨ËµõÂÖãÊï∞ÊçÆÊâ©Â¢û ‚Äì Âú®ËÆ≠ÁªÉÊúüÈó¥‰ΩøÁî® 4 ÂõæÂÉèÈï∂ÂµåÔºåËÄå‰∏çÊòØÂçï‰∏™ÂõæÂÉè</li>
<li>IT: IoU threshold ‚Äì using multiple anchors for a single ground truth IoU (truth, anchor) &gt; IoU-threshold</li>
<li>ITÔºöIoU ÈòàÂÄº ‚Äì ‰ΩøÁî®Â§ö‰∏™ÈîöÁÇπËøõË°åÂçï‰∏™Êé•Âú∞ÁúüÁõ∏ IoUÔºàÁúü„ÄÅÈîöÔºâÂíå IoU ÈòàÂÄº |</li>
<li>GA: Genetic algorithms ‚Äì using genetic algorithms for selecting the optimal hyperparameters during network training on the Ô¨Årst 10% of time periods</li>
<li>GAÔºöÈÅó‰º†ÁÆóÊ≥ï ‚Äì Âú®Ââç 10% ÁöÑÊó∂Èó¥ÊÆµÁöÑÁΩëÁªúËÆ≠ÁªÉÊúüÈó¥‰ΩøÁî®ÈÅó‰º†ÁÆóÊ≥ïÈÄâÊã©ÊúÄ‰Ω≥Ë∂ÖÂèÇÊï∞*</li>
<li>LS: Class label smoothing ‚Äì using class label smoothing for sigmoid activation</li>
<li>LSÔºöÁ±ªÊ†áÁ≠æÂπ≥Êªë ‚Äì ‰ΩøÁî®Á±ªÊ†áÁ≠æÂπ≥ÊªëËøõË°å sigmoid ÊøÄÊ¥ª</li>
<li>CBN: CmBN ‚Äì using Cross mini-Batch Normalization for collecting statistics inside the entire batch, instead of collecting statistics inside a single mini-batch</li>
<li>CBNÔºö CmBN ‚Äì ‰ΩøÁî®‰∫§ÂèâÂ∞èÊâπÂ§ÑÁêÜËßÑËåÉÂåñÊî∂ÈõÜÊï¥‰∏™ÊâπÂ§ÑÁêÜ‰∏≠ÁöÑÁªüËÆ°‰ø°ÊÅØÔºåËÄå‰∏çÊòØÂú®Âçï‰∏™Â∞èÊâπÂ§ÑÁêÜ‰∏≠Êî∂ÈõÜÁªüËÆ°‰ø°ÊÅØ*</li>
<li>CA: Cosine annealing scheduler ‚Äì altering the learning rate during sinusoid training</li>
<li>CAÔºöÂçèÂíåÁ¥†ÈÄÄÁÅ´Ë∞ÉÂ∫¶Âô® ‚Äì ÊîπÂèòÊ≠£Âº¶ËÆ≠ÁªÉ‰∏≠ÁöÑÂ≠¶‰π†ÈÄüÁéá*</li>
<li>DM: Dynamic mini-batch size ‚Äì automatic increase of mini-batch size during small resolution training by using Random training shapes</li>
<li>DMÔºöÂä®ÊÄÅÂ∞èÊâπÈáèÂ∞∫ÂØ∏ ‚Äì ‰ΩøÁî®ÈöèÊú∫ËÆ≠ÁªÉÂΩ¢Áä∂Âú®Â∞èÂàÜËæ®ÁéáËÆ≠ÁªÉÊúüÈó¥Ëá™Âä®Â¢ûÂä†Â∞èÊâπÈáèÂ§ßÂ∞è</li>
<li>OA: Optimized Anchors ‚Äì using the optimized anchors for training with the 512√ó512 network resolution</li>
<li>OAÔºö‰ºòÂåñÁöÑÈîöÁÇπ ‚Äì ‰ΩøÁî®‰ºòÂåñÁöÑÈîöÁÇπËøõË°å 512√ó512 ÁΩëÁªúÂàÜËæ®ÁéáÁöÑËÆ≠ÁªÉ*</li>
<li>GIoU, CIoU, DIoU, MSE ‚Äì using different loss algorithms for bounded box regression</li>
<li>GIoU„ÄÅCIoU„ÄÅDIoU„ÄÅMSE ‚Äì ÂØπËæπÁïåÊ°ÜÂõûÂΩí‰ΩøÁî®‰∏çÂêåÁöÑÊçüÂ§±ÁÆóÊ≥ï</li>
</ul>
<p>&emsp;Further study concerns the inÔ¨Çuence of different Bagof-Specials (BoS-detector) on the detector training accuracy, including PAN, RFB, SAM, Gaussian YOLO (G), and ASFF, as shown in Table 5. In our experiments, the detector gets best performance when using SPP, PAN, and SAM.</p>
<p>&emsp;Ëøõ‰∏ÄÊ≠•Á†îÁ©∂Ê∂âÂèä‰∏çÂêåÁöÑÂ∑¥ÊààÂ§´ÁâπËæëÔºàBoS-Ê£ÄÊµãÂô®ÔºâÂØπÊ£ÄÊµãÂô®ËÆ≠ÁªÉÁ≤æÂ∫¶ÁöÑÂΩ±ÂìçÔºåÂåÖÊã¨PAN„ÄÅRFB„ÄÅSAM„ÄÅÈ´òÊñØYOLOÔºàGÔºâÂíåASFFÔºåÂ¶ÇË°®5ÊâÄÁ§∫„ÄÇÂú®Êàë‰ª¨ÁöÑÂÆûÈ™å‰∏≠ÔºåÊ£ÄÊµãÂô®Âú®‰ΩøÁî® SPP„ÄÅPAN Âíå SAM Êó∂Ëé∑ÂæóÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇ</p>
<p>Table 4: Ablation Studies of Bag-of-Freebies. (CSPResNeXt50-PANet-SPP , 512x512).</p>
<p><img src = "04_yolo_imgs/table04.png"></p>
<p>Table 5: Ablation Studies of Bag-of-Specials. (Size 512x512).</p>
<p><img src = "04_yolo_imgs/table05.png"></p>
<h3 id="44-influence-of-different-backbones-and-pretrained-weightings-on-detector-training">4.4. Influence of different backbones and pretrained weightings on Detector training</h3>
<p>&emsp;Further on we study the influence of different backbone
models on the detector accuracy, as shown in Table 6. We
notice that the model characterized with the best classifica-
tion accuracy is not always the best in terms of the detector
accuracy.</p>
<p>&emsp; ÊúÄÂêéÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂú®‰∏çÂêåÊúÄÂ∞èÊâπÈáèÂ§ßÂ∞è‰∏ãËÆ≠ÁªÉÁöÑÊ®°ÂûãÊâÄËé∑ÂæóÁöÑÁªìÊûúÔºåÁªìÊûúÂ¶ÇË°®7ÊâÄÁ§∫„ÄÇ‰ªéË°®7ÊâÄÁ§∫ÁöÑÁªìÊûú‰∏≠ÔºåÊàë‰ª¨ÂèëÁé∞Âú®Ê∑ªÂä†BoFÂíåBoSËÆ≠ÁªÉÁ≠ñÁï•‰πãÂêéÔºåÊúÄÂ∞èÊâπÈáèÂ§ßÂ∞èÂá†‰πéÊ≤°ÊúâÂΩ±ÂìçÂú®Ê£ÄÊµãÂô®ÁöÑÊÄßËÉΩ‰∏ä„ÄÇËØ•ÁªìÊûúË°®ÊòéÔºåÂú®ÂºïÂÖ•BoFÂíåBoS‰πãÂêéÔºå‰∏çÂÜçÈúÄË¶Å‰ΩøÁî®ÊòÇË¥µÁöÑGPUËøõË°åËÆ≠ÁªÉ„ÄÇÊç¢Âè•ËØùËØ¥Ôºå‰ªª‰Ωï‰∫∫ÈÉΩÂè™ËÉΩ‰ΩøÁî®‰º†ÁªüÁöÑGPUÊù•ËÆ≠ÁªÉÂá∫Ëâ≤ÁöÑÊé¢ÊµãÂô®„ÄÇ</p>
<p>Table 7: Using different mini-batch size for detector training.</p>
<p>Ë°®7Ôºö‰ΩøÁî®‰∏çÂêåÁöÑÂ∞èÊâπÈáèÂØπ‰∫éÊ£ÄÊµãÂô®ËÆ≠ÁªÉ„ÄÇ</p>
<p><img src = "04_yolo_imgs/table07.png"></p>
<h2 id="5results">5.Results</h2>
<p>&emsp;Comparison of the results obtained with other state-of-the-art object detectors are shown in Figure 8. Our
YOLOv4 are located on the Pareto optimality curve and are superior to the fastest and most accurate detectors in terms
of both speed and accuracy.</p>
<p>&emsp;ÂæóÂà∞ÁöÑÁªìÊûú‰∏éÂÖ∂‰ªñÂÖàËøõÁöÑÁõÆÊ†áÊ£ÄÊµãÂô®ÁöÑÊØîËæÉÂ¶ÇÂõæ8ÊâÄÁ§∫„ÄÇÊàë‰ª¨ÁöÑyolo4‰Ωç‰∫éPareto optimalityÊõ≤Á∫ø‰∏äÔºåÂú®ÈÄüÂ∫¶ÂíåÂáÜÁ°ÆÊÄßÊñπÈù¢ÈÉΩ‰ºò‰∫éÊúÄÂø´ÂíåÊúÄÁ≤æÁ°ÆÁöÑÊé¢ÊµãÂô®„ÄÇ</p>
<p>&emsp;Since different methods use GPUs of different architectures for inference time verification, we operate YOLOv4
on commonly adopted GPUs of Maxwell, Pascal, and Volta
architectures, and compare them with other state-of-the-art
methods. Table 8 lists the frame rate comparison results of
using Maxwell GPU, and it can be GTX Titan X (Maxwell)
or Tesla M40 GPU. Table 9 lists the frame rate comparison
results of using Pascal GPU, and it can be Titan X (Pascal),
Titan Xp, GTX 1080 Ti, or Tesla P100 GPU. As for Table 10, it lists the frame rate comparison results of using V olta GPU, and it can be Titan V olta or Tesla V100 GPU.</p>
<p>&emsp;Áî±‰∫é‰∏çÂêåÁöÑÊñπÊ≥ï‰ΩøÁî®‰∏çÂêå‰ΩìÁ≥ªÁªìÊûÑÁöÑGPUËøõË°åÊé®ÁêÜÊó∂Èó¥È™åËØÅÔºåÊàë‰ª¨Âú®Â∏∏Áî®ÁöÑMaxwell„ÄÅPascalÂíåVoltaArchitecture‰ΩìÁ≥ªÁªìÊûÑÁöÑGPU‰∏äËøêË°åYOLOv4ÔºåÂπ∂Â∞ÜÂÆÉ‰ª¨‰∏éÂÖ∂‰ªñÂÖàËøõÁöÑÊñπÊ≥ïËøõË°å‰∫ÜÊØîËæÉ„ÄÇË°®8ÂàóÂá∫‰∫Ü‰ΩøÁî®Maxwell GPUÁöÑÂ∏ßÁéáÊØîËæÉÁªìÊûúÔºåÂèØ‰ª•ÊòØGTX Titan X(Maxwell)ÊàñTesla M40 GPU„ÄÇË°®9ÂàóÂá∫‰∫Ü‰ΩøÁî®Pascal GPUÁöÑÂ∏ßÁéáÊØîËæÉÁªìÊûúÔºåÂÆÉÂèØ‰ª•ÊòØTitan X(Pascal)„ÄÅTitan XP„ÄÅGTX 1080 TiÊàñTesla P100 GPU„ÄÇË°®10ÂàóÂá∫‰∫Ü‰ΩøÁî®VoltaGPUÁöÑÂ∏ßÁéáÂØπÊØîÁªìÊûúÔºåÂèØ‰ª•ÊòØTitan VoltaÔºå‰πüÂèØ‰ª•ÊòØTesla V100 GPU„ÄÇ</p>
<h2 id="6-conclusions">6. Conclusions</h2>
<p>We offer a state-of-the-art detector which is faster (FPS)
and more accurate (MS COCO <span class="arithmatex">\(AP_{50...95}\)</span> and <span class="arithmatex">\(AP_{50}\)</span> ) than
all available alternative detectors. The detector described
can be trained and used on a conventional GPU with 8-16
GB-VRAM this makes its broad use possible. The original
concept of one-stage anchor-based detectors has proven its
viability. We have verified a large number of features, and
selected for use such of them for improving the accuracy of
both the classifier and the detector. These features can be
used as best-practice for future studies and developments.
&emsp;Êàë‰ª¨Êèê‰æõÊúÄÂÖàËøõÁöÑÊ£ÄÊµãÂô®ÔºåÂÖ∂ÈÄüÂ∫¶ÔºàFPSÔºâÂíåÂáÜÁ°ÆÂ∫¶ÔºàMS COCO <span class="arithmatex">\(AP_{50 ... 95}\)</span> Âíå <span class="arithmatex">\(AP_{50}\)</span> ÔºâÊØîÊâÄÊúâÂèØÁî®ÁöÑÊõø‰ª£Ê£ÄÊµãÂô®ÈÉΩÈ´ò„ÄÇÊâÄÊèèËø∞ÁöÑÊ£ÄÊµãÂô®ÂèØ‰ª•Âú®ÂÖ∑Êúâ8-16GB-VRAMÁöÑÂ∏∏ËßÑGPU‰∏äËøõË°åËÆ≠ÁªÉÂíå‰ΩøÁî®ÔºåËøô‰ΩøÂæóÂÆÉÁöÑÂπøÊ≥õ‰ΩøÁî®Êàê‰∏∫ÂèØËÉΩ„ÄÇÂçïÈò∂ÊÆµÂü∫‰∫éÈîöÊ°ÜÁöÑÊ£ÄÊµãÂô®ÁöÑÂéüÂßãÊ¶ÇÂøµÂ∑≤ËØÅÊòéÂÖ∂ÂèØË°åÊÄß„ÄÇÊàë‰ª¨Â∑≤ÁªèÈ™åËØÅ‰∫ÜÂ§ßÈáèÁâπÂæÅÔºåÂπ∂ÈÄâÊã©‰ΩøÁî®ÂÖ∂‰∏≠ÁöÑ‰∏Ä‰∫õÁâπÂæÅ‰ª•ÊèêÈ´òÂàÜÁ±ªÂô®ÂíåÊ£ÄÊµãÂô®ÁöÑÂáÜÁ°ÆÊÄß„ÄÇËøô‰∫õÂäüËÉΩÂèØ‰ª•Áî®‰ΩúÂ∞ÜÊù•Á†îÁ©∂ÂíåÂºÄÂèëÁöÑÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ</p>
<h2 id="7-acknowledgements">7. Acknowledgements</h2>
<p>&emsp;The authors wish to thank Glenn Jocher for the
ideas of Mosaic data augmentation, the selection of
hyper-parameters by using genetic algorithms and solving
the grid sensitivity problem https://github.com/ultralytics/yolov3.</p>
<p>&emsp;‰ΩúËÄÖË¶ÅÊÑüË∞¢Glenn JocherËøõË°åMosaicÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊÉ≥Ê≥ïÔºåÈÄöËøá‰ΩøÁî®ÈÅó‰º†ÁÆóÊ≥ïÈÄâÊã©Ë∂ÖÂèÇÊï∞Âπ∂Ëß£ÂÜ≥ÁΩëÊ†ºÊïèÊÑüÊÄßÈóÆÈ¢òÁöÑÊñπÊ≥ïhttps://github.com/ultralytics/yolov3.10„ÄÇ</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="03_yolo.html" class="md-footer__link md-footer__link--prev" aria-label="‰∏ä‰∏ÄÈ°µ: YOLOv3" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                ‰∏ä‰∏ÄÈ°µ
              </span>
              YOLOv3
            </div>
          </div>
        </a>
      
      
        
        <a href="06_yolo.html" class="md-footer__link md-footer__link--next" aria-label="‰∏ã‰∏ÄÈ°µ: YOLOv6" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                ‰∏ã‰∏ÄÈ°µ
              </span>
              YOLOv6
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.top", "instant"], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.477d984a.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.ddd52ceb.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>