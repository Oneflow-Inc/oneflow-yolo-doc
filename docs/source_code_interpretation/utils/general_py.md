## å‰è¨€
æºç è§£è¯»ï¼š [utils/autoanchor.py](https://github.com/Oneflow-Inc/one-yolov5/blob/main/utils/autoanchor.py)

>   è¿™ä¸ªæ–‡ä»¶æ˜¯yolov5çš„é€šç”¨å·¥å…·ç±»ï¼Œå†™äº†ä¸€äº›é€šç”¨çš„å·¥å…·å‡½æ•°ï¼Œç”¨çš„å¾ˆå¹¿ï¼Œæ•´ä¸ªé¡¹ç›®å“ªé‡Œéƒ½å¯èƒ½ç”¨åˆ°ã€‚
     è¿™ä¸ªæ–‡ä»¶çš„å‡½æ•°éå¸¸å¤šï¼Œä»£ç é‡ä¹Ÿå¾ˆå¤§ï¼ˆä¸Šåƒè¡Œäº†ï¼‰ï¼Œä¹Ÿéƒ½æ¯”è¾ƒé‡è¦ï¼Œå¸Œæœ›å¤§å®¶çœ‹çš„æ—¶å€™å¤šç‚¹è€å¿ƒï¼Œéƒ½èƒ½æŒæ¡ï¼

- ğŸ‰ä»£ç ä»“åº“åœ°å€ï¼š<a href="https://github.com/Oneflow-Inc/one-yolov5" target="blank">https://github.com/Oneflow-Inc/one-yolov5</a>
- ğŸ‰æ–‡æ¡£ç½‘ç«™åœ°å€ï¼š<a href="https://start.oneflow.org/oneflow-yolo-doc/index.html" target="blank"> https://start.oneflow.org/oneflow-yolo-doc/index.html</a>
- OneFlow å®‰è£…æ–¹æ³•ï¼š<a href="https://github.com/Oneflow-Inc/oneflow#install-oneflow" target="blank"> https://github.com/Oneflow-Inc/oneflow#install-oneflow</a>

ä¸è¿‡å³ä½¿ä½ å¯¹ OneFlow å¸¦æ¥çš„æ€§èƒ½æå‡ä¸å¤ªæ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬ç›¸ä¿¡[æ–‡æ¡£ç½‘ç«™](https://start.oneflow.org/oneflow-yolo-doc/index.html)ä¸­å¯¹ YOLOv5 æ•™ç¨‹çš„æ±‰åŒ–ä»¥åŠæºç å‰–æä¹Ÿä¼šæ˜¯ä»é›¶å¼€å§‹æ·±å…¥å­¦ä¹  YOLOv5 ä¸€ä»½ä¸é”™çš„èµ„æ–™ã€‚æ¬¢è¿åœ¨ä»“åº“ç»™æˆ‘ä»¬æå‡ºå®è´µçš„æ„è§ã€‚ğŸŒŸğŸŒŸğŸŒŸ

æ¬¢è¿star [one-yolov5é¡¹ç›®](https://github.com/Oneflow-Inc/one-yolov5) è·å–<a href="https://github.com/Oneflow-Inc/one-yolov5/tags" target="blank" >æœ€æ–°çš„åŠ¨æ€ã€‚</a>




## 0ã€å¯¼å…¥éœ€è¦çš„åŒ…å’ŒåŸºæœ¬é…ç½®


```python
# import contextlib   # pythonä¸Šä¸‹æ–‡ç®¡ç†å™¨   æ‰§è¡Œwithâ€¦asâ€¦çš„æ—¶å€™è°ƒç”¨contextlib
import glob         # ä»…æ”¯æŒéƒ¨åˆ†é€šé…ç¬¦çš„æ–‡ä»¶æœç´¢æ¨¡å—
import logging      # æ—¥å¿—æ¨¡å—
import math         # æ•°å­¦å…¬å¼æ¨¡å—
import os           # ä¸æ“ä½œç³»ç»Ÿè¿›è¡Œäº¤äº’çš„æ¨¡å—
import platform     # æä¾›è·å–æ“ä½œç³»ç»Ÿç›¸å…³ä¿¡æ¯çš„æ¨¡å—
import random       # ç”Ÿæˆéšæœºæ•°çš„æ¨¡å—
import re           # ç”¨æ¥åŒ¹é…å­—ç¬¦ä¸²ï¼ˆåŠ¨æ€ã€æ¨¡ç³Šï¼‰çš„æ¨¡å—
import signal       # ä¿¡å·å¤„ç†æ¨¡å—
import time         # æ—¶é—´æ¨¡å— æ›´åº•å±‚
import urllib       # ç”¨äºæ“ä½œç½‘é¡µURL, å¹¶å¯¹ç½‘é¡µçš„å†…å®¹è¿›è¡ŒæŠ“å–å¤„ç†  å¦‚urllib.parse: è§£æurl
from itertools import repeat  # å¾ªç¯å™¨æ¨¡å—  åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨ï¼Œé‡å¤ç”Ÿæˆobject
from multiprocessing.pool import ThreadPool  # å¤šçº¿ç¨‹æ¨¡å— çº¿ç¨‹æ± 
from pathlib import Path  # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—
from subprocess import check_output  # åˆ›å»ºä¸€ä¸ªå­è¿›ç¨‹å†å‘½ä»¤è¡Œæ‰§è¡Œ..., æœ€åè¿”å›æ‰§è¡Œç»“æœ(æ–‡ä»¶)
from typing import Optional
from zipfile import ZipFile

import cv2 # opencvåº“
import numpy as np # numpyçŸ©é˜µå¤„ç†å‡½æ•°åº“
import oneflow as flow # OneFlowæ¡†æ¶
import oneflow.backends.cudnn as cudnn
import pandas as pd # pandasçŸ©é˜µæ“ä½œæ¨¡å—
import pkg_resources as pkg # ç”¨äºæŸ¥æ‰¾, è‡ªçœ, æ¿€æ´»å’Œä½¿ç”¨å·²å®‰è£…çš„Pythonå‘è¡Œç‰ˆ
import yaml # yamlé…ç½®æ–‡ä»¶è¯»å†™æ¨¡å—

from utils.downloads import gsutil_getsize
from utils.metrics import box_iou, fitness



FILE = Path(__file__).resolve()
ROOT = FILE.parents[1]  # YOLOv5 root directory
RANK = int(os.getenv("RANK", -1))

# Settings
DATASETS_DIR = ROOT.parent / "datasets"  # YOLOv5 datasets directory
# ç¡®å®šæœ€å¤§çš„çº¿ç¨‹æ•° è¿™é‡Œè¢«é™åˆ¶åœ¨äº†8
NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of YOLOv5 multiprocessing threads
AUTOINSTALL = str(os.getenv("YOLOv5_AUTOINSTALL", True)).lower() == "true"  # global auto-install mode
VERBOSE = str(os.getenv("YOLOv5_VERBOSE", True)).lower() == "true"  # global verbose mode
FONT = "Arial.ttf"  # https://ultralytics.com/assets/Arial.ttf
# è®¾ç½®è¿è¡Œç›¸å…³çš„ä¸€äº›åŸºæœ¬çš„é…ç½®  Settings
# æ§åˆ¶printæ‰“å°torch.tensoræ ¼å¼è®¾ç½®  tensorç²¾åº¦ä¸º5(å°æ•°ç‚¹å5ä½)  æ¯è¡Œå­—ç¬¦æ•°ä¸º320ä¸ª  æ˜¾ç¤ºæ–¹æ³•ä¸ºlong
flow.set_printoptions(linewidth=320, precision=5, profile="long")
# æ§åˆ¶printæ‰“å°np.arrayæ ¼å¼è®¾ç½®  ç²¾åº¦ä¸º5  æ¯è¡Œå­—ç¬¦æ•°ä¸º320ä¸ª  format short g, %precision=5
np.set_printoptions(linewidth=320, formatter={"float_kind": "{:11.5g}".format})  # format short g, %precision=5
# pandasçš„æœ€å¤§æ˜¾ç¤ºè¡Œæ•°æ˜¯10
pd.options.display.max_columns = 10
# é˜»æ­¢opencvå‚ä¸å¤šçº¿ç¨‹(ä¸ Pytorchçš„ Dataloaderä¸å…¼å®¹)
cv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)
os.environ["NUMEXPR_MAX_THREADS"] = str(NUM_THREADS)  # NumExpr max threads
os.environ["OMP_NUM_THREADS"] = "1" if platform.system() == "darwin" else str(NUM_THREADS)  # OpenMP (Pyflow and SciPy)
```

## 1ã€timeoutï¼ˆæ²¡ç”¨åˆ°ï¼‰
è¿™ä¸ªå‡½æ•°æ˜¯è‡ªå®šä¹‰çš„timeoutè¶…æ—¶å‡½æ•°ï¼Œå¦‚æœæŸä¸ªç¨‹åºæ‰§è¡Œè¶…æ—¶ï¼Œ

å°±ä¼šè§¦å‘è¶…æ—¶å¤„ç†å‡½æ•°_timeout_handler è¿”å›è¶…æ—¶å¼‚å¸¸ä¿¡æ¯ã€‚

ä½†æ˜¯è¿™ä¸ªå‡½æ•°æ²¡ç”¨åˆ°ï¼Œä»£ç ä¸­éƒ½æ˜¯ä½¿ç”¨åº“å‡½æ•°è‡ªå·±å®šä¹‰çš„timeoutï¼Œæ²¡ç”¨ç”¨è¿™ä¸ªè‡ªå®šä¹‰çš„timeoutå‡½æ•°ã€‚

æ‰€ä»¥è¿™ä¸ªå‡½æ•°å¯ä»¥äº†è§£ä¸‹å°±è¡Œï¼Œä¸è¿‡è¿™ç§è¶…æ—¶æç¤ºçš„ä»£ç è¿˜æ˜¯æœ‰å¿…è¦å­¦ä¹ çš„ã€‚

timeoutå‡½æ•°ä»£ç ï¼š


```python
class timeout(contextlib.ContextDecorator):
    """æ²¡ç”¨åˆ°  ä»£ç ä¸­éƒ½æ˜¯ä½¿ç”¨åº“å‡½æ•°è‡ªå·±å®šä¹‰çš„timeout æ²¡ç”¨ç”¨è¿™ä¸ªè‡ªå®šä¹‰çš„timeoutå‡½æ•°
    è®¾ç½®ä¸€ä¸ªè¶…æ—¶å‡½æ•° å¦‚æœæŸä¸ªç¨‹åºæ‰§è¡Œè¶…æ—¶  å°±ä¼šè§¦å‘è¶…æ—¶å¤„ç†å‡½æ•°_timeout_handler è¿”å›è¶…æ—¶å¼‚å¸¸ä¿¡æ¯
    å¹¶æ²¡æœ‰ç”¨åˆ°  è¿™é‡Œé¢çš„timeoutéƒ½æ˜¯ç”¨pythonåº“å‡½æ•°å®ç°çš„ å¹¶ä¸éœ€è¦è‡ªå·±å¦å¤–å†™ä¸€ä¸ª
    ä½¿ç”¨: with timeout(seconds):  sleep(10)   æˆ–è€…   @timeout(seconds) decorator
    dealing with wandb login-options timeout issues as well as check_github() timeout issues
    """
    def __init__(self, seconds, *, timeout_msg='', suppress_timeout_errors=True):
        self.seconds = int(seconds)   # é™åˆ¶æ—¶é—´
        self.timeout_message = timeout_msg  # æŠ¥é”™ä¿¡æ¯
        self.suppress = bool(suppress_timeout_errors)

    def _timeout_handler(self, signum, frame):
        # è¶…æ—¶å¤„ç†å‡½æ•° ä¸€æ—¦è¶…æ—¶ å°±åœ¨secondsåå‘é€è¶…æ—¶ä¿¡æ¯
        raise TimeoutError(self.timeout_message)

    def __enter__(self):
        # signal.signal: è®¾ç½®ä¿¡å·å¤„ç†çš„å‡½æ•°_timeout_handler
        # æ‰§è¡Œæµè¿›å…¥withä¸­ä¼šæ‰§è¡Œ__enter__æ–¹æ³• å¦‚æœå‘ç”Ÿè¶…æ—¶, å°±ä¼šè§¦å‘è¶…æ—¶å¤„ç†å‡½æ•°_timeout_handler è¿”å›è¶…æ—¶å¼‚å¸¸ä¿¡æ¯
        signal.signal(signal.SIGALRM, self._timeout_handler)  # Set handler for SIGALRM
        # signal.alarm: è®¾ç½®å‘é€SIGALRMä¿¡å·çš„å®šæ—¶å™¨
        signal.alarm(self.seconds)  # start countdown for SIGALRM to be raised

    def __exit__(self, exc_type, exc_val, exc_tb):
        # æ‰§è¡Œæµç¦»å¼€ with å—æ—¶(æ²¡æœ‰å‘ç”Ÿè¶…æ—¶), åˆ™è°ƒç”¨è¿™ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„__exit__æ–¹æ³•æ¥æ¸…ç†æ‰€ä½¿ç”¨çš„èµ„æº
        signal.alarm(0)  # Cancel SIGALRM if it's scheduled
        if self.suppress and exc_type is TimeoutError:  # Suppress TimeoutError
            return True

```

## 2ã€set_loggingã€init_seeds

è¿™ä¸¤ä¸ªå‡½æ•°æ˜¯ä¸€äº›åˆå§‹åŒ–æ“ä½œã€‚

set_loggingæ˜¯å¯¹æ—¥å¿—çš„è®¾ç½®(formatã€level)ç­‰è¿›è¡Œåˆå§‹åŒ–ï¼Œinit_seedsæ˜¯è¿›è¡Œä¸€ç³»åˆ—çš„éšæœºæ•°ç§å­

### 2.1ã€set_logging
è¿™ä¸ªå‡½æ•°æ˜¯å¯¹æ—¥å¿—çš„æ ¼å¼ã€ç­‰çº§ç­‰è¿›è¡Œä¸€ä¸ªåˆå§‹åŒ–ï¼Œä½†æ˜¯è¿™ä¸ªå‡½æ•°æ²¡ç”¨åˆ°ã€‚


```python
def set_logging(name=None, verbose=VERBOSE):
    """å¹¿æ³›ä½¿ç”¨åœ¨train.pyã€test.pyã€detect.pyç­‰æ–‡ä»¶çš„mainå‡½æ•°çš„ç¬¬ä¸€æ­¥
    å¯¹æ—¥å¿—çš„è®¾ç½®(formatã€level)ç­‰è¿›è¡Œåˆå§‹åŒ–
    """
    # Sets level and returns logger
    if is_kaggle():
        for h in logging.root.handlers:
            logging.root.removeHandler(h)  # remove all handlers associated with the root logger object
    rank = int(os.getenv("RANK", -1))  # rank in world for Multi-GPU trainings'
    # è®¾ç½®æ—¥å¿—çº§åˆ« 
    level = logging.INFO if verbose and rank in {-1, 0} else logging.ERROR
    log = logging.getLogger(name)
    log.setLevel(level)
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter("%(message)s"))
    handler.setLevel(level)
    log.addHandler(handler)

```

### 2.2ã€init_seeds
è¿™ä¸ªå‡½æ•°æ˜¯ä½¿ç”¨random.random()ã€np.random.rand()ã€init_torch_seedsï¼ˆè°ƒç”¨torch_utils.pyä¸­çš„å‡½æ•°ï¼‰

ç­‰ç”Ÿæˆä¸€ç³»åˆ—çš„éšæœºæ•°ç§å­ï¼Œä»¥ä¿è¯ç»“æœçš„å¯å¤ç°æ€§ã€‚

init_seedså‡½æ•°ä»£ç ï¼š


```python
def init_seeds(seed=0, deterministic=False):
    # Initialize random number generator (RNG) seeds https://pyflow.org/docs/stable/notes/randomness.html
    # cudnn seed 0 settings are slower and more reproducible, else faster and less reproducible
    # è®¾ç½®éšæœºæ•° é’ˆå¯¹ä½¿ç”¨random.random()ç”Ÿæˆéšæœºæ•°çš„æ—¶å€™ç›¸åŒ
    random.seed(seed)    
    # è®¾ç½®éšæœºæ•° é’ˆå¯¹ä½¿ç”¨np.random.rand()ç”Ÿæˆéšæœºæ•°çš„æ—¶å€™ç›¸åŒ
    np.random.seed(seed)
    # ä¸ºCPUè®¾ç½®ç§å­ç”¨äºç”Ÿæˆéšæœºæ•°çš„æ—¶å€™ç›¸åŒ  å¹¶ç¡®å®šè®­ç»ƒæ¨¡å¼
    flow.manual_seed(seed)
    cudnn.benchmark, cudnn.deterministic = (False, True)
    flow.cuda.manual_seed(seed)
    flow.cuda.manual_seed_all(seed)  # for Multi-GPU, exception safe
```

## 3ã€get_latest_run

è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯æŸ¥æ‰¾æœ€è¿‘ä¿å­˜çš„æƒé‡æ–‡ä»¶ last*.ptï¼Œç”¨ä»¥è¿›è¡Œæ–­ç‚¹ç»­è®­ã€‚

get_latest_runå‡½æ•°ä»£ç ï¼š


```python
def get_latest_run(search_dir="."):
    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)
    """ç”¨åœ¨train.pyæŸ¥æ‰¾æœ€è¿‘çš„ptæ–‡ä»¶è¿›è¡Œæ–­ç‚¹ç»­è®­
    ç”¨äºè¿”å›è¯¥é¡¹ç›®ä¸­æœ€è¿‘çš„æ¨¡å‹ 'last.pt'å¯¹åº”çš„è·¯å¾„
    :params search_dir: è¦æœç´¢çš„æ–‡ä»¶çš„æ ¹ç›®å½• é»˜è®¤æ˜¯ '.'  è¡¨ç¤ºæœç´¢è¯¥é¡¹ç›®ä¸­çš„æ–‡ä»¶
    """
    # ä»Pythonç‰ˆæœ¬3.5å¼€å§‹, globæ¨¡å—æ”¯æŒè¯¥"**"æŒ‡ä»¤ï¼ˆä»…å½“ä¼ é€’recursiveæ ‡å¿—æ—¶æ‰ä¼šè§£æè¯¥æŒ‡ä»¤)
    # glob.globå‡½æ•°åŒ¹é…æ‰€æœ‰çš„ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶, å¹¶å°†å…¶ä»¥listçš„å½¢å¼è¿”å›
    last_list = glob.glob(f"{search_dir}/**/last", recursive=True)
    # os.path.getctime è¿”å›è·¯å¾„å¯¹åº”æ–‡ä»¶çš„åˆ›å»ºæ—¶é—´
    # æ‰€ä»¥è¿™é‡Œæ˜¯è¿”å›è·¯å¾„åˆ—è¡¨ä¸­åˆ›å»ºæ—¶é—´æœ€æ™š(æœ€è¿‘çš„lastæ–‡ä»¶)çš„è·¯å¾„
    return max(last_list, key=os.path.getctime) if last_list else ""
```

å‡½æ•°åœ¨train.pyä¸­è¢«è°ƒç”¨ï¼š
![image.png](general_imgs/picture_00.png)

## 4ã€is_dockerã€is_colabã€is_pip
ä¸‹é¢æ˜¯ä¸‰ä¸ªæ£€æµ‹å‡½æ•°ï¼Œis_dockeræ£€æµ‹å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯dockerç¯å¢ƒï¼Œ

is_colabæ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯Google Colabç¯å¢ƒï¼Œis_pipæ£€æµ‹

### 4.1ã€is_docker
è¿™ä¸ªå‡½æ•°æ˜¯æŸ¥è¯¢å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯dockerç¯å¢ƒï¼Œä¼šç”¨åˆ°åé¢çš„check_git_statuså’Œcheck_imshowç­‰å‡½æ•°ä¸­ã€‚

is_dockerå‡½æ•°ä»£ç ï¼š


```python
def is_docker() -> bool:
    """
    åœ¨åé¢çš„check_git_statuså’Œcheck_imshowç­‰å‡½æ•°ä¸­è¢«è°ƒç”¨
    æŸ¥è¯¢å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯dockerç¯å¢ƒ  Is environment a Docker container?
    Check if the process runs inside a docker container.
    """
    if Path("/.dockerenv").exists():
        return True
    try:  # check if docker is in control groups
        with open("/proc/self/cgroup") as file:
            return any("docker" in line for line in file)
    except OSError:
        return False
```

### 4.2ã€is_colab
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯Google Colabç¯å¢ƒï¼Œä¼šç”¨åˆ°åé¢çš„check_imshowå‡½æ•°ä¸­ã€‚

is_colabå‡½æ•°ä»£ç ï¼š


```python
def is_colab():
    """ç”¨åˆ°åé¢çš„check_imshowå‡½æ•°ä¸­
    æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ˜¯Google Colabç¯å¢ƒ  Is environment a Google Colab instance?
    """
    try:
        import google.colab
        return True
    except Exception as e:
        return False
```

### 4.3ã€is_pipï¼ˆæ²¡ç”¨åˆ°ï¼‰
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æµ‹å½“å‰æ–‡ä»¶æ˜¯å¦åœ¨pip package(site-packages)æ–‡ä»¶é‡Œï¼Œä¸è¿‡è¿™ä¸ªå‡½æ•°æ²¡ç”¨åˆ°ã€‚

is_pipå‡½æ•°ä»£ç ï¼š


```python
def is_pip():
    """æ²¡ç”¨åˆ°
    å½“å‰æ–‡ä»¶æ˜¯å¦åœ¨pip package(site-packages)æ–‡ä»¶é‡Œ
    Is file in a pip package?
    """
    return 'site-packages' in Path(__file__).absolute().parts
```

## 5ã€file_sizeï¼ˆæ²¡ç”¨åˆ°ï¼‰
è¿™ä¸ªå‡½æ•°æ˜¯è¿”å›æœ¬åœ°æ–‡ä»¶çš„å¤§å°ï¼Œ

åŠŸèƒ½å’Œä¹‹å‰google_utils.pyä¸­çš„gsutil_getsizeå‡½æ•°ï¼ˆè¿”å›ç½‘ç«™é“¾æ¥å¯¹åº”æ–‡ä»¶çš„å¤§å°ï¼‰å¾ˆåƒã€‚

ä¸è¿‡è¿™ä¸ªå‡½æ•°å¹¶æ²¡æœ‰ç”¨åˆ°å“¦ï¼Œéšä¾¿çœ‹çœ‹å°±å¥½ã€‚


```python

def file_size(path):
    # Return file/dir size (MB) è¿”å›æœ¬åœ°æ–‡ä»¶çš„å¤§å°(MB)
    #:params path: è¦æŸ¥è¯¢çš„æ–‡ä»¶åœ°å€
    mb = 1 << 20  # bytes to MiB (1024 ** 2)
    path = Path(path)
    if path.is_file():
        return path.stat().st_size / mb
    elif path.is_dir():
        # .stat(): è¿”å›æ–‡ä»¶ç›¸å…³çŠ¶æ€  st_size: è¿”å›æ–‡ä»¶çš„å¤§å°
        return sum(f.stat().st_size for f in path.glob("**/*") if f.is_file()) / mb
    else:
        return 0.0
```

## 6ã€colorstr
è¿™ä¸ªå‡½æ•°æ˜¯å°†è¾“å‡ºçš„å¼€å¤´å’Œç»“å°¾åŠ ä¸Šé¢œè‰²ï¼Œä½¿å‘½ä»¤è¡Œè¾“å‡ºæ˜¾ç¤ºä¼šæ›´åŠ å¥½çœ‹ã€‚

colorstrå‡½æ•°ä»£ç ï¼š


```python
def colorstr(*input):
    """ç”¨åˆ°ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°  train.pyã€test.pyã€detect.pyç­‰æ–‡ä»¶ä¸­
    æŠŠè¾“å‡ºçš„å¼€å¤´å’Œç»“å°¾åŠ ä¸Šé¢œè‰²  å‘½ä»¤è¡Œè¾“å‡ºæ˜¾ç¤ºä¼šæ›´åŠ å¥½çœ‹  å¦‚: colorstr('blue', 'hello world')
    Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code
    """
    # å¦‚æœè¾“å…¥é•¿åº¦ä¸º1, å°±æ˜¯æ²¡æœ‰é€‰æ‹©é¢œè‰² åˆ™é€‰æ‹©é»˜è®¤é¢œè‰²è®¾ç½® blue + bold
    # args: è¾“å…¥çš„é¢œè‰²åºåˆ— string: è¾“å…¥çš„å­—ç¬¦ä¸²
    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')
    *args, string = input if len(input) > 1 else ("blue", "bold", input[0])  # color arguments, string
    # å®šä¹‰ä¸€äº›åŸºç¡€çš„é¢œè‰² å’Œ å­—ä½“è®¾ç½®
    colors = {
        "black": "\033[30m",  # basic colors
        "red": "\033[31m",
        "green": "\033[32m",
        "yellow": "\033[33m",
        "blue": "\033[34m",
        "magenta": "\033[35m",
        "cyan": "\033[36m",
        "white": "\033[37m",
        "bright_black": "\033[90m",  # bright colors
        "bright_red": "\033[91m",
        "bright_green": "\033[92m",
        "bright_yellow": "\033[93m",
        "bright_blue": "\033[94m",
        "bright_magenta": "\033[95m",
        "bright_cyan": "\033[96m",
        "bright_white": "\033[97m",
        "end": "\033[0m",  # misc
        "bold": "\033[1m",
        "underline": "\033[4m",
    }
    # æŠŠè¾“å‡ºçš„å¼€å¤´å’Œç»“å°¾åŠ ä¸Šé¢œè‰²  å‘½ä»¤è¡Œè¾“å‡ºæ˜¾ç¤ºä¼šæ›´åŠ å¥½çœ‹
    return "".join(colors[x] for x in args) + f"{string}" + colors["end"]
```

è¿™ä¸ªå‡½æ•°ä¼šç”¨åˆ°ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°ä¸­ï¼Œ

è€Œä¸”è¿˜ä¼šå¹¿æ³›ç”¨åœ¨train.pyã€val.pyã€detect.pyç­‰å…¶ä»–æ–‡ä»¶ä¸­å¦‚ï¼š
![image.png](general_imgs/picture_01.png)
å‡½æ•°æ•ˆæœå¦‚ä¸‹ï¼ˆå¯ä»¥çœ‹åˆ°è¾“å‡ºå¼€å¤´ã€ç»“å°¾å˜é‡ä½¿ç”¨å…¶ä»–é¢œè‰²ï¼‰ï¼š
![image-3.png](general_imgs/picture_02.png)


## 7ã€check_online

è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥å½“å‰ä¸»æœºæ˜¯å¦è”ç½‘äº†ã€‚ä¼šåœ¨ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°ä¸­ä½¿ç”¨ã€‚

check_onlineå‡½æ•°ä»£ç :


```python
def check_online():
    """åœ¨ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°ä¸­ä½¿ç”¨
    æ£€æŸ¥å½“å‰ä¸»æœºç½‘ç»œè¿æ¥æ˜¯å¦å¯ç”¨
    """
    import socket  # å¯¼å…¥socketæ¨¡å— å¯è§£å†³åŸºäºtcpå’Œucpåè®®çš„ç½‘ç»œä¼ è¾“
    try:
        # è¿æ¥åˆ°ä¸€ä¸ªip åœ°å€addr("1.1.1.1")çš„TCPæœåŠ¡ä¸Š, ç«¯å£å·port=443 timeout=5 æ—¶é™5ç§’ å¹¶è¿”å›ä¸€ä¸ªæ–°çš„å¥—æ¥å­—å¯¹è±¡
        socket.create_connection(("1.1.1.1", 443), 5)  # check host accessibility
        # æ²¡å‘ç°ä»€ä¹ˆå¼‚å¸¸, è¿æ¥æˆåŠŸ, æœ‰ç½‘, å°±è¿”å›True
        return True
    except OSError:
        # è¿æ¥å¼‚å¸¸, æ²¡ç½‘, è¿”å›False
        return False
```

## 8ã€emojis
è¿™ä¸ªå‡½æ•°æ˜¯å¿½ç•¥æ‰å­—ç¬¦ä¸²ä¸­æ— æ³•ç”¨asciiç¼–ç çš„å†…å®¹(æ¯”å¦‚è¡¨æƒ…ã€å›¾åƒ)ï¼Œè¿”å›Windowsç³»ç»Ÿå¯ä»¥å®‰å…¨ã€å®Œæ•´æ˜¾ç¤ºçš„å­—ç¬¦ä¸²ã€‚ä¼šåœ¨ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°ä¸­ä½¿ç”¨ã€‚

emojiså‡½æ•°ä»£ç ï¼š


```python
def emojis(str=''):
    """åœ¨ä¸‹é¢çš„check_git_statusã€check_requirementsç­‰å‡½æ•°ä¸­ä½¿ç”¨
    è¿”å›Windowsç³»ç»Ÿå¯ä»¥å®‰å…¨ã€å®Œæ•´æ˜¾ç¤ºçš„å­—ç¬¦ä¸²
    Return platform-dependent emoji-safe version of string
    """
    # é€šè¿‡.encode().decode()çš„ç»„åˆå¿½ç•¥æ‰æ— æ³•ç”¨asciiç¼–ç çš„å†…å®¹(æ¯”å¦‚è¡¨æƒ…ã€å›¾åƒ)
    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str
```


```python
## 9ã€check_git_status

è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥å½“å‰çš„ä»£ç ç‰ˆæœ¬æ˜¯å¦æ˜¯æœ€æ–°çš„ã€‚å¦‚æœä¸æ˜¯æœ€æ–°çš„ï¼Œä¼šæç¤ºä½¿ç”¨git pullå‘½ä»¤è¿›è¡Œå‡çº§ã€‚

å‡½æ•°ä»£ç ï¼š
```


```python

@try_except
@WorkingDirectory(ROOT)
def check_git_status(repo="Oneflow-Inc/one-yolo"):
    """ç”¨åœ¨train.pyçš„mainå‡½æ•°çš„ä¸€å¼€å§‹éƒ¨åˆ†
    æ£€æŸ¥å½“å‰ä»£ç ç‰ˆæœ¬æ˜¯å¦æ˜¯æœ€æ–°çš„   å¦‚æœä¸æ˜¯æœ€æ–°çš„ ä¼šæç¤ºä½¿ç”¨git pullå‘½ä»¤è¿›è¡Œå‡çº§
    """
    # YOLOv5 status check, recommend 'git pull' if code is out of date
    url = f"https://github.com/{repo}"
    msg = f", for updates see {url}"
    s = colorstr("github: ")  # string
    # æ£€æŸ¥ç”µè„‘æœ‰æ²¡æœ‰å®‰è£…gitä»“åº“  æ²¡æœ‰å®‰è£…ç›´æ¥æŠ¥å¼‚å¸¸å¹¶è¾“å‡ºå¼‚å¸¸ä¿¡æ¯
    assert Path(".git").exists(), s + "skipping check (not a git repository)" + msg
    # æ£€æŸ¥ä¸»æœºæ˜¯å¦è”ç½‘
    assert check_online(), s + "skipping check (offline)" + msg

    splits = re.split(pattern=r"\s", string=check_output("git remote -v", shell=True).decode())
    matches = [repo in s for s in splits]
    if any(matches):
        remote = splits[matches.index(True) - 1]
    else:
        remote = "Oneflow-Inc"
        check_output(f"git remote add {remote} {url}", shell=True)
    check_output(f"git fetch {remote}", shell=True, timeout=5)  # git fetch
    branch = check_output("git rev-parse --abbrev-ref HEAD", shell=True).decode().strip()  # checked out
    n = int(check_output(f"git rev-list {branch}..{remote}/master --count", shell=True))  # commits behind
    if n > 0:
        # å¦‚æœä¸æ˜¯æœ€æ–°  æå‡å­—ç¬¦s: WARNING...
        pull = "git pull" if remote == "origin" else f"git pull {remote} master"
        s += f"âš ï¸ YOLOv5 is out of date by {n} commit{'s' * (n > 1)}. Use `{pull}` or `git clone {url}` to update."
    else:
        s += f"up to date with {url} âœ…"
    LOGGER.info(s)
```

è¿™ä¸ªå‡½æ•°åªç”¨åœ¨train.pyçš„mainå‡½æ•°çš„ä¸€å¼€å§‹éƒ¨åˆ†ï¼š
![image.png](general_imgs/picture_03.png)

## 10ã€check_pythonã€check_requirements

check_pythonæ˜¯æ£€æŸ¥å½“å‰çš„ç‰ˆæœ¬å·æ˜¯å¦æ»¡è¶³æœ€å°ç‰ˆæœ¬å·minimumï¼Œ

check_requirementsæ˜¯æ£€æŸ¥å·²ç»å®‰è£…çš„åŒ…æ˜¯å¦æ»¡è¶³requirementså¯¹åº”txtæ–‡ä»¶çš„è¦æ±‚ã€‚

check_requirementsä¼šè°ƒç”¨check_pythonã€‚


### 10.1ã€check_python
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥å½“å‰çš„ç‰ˆæœ¬å·æ˜¯å¦æ»¡è¶³æœ€å°ç‰ˆæœ¬å·minimumã€‚

ä¼šåœ¨ä¸‹é¢çš„check_requirementså‡½æ•°è¢«è°ƒç”¨ã€‚

check_pythonå‡½æ•°ä»£ç ï¼š


```python
def check_python(minimum='3.7.0'):
    # Check current python version vs. required python version
    check_version(platform.python_version(), minimum, name='Python ', hard=True)
```

### 10.2ã€check_requirements
è¿™ä¸ªå‡½æ•°ç”¨äºæ£€æŸ¥å·²ç»å®‰è£…çš„åŒ…æ˜¯å¦æ»¡è¶³requirementså¯¹åº”txtæ–‡ä»¶çš„è¦æ±‚ã€‚ä¼šè°ƒç”¨colorstrã€check_pythonã€check_onlineç­‰å‡½æ•°ã€‚

check_requirementså‡½æ•°ä»£ç ï¼š


```python

@TryExcept()
def check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True, cmds=''):
    """ç”¨åœ¨train.pyã€test.pyã€detect.pyç­‰æ–‡ä»¶
    ç”¨äºæ£€æŸ¥å·²ç»å®‰è£…çš„åŒ…æ˜¯å¦æ»¡è¶³requirementså¯¹åº”txtæ–‡ä»¶çš„è¦æ±‚
    Check installed dependencies meet requirements (pass *.txt file or list of packages)
    """
    # Check installed dependencies meet YOLOv5 requirements (pass *.txt file or list of packages or single package str)
    # çº¢è‰²æ˜¾ç¤ºrequirementså•è¯  requirements:
    prefix = colorstr('red', 'bold', 'requirements:')
    # æ£€æŸ¥å½“å‰çš„pythonç‰ˆæœ¬ç¬¦ä¸ç¬¦åˆæœ€ä½ç‰ˆæœ¬è¦æ±‚   check python version
    check_python()  # check python version
    # è§£ærequirements.txtä¸­çš„æ‰€æœ‰åŒ… è§£ææˆlist é‡Œé¢å­˜æ”¾ç€ä¸€ä¸ªä¸ªçš„pkg_resources.Requirementç±»
    # å¦‚: ['matplotlib>=3.2.2', 'numpy>=1.18.5', â€¦â€¦]
    if isinstance(requirements, Path):  # requirements.txt file
        file = requirements.resolve()
        assert file.exists(), f"{prefix} {file} not found, check failed."
        with file.open() as f:
            requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(f) if x.name not in exclude]
    elif isinstance(requirements, str):
        requirements = [requirements]

    s = ''
    n = 0 # ç»Ÿè®¡ä¸‹é¢ç¨‹åºæ›´æ–°åŒ…çš„ä¸ªæ•° number of packages updates
    # ä¾æ¬¡æ£€æŸ¥ç¯å¢ƒä¸­å®‰è£…çš„åŒ…(åŠæ¯ä¸ªåŒ…å¯¹åº”çš„ä¾èµ–åŒ…)æ˜¯å¦æ»¡è¶³requirementsä¸­çš„æ¯ä¸€ä¸ªæœ€ä½è¦æ±‚å®‰è£…åŒ…
    for r in requirements:
        try:
            # pkg_resources.require(file) è¿”å›å¯¹åº”åŒ…æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–åŒ… å½“è¿™äº›åŒ…æœ‰å“ªä¸ªæœªå®‰è£…æˆ–è€…ç‰ˆæœ¬ä¸å¯¹çš„æ—¶å€™å°±ä¼šæŠ¥é”™
            pkg.require(r)
        except (pkg.VersionConflict, pkg.DistributionNotFound):  # exception if requirements not met
            s += f'"{r}" '
            n += 1

    if s and install and AUTOINSTALL:  # check environment variable
        LOGGER.info(f"{prefix} YOLOv5 requirement{'s' * (n > 1)} {s}not found, attempting AutoUpdate...")
        try:
            # å†æ£€æŸ¥å½“å‰ä¸»æœºæ˜¯å¦è”ç½‘
            assert check_online(), "AutoUpdate skipped (offline)"
            # æœ€ååˆ›å»ºä¸€ä¸ªå­è¿›ç¨‹å†æ‰§è¡ŒpipæŒ‡ä»¤å¹¶è¿”å›æ‰§è¡Œç»“æœ
            LOGGER.info(check_output(f'pip install {s} {cmds}', shell=True).decode())
            source = file if 'file' in locals() else requirements
            s = f"{prefix} {n} package{'s' * (n > 1)} updated per {source}\n" \
                f"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\n"
            LOGGER.info(s)
        except Exception as e:
            LOGGER.warning(f'{prefix} âŒ {e}')
```

ç”¨åœ¨train.pyä¸­ï¼š

![image.png](general_imgs/picture_04.png)

val.py ä¸­ï¼š
![image-2.png](general_imgs/picture_05.png)

## 11ã€make_divisibleã€check_img_size
è¿™ä¸¤ä¸ªå‡½æ•°ä¸»è¦æ˜¯ç”¨æ¥çº¦æŸå›¾åƒçš„é•¿æ¬¾æˆ–è€…feature mapçš„é•¿æ¬¾ï¼Œ

å¿…é¡»æ˜¯divisorï¼ˆç­‰äºç®—æ³•çš„æœ€å¤§ä¸‹é‡‡æ ·ç‡ä¸€èˆ¬æ˜¯32ï¼‰çš„æœ€å°å€æ•°ã€‚



```python
def make_divisible(x, divisor):
    """ç”¨åœ¨ä¸‹é¢çš„make_divisibleå‡½æ•°ä¸­  yolo.pyçš„parse_modelå‡½æ•°å’Œcommom.pyçš„AutoShapeå‡½æ•°ä¸­
    å–å¤§äºç­‰äºxä¸”æ˜¯divisorçš„æœ€å°å€æ•°
    Returns x evenly divisible by divisor
    """
    if isinstance(divisor, flow.Tensor):
        divisor = int(divisor.max())  # to int
    # math.ceil å‘ä¸Šå–æ•´
    return math.ceil(x / divisor) * 
```

è¿™ä¸ªå‡½æ•°ç”¨åœ¨ä¸‹é¢çš„make_divisibleå‡½æ•°ä¸­åŠ yolo.pyçš„parse_modelå‡½æ•°å’Œcommom.pyçš„AutoShapeå‡½æ•°ä¸­ï¼š
![image.png](general_imgs/picture_06.png)

### 11.2ã€check_img_size
è¿™ä¸ªå‡½æ•°æ˜¯ä¸ºäº†ä¿è¯img_sizeæ˜¯èƒ½è¢«sï¼ˆ32ï¼‰æ•´é™¤ï¼Œå¦‚æœä¸èƒ½å°±è¿”å›å¤§äºç­‰äºimg_sizeä¸”æ˜¯sçš„æœ€å°å€æ•°ã€‚

è¿™ä¸ªå‡½æ•°æœ¬è´¨æ˜¯é€šè¿‡è°ƒç”¨make_divisibleå‡½æ•°å®ç°çš„ã€‚

check_img_sizeå‡½æ•°ä»£ç ï¼š


```python
def check_img_size(imgsz, s=32, floor=0):
    """è¿™ä¸ªå‡½æ•°ä¸»è¦ç”¨äºtrain.pyä¸­å’Œdetect.pyä¸­  ç”¨æ¥æ£€æŸ¥å›¾ç‰‡çš„é•¿å®½æ˜¯å¦ç¬¦åˆè§„å®š
    æ£€æŸ¥img_sizeæ˜¯å¦èƒ½è¢«sæ•´é™¤ï¼Œè¿™é‡Œé»˜è®¤sä¸º32  è¿”å›å¤§äºç­‰äºimg_sizeä¸”æ˜¯sçš„æœ€å°å€æ•°
    Verify img_size is a multiple of stride s
    """
    # Verify image size is a multiple of stride s in each dimension
    if isinstance(imgsz, int):  # integer i.e. img_size=640
        # å–å¤§äºç­‰äºxçš„æœ€å°å€¼ä¸”è¯¥å€¼èƒ½è¢«divisoræ•´é™¤
        new_size = max(make_divisible(imgsz, int(s)), floor)
    else:  # list i.e. img_size=[640, 480]
        imgsz = list(imgsz)  # convert to list if tuple
        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]
    if new_size != imgsz:
        LOGGER.warning(f"WARNING: --img-size {imgsz} must be multiple of max stride {s}, updating to {new_size}")
    return new_size
```

ç”¨æ¥ä¿è¯imgçš„é•¿å®½ç¬¦åˆè§„å®šï¼Œç”¨åœ¨val.py , detect.py ,train.pyä¸­ï¼š
![image.png](general_imgs/picture_07.png)


## 12ã€check_imshow
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥ä¸€ä¸‹å‰ç¯å¢ƒæ˜¯å¦å¯ä»¥ä½¿ç”¨opencv.imshowæ˜¾ç¤ºå›¾ç‰‡ã€‚


```python
def check_imshow():
    """ç”¨åœ¨detect.pyä¸­  ä½¿ç”¨webcamçš„æ—¶å€™è°ƒç”¨
    æ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦å¯ä»¥ä½¿ç”¨opencv.imshowæ˜¾ç¤ºå›¾ç‰‡
    ä¸»è¦æœ‰ä¸¤ç‚¹é™åˆ¶: Dockerç¯å¢ƒ + Google Colabç¯å¢ƒ
    """
    # Check if environment supports image displays
    try:
        assert not is_docker(), "cv2.imshow() is disabled in Docker environments"
        assert not is_colab(), "cv2.imshow() is disabled in Google Colab environments"
        # åˆå§‹åŒ–ä¸€å¼ å›¾ç‰‡æ£€æŸ¥ä¸‹opencvæ˜¯å¦å¯ç”¨
        cv2.imshow("test", np.zeros((1, 1, 3)))
        cv2.waitKey(1)
        cv2.destroyAllWindows()
        cv2.waitKey(1)
        return True
    except Exception as e:
        LOGGER.warning(f"WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\n{e}")
        return False
```

ä¼šåœ¨detect.pyä¸­ä½¿ç”¨webcamçš„æ—¶å€™è°ƒç”¨ï¼š

![image.png](general_imgs/picture_08.png)

## 13ã€check_file
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥æœ¬éƒ½ç›¸å…³æ–‡ä»¶è·¯å¾„èƒ½å¦æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶ï¼Œæ²¡æ‰¾åˆ°å°±è¯´æ˜æ–‡ä»¶ä¸¢å¤±äº†ï¼Œ

è¿”å›ç©ºï¼›

å¦‚æœä¼ å…¥çš„æ˜¯ä¸€ä¸ªç½‘ç»œåœ°å€å°±ç›´æ¥ä¸‹è½½è¿™ä¸ªæ–‡ä»¶ï¼›

å¦åˆ™æ‰¾åˆ°å°±è¿”å›æœ¬åœ°åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶åã€‚è¿™ä¸ªå‡½æ•°å¾ˆæœ‰ç”¨ï¼Œç”¨çš„å¾ˆå¹¿ã€‚



```python
def check_file(file, suffix=""):
    """ç”¨åœ¨train.pyå’Œtest.pyæ–‡ä»¶ä¸­  æ£€æŸ¥æœ¬åœ°æœ‰æ²¡æœ‰è¿™ä¸ªæ–‡ä»¶
    æ£€æŸ¥ç›¸å…³æ–‡ä»¶è·¯å¾„èƒ½å¦æ‰¾åˆ°æ–‡ä»¶ å¹¶è¿”å›æ–‡ä»¶å
    Search/download file (if necessary) and return path
    """
    # Search/download file (if necessary) and return path
    check_suffix(file, suffix)  # optional
    file = str(file)  # convert to str()
    # å¦‚æœä¼ è¿›æ¥çš„æ˜¯æ–‡ä»¶æˆ–è€…æ˜¯â€™â€˜, ç›´æ¥è¿”å›æ–‡ä»¶åstr
    if Path(file).is_file() or not file:  # exists
        return file
    # å¦‚æœä¼ è¿›æ¥çš„ä»¥ 'http:/' æˆ–è€… 'https:/' å¼€å¤´çš„urlåœ°å€, å°±ä¸‹è½½
    elif file.startswith(("http:/", "https:/")):  # download
        url = file  # warning: Pathlib turns :// -> :/
        file = Path(urllib.parse.unquote(file).split("?")[0]).name  # '%2F' to '/', split https://url.com/file.txt?auth
        if Path(file).is_file():
            LOGGER.info(f"Found {url} locally at {file}")  # file already exists
        else:
            LOGGER.info(f"Downloading {url} to {file}...")
            # ä½¿ç”¨flow.hub.download_url_to_fileä»urlåœ°å€ä¸Šä¸­ä¸‹è½½æ–‡ä»¶åä¸ºfileçš„æ–‡ä»¶
            flow.hub.download_url_to_file(url, file)
            # æ£€æŸ¥æ˜¯å¦ä¸‹è½½æˆåŠŸ
            assert Path(file).exists() and Path(file).stat().st_size > 0, f"File download failed: {url}"  # check
            # è¿”å›ä¸‹è½½çš„æ–‡ä»¶å
        return file
    elif file.startswith("clearml://"):  # ClearML Dataset ID
        assert "clearml" in sys.modules, "ClearML is not installed, so cannot use ClearML dataset. Try running 'pip install clearml'."
        return file
    else:  # search
        files = []
        for d in "data", "models", "utils":  # search directories
            files.extend(glob.glob(str(ROOT / d / "**" / file), recursive=True))  # find file
        assert len(files), f"File not found: {file}"  # assert file was found
        assert len(files) == 1, f"Multiple files match '{file}', specify exact path: {files}"  # assert unique
        # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„æ–‡ä»¶å
        return files[0]  # return file
```

1. åœ¨train.pyä¸­ä½¿ç”¨ï¼ˆæ£€æŸ¥æœ¬åœ°dataã€cfgã€hypç­‰æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
2. åœ¨test.pyä¸­ä½¿ç”¨ï¼ˆæ£€æŸ¥æœ¬åœ°dataæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰


## 14ã€check_dataset
è¿™ä¸ªå‡½æ•°æ˜¯æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æŒ‡å®šçš„æ•°æ®é›†ï¼Œæ²¡ç”¨å°±ä»torchåº“ä¸­ä¸‹è½½å¹¶è§£å‹æ•°æ®é›†ã€‚

check_datasetå‡½æ•°ä»£ç :


```python
def check_dataset(data, autodownload=True):
    # Download, check and/or unzip dataset if not found locally
    """ç”¨åœ¨train.pyå’Œdetect.pyä¸­ æ£€æŸ¥æœ¬åœ°æœ‰æ²¡æœ‰æ•°æ®é›†
    æ£€æŸ¥æ•°æ®é›† å¦‚æœæœ¬åœ°æ²¡æœ‰åˆ™ä»one-yolov5åº“ä¸­ä¸‹è½½å¹¶è§£å‹æ•°æ®é›†
    :params data: æ˜¯ä¸€ä¸ªè§£æè¿‡çš„data_dict   len=7
                  ä¾‹å¦‚: ['path'='../datasets/coco128', 'train','val', 'test', 'nc', 'names', 'download']
    :params autodownload: å¦‚æœæœ¬åœ°æ²¡æœ‰æ•°æ®é›†æ˜¯å¦éœ€è¦ç›´æ¥ä»one-yolov5åº“ä¸­ä¸‹è½½æ•°æ®é›†  é»˜è®¤True
    """
    # Download (optional)
    extract_dir = ""
    if isinstance(data, (str, Path)) and str(data).endswith(".zip"):  # i.e. gs://bucket/dir/coco128.zip
        download(data, dir=DATASETS_DIR, unzip=True, delete=False, curl=False, threads=1)
        data = next((DATASETS_DIR / Path(data).stem).rglob("*.yaml"))
        extract_dir, autodownload = data.parent, False

    # Read yaml (optional)
    if isinstance(data, (str, Path)):
        with open(data, errors="ignore") as f:
            data = yaml.safe_load(f)  # dictionary

    # Checks
    for k in "train", "val", "nc":
        assert k in data, f"data.yaml '{k}:' field missing âŒ"
    if "names" not in data:
        LOGGER.warning("data.yaml 'names:' field missing âš ï¸, assigning default names 'class0', 'class1', etc.")
        data["names"] = [f"class{i}" for i in range(data["nc"])]  # default names

    # Resolve paths
    path = Path(extract_dir or data.get("path") or "")  # optional 'path' default to '.'
    if not path.is_absolute():
        path = (ROOT / path).resolve()
    for k in "train", "val", "test":
        if data.get(k):  # prepend path
            data[k] = str(path / data[k]) if isinstance(data[k], str) else [str(path / x) for x in data[k]]

    # Parse yaml
    train, val, test, s = (data.get(x) for x in ("train", "val", "test", "download"))
    if val:
        # path.resolve() è¯¥æ–¹æ³•å°†ä¸€äº›çš„ è·¯å¾„/è·¯å¾„æ®µ è§£æä¸ºç»å¯¹è·¯å¾„
        # val: [WindowsPath('E:/yolo_v5/datasets/coco128/images/train2017')]
        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path
        # å¦‚æœvalä¸å­˜åœ¨ è¯´æ˜æœ¬åœ°ä¸å­˜åœ¨æ•°æ®é›†
        if not all(x.exists() for x in val):
            LOGGER.info("\nDataset not found âš ï¸, missing paths %s" % [str(x) for x in val if not x.exists()])
            if not s or not autodownload:
                raise Exception("Dataset not found âŒ")
            t = time.time()
            root = path.parent if "path" in data else ".."  # unzip directory i.e. '../'
            if s.startswith("http") and s.endswith(".zip"):  # URL
                f = Path(s).name  # filename
                LOGGER.info(f"Downloading {s} to {f}...")
                flow.hub.download_url_to_file(s, f)
                Path(root).mkdir(parents=True, exist_ok=True)  # create root
                ZipFile(f).extractall(path=root)  # unzip
                Path(f).unlink()  # remove zip
                r = None  # success
            # å¦‚æœä¸‹è½½åœ°å€sæ˜¯bashå¼€å¤´å°±ä½¿ç”¨bashæŒ‡ä»¤ä¸‹è½½æ•°æ®é›†
            elif s.startswith("bash "):  # bash script
                LOGGER.info(f"Running {s} ...")
                # ä½¿ç”¨bashå‘½ä»¤ä¸‹è½½
                r = os.system(s)
            # å¦åˆ™ä¸‹è½½åœ°å€å°±æ˜¯ä¸€ä¸ªpythonè„šæœ¬ æ‰§è¡Œpythonè„šæœ¬ä¸‹è½½æ•°æ®é›†
            else:  # python script
                r = exec(s, {"yaml": data})  # return None
            dt = f"({round(time.time() - t, 1)}s)"
            s = f"success âœ… {dt}, saved to {colorstr('bold', root)}" if r in (0, None) else f"failure {dt} âŒ"
            LOGGER.info(f"Dataset download {s}")
    check_font("Arial.ttf" if is_ascii(data["names"]) else "Arial.Unicode.ttf", progress=True)  # download fonts
    return data  # dictionary
```


![image.png](general_imgs/picture_09.png)

### 15ã€download
è¿™ä¸ªå‡½æ•°æ˜¯å°†urlä¸­çš„æ–‡ä»¶ä¸‹è½½ä¸‹æ¥ï¼Œå†è§£å‹ã€‚ä½†æ˜¯è¿™ä¸ªæ–‡ä»¶å¹¶æ²¡æœ‰åœ¨ç¨‹åºä¸­è¢«è°ƒç”¨ï¼Œ

flow.hub.download_url_to_fileç³»ç»Ÿå‡½æ•°å’Œgoogle_utils.py

ä¸­çš„attempt_downloadå‡½æ•°è¿›è¡Œä¸‹è½½æ–‡ä»¶ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªå‡½æ•°éšä¾¿çœ‹çœ‹å°±å¥½ã€‚



```python
def download(url, dir=".", unzip=True, delete=True, curl=False, threads=1, retry=3):
    # Multi-threaded file download and unzip function, used in data.yaml for autodownload
    """åœ¨coco.yamlä¸­ä¸‹è½½æ•°æ®é›†
    Multi-threaded file download and unzip function
    :params url: ä¸‹è½½æ–‡ä»¶çš„urlåœ°å€
    :params dir: ä¸‹è½½ä¸‹æ¥æ–‡ä»¶ä¿å­˜çš„ç›®å½•
    :params unzip: ä¸‹è½½åæ–‡ä»¶æ˜¯å¦éœ€è¦è§£å‹
    :params delete: è§£å‹ååŸæ–‡ä»¶(æœªè§£å‹)æ˜¯å¦éœ€è¦åˆ é™¤
    :params curl: æ˜¯å¦ä½¿ç”¨cmd curlè¯­å¥ä¸‹è½½æ–‡ä»¶  Falseå°±ä½¿ç”¨torch.hubä¸‹è½½
    :params threads: ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶éœ€è¦çš„çº¿ç¨‹æ•°
    """
    def download_one(url, dir):
        """
        Download 1 file
        :params url: æ–‡ä»¶ä¸‹è½½åœ°å€  Path(url).name=æ–‡ä»¶å
        :params dir: æ–‡ä»¶ä¿å­˜çš„ç›®å½•
        """
        # Download 1 file
        success = True
        f = dir / Path(url).name  # filename
        if Path(url).is_file():  # exists in current path
            Path(url).rename(f)  # move to dir
        # è¿™ä¸ªç›®å½•ä¸‹ä¸å­˜åœ¨è¿™ä¸ªæ–‡ä»¶ å°±ç›´æ¥ä¸‹è½½
        elif not f.exists():
            LOGGER.info(f"Downloading {url} to {f}...")
            for i in range(retry + 1):
                if curl:
                    s = "sS" if threads > 1 else ""  # silent
                    r = os.system(f'curl -{s}L "{url}" -o "{f}" --retry 9 -C -')  # curl download with retry, continue
                    success = r == 0
                else:
                    flow.hub.download_url_to_file(url, f, progress=threads == 1)  # torch download
                    success = f.is_file()
                if success:
                    break
                elif i < retry:
                    LOGGER.warning(f"Download failure, retrying {i + 1}/{retry} {url}...")
                else:
                    LOGGER.warning(f"Failed to download {url}...")
        
        # å¦‚æœéœ€è¦è§£å‹ ä¸”ä¸‹è½½çš„æ–‡ä»¶åç¼€æ˜¯ '.zip' æˆ– '.gz'
        if unzip and success and f.suffix in (".zip", ".gz"):
            LOGGER.info(f"Unzipping {f}...")
            if f.suffix == ".zip":
                ZipFile(f).extractall(path=dir)  # unzip
            elif f.suffix == ".gz":
                os.system(f"tar xfz {f} --directory {f.parent}")  # unzip
            # è§£å‹åæ˜¯å¦éœ€è¦åˆ é™¤æœªè§£å‹çš„æ–‡ä»¶
            if delete:
                f.unlink()  # remove zip

    dir = Path(dir)
    dir.mkdir(parents=True, exist_ok=True)  # make directory
    if threads > 1: # ä½¿ç”¨çº¿ç¨‹æ± 
        # å®šä¹‰äº†ä¸€ä¸ªçº¿ç¨‹æ± , æœ€å¤šåˆ›å»ºthreadsä¸ªçº¿ç¨‹
        pool = ThreadPool(threads)
        # è¿›ç¨‹æ± ä¸­çš„è¯¥æ–¹æ³•ä¼šå°† iterable å‚æ•°ä¼ å…¥çš„å¯è¿­ä»£å¯¹è±¡åˆ†æˆ chunksize ä»½ä¼ é€’ç»™ä¸åŒçš„è¿›ç¨‹æ¥å¤„ç†ã€‚
        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # multi-threaded
        pool.close()
        pool.join()
    else:
        for u in [url] if isinstance(url, (str, Path)) else url:
            download_one(u, dir)
```

## 16ã€clean_str
è¿™ä¸ªå‡½æ•°æ˜¯å°†å­—ç¬¦ä¸²ä¸­ä¸€äº›å¥‡æ€ªçš„ç¬¦å· â€œ|@#!Â¡Â·$â‚¬%&()=?Â¿^*;:,Â¨Â´><+â€ æ¢æˆä¸‹åˆ’çº¿ â€˜_â€™ã€‚


```python
def clean_str(s):
    """åœ¨datasets.pyä¸­çš„LoadStreamsç±»ä¸­è¢«è°ƒç”¨
    å­—ç¬¦ä¸²sé‡Œåœ¨patternä¸­å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿_  æ³¨æ„patternä¸­[]ä¸èƒ½çœ
    Cleans a string by replacing special characters with underscore _
    """
    # re: ç”¨æ¥åŒ¹é…å­—ç¬¦ä¸²ï¼ˆåŠ¨æ€ã€æ¨¡ç³Šï¼‰çš„æ¨¡å—  æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
    # pattern: è¡¨ç¤ºæ­£åˆ™ä¸­çš„æ¨¡å¼å­—ç¬¦ä¸²  repl: å°±æ˜¯replacementçš„å­—ç¬¦ä¸²  string: è¦è¢«å¤„ç†, è¦è¢«æ›¿æ¢çš„é‚£ä¸ªstringå­—ç¬¦ä¸²
    # æ‰€ä»¥è¿™å¥è¯æ‰§è¡Œçš„æ˜¯å°†å­—ç¬¦ä¸²sé‡Œåœ¨patternä¸­çš„å­—ç¬¦ä¸²æ›¿æ¢ä¸º "_"
    # Cleans a string by replacing special characters with underscore _
    return re.sub(pattern="[|@#!Â¡Â·$â‚¬%&()=?Â¿^*;:,Â¨Â´><+]", repl="_", string=s)
```

åªç”¨åœ¨datasets.pyä¸­çš„LoadStreamsç±»ä¸­ï¼š
![image.png](general_imgs/picture_10.png)

## 17. one_cycle

è¿™ä¸ªå‡½æ•°æ˜¯ä¸€ç§ç‰¹æ®Šçš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ã€‚æ¥è‡ªè¿™ç¯‡è®ºæ–‡ï¼š [one_cycle](https://arxiv.org/pdf/1803.09820.pdf). æ„Ÿå…´è¶£çš„
æœ‹å‹å¯ä»¥è¯»ä¸€è¯»ã€‚



```python
def one_cycle(y1=0.0, y2=1.0, steps=100):
    """ç”¨åœ¨train.pyçš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥æ¨¡å—
    one_cycle lr  lrå…ˆå¢åŠ , å†å‡å°‘, å†ä»¥æ›´å°çš„æ–œç‡å‡å°‘
    è®ºæ–‡: https://arxiv.org/pdf/1803.09820.pdf
    """
    # lambda function for sinusoidal ramp from y1 to y2 https://arxiv.org/pdf/1812.01187.pdf
    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1
```

ä¸€èˆ¬ä½¿ç”¨one_cycleçš„æ•ˆæœä¼šæ¯”è¾ƒå¥½ã€‚
![image.png](general_imgs/picture_11.png)

## 18. labels_to_class_weights & labels_to_image_weights
è¿™ä¸¤ä¸ªå‡½æ•°æ˜¯è”åˆä½¿ç”¨çš„ã€‚

æœ€ç»ˆçš„ç›®çš„æ˜¯ä¸ºäº†åœ¨æ•°æ®é›†ä¸­é‡‡æ ·çš„æ—¶å€™ï¼Œä¸ä½¿ç”¨éšæœºé‡‡æ ·ï¼Œè€Œæ˜¯ä½¿ç”¨æ›´åŠ ç§‘å­¦çš„æŒ‰å›¾ç‰‡æƒé‡è¿›è¡Œé‡‡æ ·ã€‚

ç¬¬ä¸€ä¸ªå‡½æ•°labels_to_class_weightsæ˜¯ä¸ºäº†å¾—åˆ°æ•°æ®é›†ä¸­æ‰€æœ‰ç±»åˆ«çš„æƒé‡ï¼ˆé¢‘ç‡å¤§çš„æƒé‡å°ï¼‰ã€‚

ç¬¬äºŒä¸ªå‡½æ•°labels_to_image_weightsæ˜¯åˆ©ç”¨labels_to_class_weightså‡½æ•°å¾—åˆ°çš„ç±»åˆ«æƒé‡å¾—åˆ°æ¯å¼ å›¾ç‰‡å¯¹åº”çš„ä¸€ä¸ªæƒé‡ã€‚

ç„¶ååˆ©ç”¨æ¯å¼ å›¾ç‰‡çš„æƒé‡åœ¨å½“å‰batchè¿›è¡Œé‡‡æ ·ï¼Œè¿™æ ·çš„é‡‡æ ·æ–¹å¼ä¼šæ›´åŠ ç§‘å­¦ç‚¹ã€‚

ä¸¤ä¸ªå‡½æ•°éƒ½åªåœ¨train.pyä¸­ä½¿ç”¨ï¼Œä¸”æ˜¯åŒæ—¶ä½¿ç”¨çš„å¦‚å›¾ï¼š
![image.png](general_imgs/imgs_00.png)
![image-2.png](general_imgs/imgs_01.png)

## 18.1 ã€labels_to_class_weights
è¿™ä¸ªå‡½æ•°æ˜¯ä»è®­ç»ƒ(gt)æ ‡ç­¾è·å¾—æ¯ä¸ªç±»çš„æƒé‡ ï¼Œæ ‡ç­¾é¢‘ç‡é«˜çš„ç±»æƒé‡ä½ã€‚

labels_to_class_weightså‡½æ•°ä»£ç ï¼š


```python
def labels_to_class_weights(labels, nc=80):
    """ç”¨åœ¨train.pyä¸­  å¾—åˆ°æ¯ä¸ªç±»åˆ«çš„æƒé‡   æ ‡ç­¾é¢‘ç‡é«˜çš„ç±»æƒé‡ä½
    ä»è®­ç»ƒ(gt)æ ‡ç­¾è·å¾—æ¯ä¸ªç±»çš„æƒé‡  æ ‡ç­¾é¢‘ç‡é«˜çš„ç±»æƒé‡ä½
    Get class weights (inverse frequency) from training labels
    :params labels: gtæ¡†çš„æ‰€æœ‰çœŸå®æ ‡ç­¾labels
    :params nc: æ•°æ®é›†çš„ç±»åˆ«æ•°
    :return torch.from_numpy(weights): æ¯ä¸€ä¸ªç±»åˆ«æ ¹æ®labelså¾—åˆ°çš„å æ¯”(æ¬¡æ•°è¶Šå¤šæƒé‡è¶Šå°) tensor
    """
    if labels[0] is None:  # no labels loaded
        return flow.Tensor()

    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO
    # classes: æ‰€æœ‰æ ‡ç­¾å¯¹åº”çš„ç±»åˆ«labels   labels[:, 0]: ç±»åˆ«   .astype(np.int): å–æ•´
    classes = labels[:, 0].astype(np.int)  # labels = [labels_num, class+xywh]
    # weight: è¿”å›æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•° [1, nc]
    weights = np.bincount(classes, minlength=nc)  # occurrences per class

    # Prepend gridpoint count (for uCE training)
    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image
    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start

    # å°†å‡ºç°æ¬¡æ•°ä¸º0çš„ç±»åˆ«æƒé‡å…¨éƒ¨å–1  replace empty bins with 1
    weights[weights == 0] = 1
    # å…¶ä»–æ‰€æœ‰çš„ç±»åˆ«çš„æƒé‡å…¨éƒ¨å–æ¬¡æ•°çš„å€’æ•°  number of targets per class
    weights = 1 / weights
    # normalize æ±‚å‡ºæ¯ä¸€ç±»åˆ«çš„å æ¯”
    weights /= weights.sum()
    return flow.from_numpy(weights)  # numpy -> tensor

```

## 18.2 labels_to_image_weights

è¿™ä¸ªå‡½æ•°æ˜¯åˆ©ç”¨æ¯å¼ å›¾ç‰‡çœŸå®gtæ¡†çš„çœŸå®æ ‡ç­¾labelså’Œä¸Šä¸€æ­¥labels_to_class_weightså¾—åˆ°çš„æ¯ä¸ªç±»åˆ«çš„æƒé‡å¾—åˆ°æ•°æ®é›†ä¸­æ¯

å¼ å›¾ç‰‡å¯¹åº”çš„æƒé‡ã€‚

labels_to_image_weightså‡½æ•°ä»£ç ï¼š


```python
def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):
    """ç”¨åœ¨train.pyä¸­ åˆ©ç”¨ä¸Šé¢å¾—åˆ°çš„æ¯ä¸ªç±»åˆ«çš„æƒé‡å¾—åˆ°æ¯ä¸€å¼ å›¾ç‰‡çš„æƒé‡  å†å¯¹å›¾ç‰‡è¿›è¡ŒæŒ‰æƒé‡è¿›è¡Œé‡‡æ ·
    é€šè¿‡æ¯å¼ å›¾ç‰‡çœŸå®gtæ¡†çš„çœŸå®æ ‡ç­¾labelså’Œä¸Šä¸€æ­¥labels_to_class_weightså¾—åˆ°çš„æ¯ä¸ªç±»åˆ«çš„æƒé‡è¿›è¡Œé‡‡æ ·
    Produces image weights based on class_weights and image contents
    :params labels: æ¯å¼ å›¾ç‰‡çœŸå®gtæ¡†çš„çœŸå®æ ‡ç­¾
    :params nc: æ•°æ®é›†çš„ç±»åˆ«æ•° é»˜è®¤80
    :params class_weights: [80] ä¸Šä¸€æ­¥labels_to_class_weightså¾—åˆ°çš„æ¯ä¸ªç±»åˆ«çš„æƒé‡
    """
    # class_counts: æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•°  [num_labels, nc]  æ¯ä¸€è¡Œæ˜¯å½“å‰è¿™å¼ å›¾ç‰‡æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•°  num_labels=å›¾ç‰‡æ•°é‡=labelæ•°é‡
    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])
    # [80] -> [1, 80]
    # æ•´ä¸ªæ•°æ®é›†çš„æ¯ä¸ªç±»åˆ«æƒé‡[1, 80] *  æ¯å¼ å›¾ç‰‡çš„æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•°[num_labels, 80] = å¾—åˆ°æ¯ä¸€å¼ å›¾ç‰‡æ¯ä¸ªç±»å¯¹åº”çš„æƒé‡[128, 80]
    # å¦å¤–æ³¨æ„: è¿™é‡Œä¸æ˜¯çŸ©é˜µç›¸ä¹˜, æ˜¯å…ƒç´ ç›¸ä¹˜ [1, 80] å’Œæ¯ä¸€è¡Œå›¾ç‰‡çš„æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•° [1, 80] åˆ†åˆ«æŒ‰å…ƒç´ ç›¸ä¹˜
    # å†sum(1): æŒ‰è¡Œç›¸åŠ   å¾—åˆ°æœ€ç»ˆimage_weights: å¾—åˆ°æ¯ä¸€å¼ å›¾ç‰‡å¯¹åº”çš„é‡‡æ ·æƒé‡[128]
    return (class_weights.reshape(1, nc) * class_counts).sum(1)
```

## 19. coco80_to_coco91_class
è¿™ä¸ªå‡½æ•°æ˜¯å°†80ä¸ªç±»çš„cocoç´¢å¼•æ¢æˆ91ç±»çš„cocoç´¢å¼•ã€‚

coco80_to_coco91_classå‡½æ•°ä»£ç :


```python
def coco80_to_coco91_class():
    """ç”¨åœ¨test.pyä¸­   ä»80ç±»æ˜ å°„åˆ°91ç±»çš„cocoç´¢å¼• å–å¾—å¯¹åº”çš„class id
    å°†80ä¸ªç±»çš„cocoç´¢å¼•æ¢æˆ91ç±»çš„cocoç´¢å¼•
    :return x: ä¸º80ç±»çš„æ¯ä¸€ç±»åœ¨91ç±»ä¸­çš„ä½ç½®
    """
    # converts 80-index (val2014) to 91-index (paper)
    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/
    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\n')
    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\n')
    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco
    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet
    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,
         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,
         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]
    return x

```

åœ¨val.pyä¸­å®šä¹‰ï¼š
![image.png](general_imgs/picture_12.png)

## 20. clip_coords
è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ï¼š

å°†boxesçš„åæ ‡(x1y1x2y2 å·¦ä¸Šè§’å³ä¸‹è§’)é™å®šåœ¨å›¾åƒçš„å°ºå¯¸(img_shape hw)å†…ï¼Œé˜²æ­¢å‡ºç•Œã€‚

è¿™ä¸ªå‡½æ•°ä¼šç”¨åœ¨ä¸‹é¢çš„xyxy2xywhnã€save_one_boxdç­‰å‡½æ•°ä¸­ï¼Œå¾ˆé‡è¦ï¼Œå¿…é¡»æŒæ¡ã€‚

clip_coordså‡½æ•°ä»£ç ï¼š


```python
def clip_coords(boxes, shape):
    """ç”¨åœ¨ä¸‹é¢çš„xyxy2xywhnã€save_one_boxdç­‰å‡½æ•°ä¸­
    å°†boxesçš„åæ ‡(x1y1x2y2 å·¦ä¸Šè§’å³ä¸‹è§’)é™å®šåœ¨å›¾åƒçš„å°ºå¯¸(img_shape hw)å†…
    Clip bounding x1y1x2y2 bounding boxes to image shape (height, width)
    """
    # Clip bounding xyxy bounding boxes to image shape (height, width)
    if isinstance(boxes, flow.Tensor):  # faster individually
        boxes[:, 0].clamp_(0, shape[1])  # x1
        boxes[:, 1].clamp_(0, shape[0])  # y1
        boxes[:, 2].clamp_(0, shape[1])  # x2
        boxes[:, 3].clamp_(0, shape[0])  # y2
    else:  # np.array (faster grouped)
        boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2
        boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2

```

## 21. scale_coords
è¿™ä¸ªå‡½æ•°æ˜¯å°†åæ ‡coords(x1y1x2y2)ä»img1_shapeå°ºå¯¸ç¼©æ”¾åˆ°img0_shapeå°ºå¯¸ã€‚

xçš„æ­£åæ ‡æ˜¯å‘å³ï¼Œyçš„æ­£åæ ‡æ˜¯å‘ä¸‹ã€‚è¿™ä¸ªå‡½æ•°ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚

scale_coordså‡½æ•°ä»£ç ï¼š


```python
def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):
    """ç”¨åœ¨detect.pyå’Œtest.pyä¸­  å°†é¢„æµ‹åæ ‡ä»feature mapæ˜ å°„å›åŸå›¾
    å°†åæ ‡coords(x1y1x2y2)ä»img1_shapeç¼©æ”¾åˆ°img0_shapeå°ºå¯¸
    Rescale coords (xyxy) from img1_shape to img0_shape
    :params img1_shape: coordsç›¸å¯¹äºçš„shapeå¤§å°
    :params coords: è¦è¿›è¡Œç¼©æ”¾çš„boxåæ ‡ä¿¡æ¯ x1y1x2y2  å·¦ä¸Šè§’ + å³ä¸‹è§’
    :params img0_shape: è¦å°†coordsç¼©æ”¾åˆ°ç›¸å¯¹çš„ç›®æ ‡shapeå¤§å°
    :params ratio_pad: ç¼©æ”¾æ¯”ä¾‹gainå’Œpadå€¼   Noneå°±å…ˆè®¡ç®—gainå’Œpadå€¼å†pad+scale  ä¸ä¸ºç©ºå°±ç›´æ¥pad+scale
    """
    # ratio_padä¸ºç©ºå°±å…ˆç®—æ”¾ç¼©æ¯”ä¾‹gainå’Œpadå€¼ calculate from img0_shape
    if ratio_pad is None:
        # gain  = old / new  å–é«˜å®½ç¼©æ”¾æ¯”ä¾‹ä¸­è¾ƒå°çš„,ä¹‹åè¿˜å¯ä»¥å†pad  å¦‚æœç›´æ¥å–å¤§çš„, è£å‰ªå°±å¯èƒ½å‡å»ç›®æ ‡
        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])
        # wh padding  whä¸­æœ‰ä¸€ä¸ªä¸º0  ä¸»è¦æ˜¯padå¦ä¸€ä¸ª
        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2
    else:
        gain = ratio_pad[0][0]  # æŒ‡å®šæ¯”ä¾‹
        pad = ratio_pad[1]  # æŒ‡å®špadå€¼

    # å› ä¸ºpad = img1_shape - img0_shape æ‰€ä»¥è¦æŠŠå°ºå¯¸ä»img1 -> img0 å°±åŒæ ·ä¹Ÿéœ€è¦å‡å»pad
    # å¦‚æœimg1_shape>img0_shape  pad>0   coordsä»å¤§å°ºå¯¸ç¼©æ”¾åˆ°å°å°ºå¯¸ å‡å»pad ç¬¦åˆ
    # å¦‚æœimg1_shape<img0_shape  pad<0   coordsä»å°å°ºå¯¸ç¼©æ”¾åˆ°å¤§å°ºå¯¸ å‡å»pad ç¬¦åˆ
    coords[:, [0, 2]] -= pad[0]  # x padding
    coords[:, [1, 3]] -= pad[1]  # y padding
    # ç¼©æ”¾scale
    coords[:, :4] /= gain
    # é˜²æ­¢æ”¾ç¼©åçš„åæ ‡è¿‡ç•Œ è¾¹ç•Œå¤„ç›´æ¥å‰ªåˆ‡
    clip_coords(coords, img0_shape)
    return coords

```

![image.png](general_imgs/picture_13.png)
![image-2.png](general_imgs/picture_14.png)

##  22. xyxy2xywh & xywh2xyxy
è¿™ä¸¤ä¸ªå‡½æ•°æ˜¯ä¸¤ä¸ªç›¸åçš„è¿‡ç¨‹ã€‚

xyxy2xywhæ˜¯å°†é¢„æµ‹ä¿¡æ¯xyxyæ ¼å¼è½¬åŒ–ä¸ºxywhçš„æ ¼å¼ï¼Œè€Œxywh2xyxyæ˜¯å°†é¢„æµ‹ä¿¡æ¯xywhæ ¼å¼è½¬åŒ–ä¸ºxyxyçš„æ ¼å¼ã€‚

è¿™ä¸¤ä¸ªå‡½æ•°çš„ä»£ç å¾ˆé‡è¦ï¼Œä¸€å®šè¦æŒæ¡ã€‚

ä»£ç è¿˜æ˜¯é‚£å¥è¯ï¼šxçš„æ­£åæ ‡æ˜¯å‘å³ï¼Œyçš„æ­£åæ ‡æ˜¯å‘ä¸‹ã€‚

### 22.1 xyxy2xywh

è¿™ä¸ªå‡½æ•°æ˜¯å°†é¢„æµ‹ä¿¡æ¯xyxyæ ¼å¼è½¬åŒ–ä¸ºxywhçš„æ ¼å¼ã€‚

xyxy2xywhå‡½æ•°ä»£ç ï¼š


```python
def xyxy2xywh(x):
    """"ç”¨åœ¨detect.pyå’Œtest.pyä¸­   æ“ä½œæœ€å, å°†é¢„æµ‹ä¿¡æ¯ä»xyxyæ ¼å¼è½¬ä¸ºxywhæ ¼å¼ å†ä¿å­˜
    Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where x1y1=top-left, x2y2=bottom-right
    :params x: [n, x1y1x2y2] (x1, y1): å·¦ä¸Šè§’   (x2, y2): å³ä¸‹è§’
    :return y: [n, xywh] (x, y): ä¸­å¿ƒç‚¹  wh: å®½é«˜
    """
    y = x.clone() if isinstance(x, flow.Tensor) else np.copy(x)
    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center
    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center
    y[:, 2] = x[:, 2] - x[:, 0]  # width
    y[:, 3] = x[:, 3] - x[:, 1]  # height
    return y
```

åœ¨detect.pyä¸­ä½¿ç”¨ï¼š
```
# Write results
for *xyxy, conf, cls in reversed(det):
    if save_txt:  # Write to file
        xywh = (xyxy2xywh(flow.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
```

### 22.2 xywh2xyxy

è¿™ä¸ªå‡½æ•°æ˜¯å°†é¢„æµ‹ä¿¡æ¯xywhæ ¼å¼è½¬åŒ–ä¸ºxyxyçš„æ ¼å¼ã€‚

xywh2xyxyå‡½æ•°ä»£ç ï¼š


```python
def xywh2xyxy(x):
    """ç”¨åœ¨val.pyä¸­ æ“ä½œä¹‹å‰ è½¬ä¸ºxyxyæ‰å¯ä»¥è¿›è¡Œæ“ä½œ
    æ³¨æ„: xçš„æ­£æ–¹å‘ä¸ºå³é¢   yçš„æ­£æ–¹å‘ä¸ºä¸‹é¢
    Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where x1y1=top-left, x2y2=bottom-right
    :params x: [n, xywh] (x, y):
    :return y: [n, x1y1x2y2] (x1, y1): å·¦ä¸Šè§’  (x2, y2): å³ä¸‹è§’
    """
    y = x.clone() if isinstance(x, flow.Tensor) else np.copy(x)
    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x
    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y
    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x
    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y
    return y
```

## 23. xywhn2xyxy & xyxy2xywhn & xyn2xy
è¿™ä¸‰ä¸ªå‡½æ•°ä¸»è¦ç”¨äºdataloaders.pyæ–‡ä»¶ä¸­ã€‚ä¸»è¦æ˜¯å¯¹å›¾åƒè¿›è¡Œä¸€äº›å˜æ¢æ“ä½œã€‚

xywhn2xyxyæ˜¯å°†xywh(normalized) -> x1y1x2y2ã€‚xyxy2xywhnæ˜¯å°†x1y1x2y2 -> xywh(normalized)ã€‚

xyn2xyæ˜¯å°†xy(normalized) -> xyã€‚è¿™ä¸‰ä¸ªå‡½æ•°ä¹Ÿæ˜¯æ¯”è¾ƒé‡è¦çš„ï¼Œå¤§å®¶å¿…é¡»æŒæ¡ã€‚

![image.png](general_imgs/imgs_02.png)

### 23.1 xywhn2xyxy

è¿™ä¸ªå‡½æ•°æ˜¯xywh(normalized) -> x1y1x2y2ã€‚

xywhn2xyxyå‡½æ•°ä»£ç :


```python
def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):
    """ç”¨åœ¨dataloaders.pyçš„ LoadImagesAndLabelsç±»çš„__getitem__å‡½æ•°ã€load_mosaicã€load_mosaic9ç­‰å‡½æ•°ä¸­  
    å°†xywh(normalized) -> x1y1x2y2   (x, y): ä¸­é—´ç‚¹  wh: å®½é«˜   (x1, y1): å·¦ä¸Šç‚¹  (x2, y2): å³ä¸‹ç‚¹
    Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right
    """
    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)
    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x
    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y
    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x
    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y
    return y
```

### 23.2 xyxy2xywhn
è¿™ä¸ªå‡½æ•°æ˜¯å°†x1y1x2y2 -> xywh(normalized)ã€‚

xyxy2xywhnå‡½æ•°ä»£ç ï¼š


```python
def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):
    """ç”¨åœ¨dataloaders.pyçš„ LoadImagesAndLabelsç±»çš„__getitem__å‡½æ•°ä¸­
    å°† x1y1x2y2 -> xywh(normalized)  (x1, y1): å·¦ä¸Šç‚¹  (x2, y2): å³ä¸‹ç‚¹  (x, y): ä¸­é—´ç‚¹  wh: å®½é«˜
    Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right
    """
    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right
    if clip:
        clip_coords(x, (h - eps, w - eps))  # warning: inplace clip
    y = x.clone() if isinstance(x, flow.Tensor) else np.copy(x)
    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center
    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center
    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width
    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height
    return y
```

### 23.3 xyn2xy
è¿™ä¸ªå‡½æ•°æ˜¯å°†xy(normalized) -> xyã€‚

xyn2xyå‡½æ•°ä»£ç ï¼š


```python
def xyn2xy(x, w=640, h=640, padw=0, padh=0):
    """ç”¨åœ¨dataloaders.pyçš„load_mosaicå’Œload_mosaic9å‡½æ•°ä¸­
    xy(normalized) -> xy
    Convert normalized segments into pixel segments, shape (n,2)
    """
    # Convert normalized segments into pixel segments, shape (n,2)
    y = x.clone() if isinstance(x, flow.Tensor) else np.copy(x)
    y[:, 0] = w * x[:, 0] + padw  # top left x
    y[:, 1] = h * x[:, 1] + padh  # top left y
    return 
```

### 24ã€non_max_suppression
NMS(éæå¤§å€¼æŠ‘åˆ¶)ï¼Œè¿™ä¸ªå‡½æ•°ç›¸ä¿¡å¤§å®¶éƒ½å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œè¿™æ˜¯ç›®æ ‡æ£€æµ‹æœ€åŸºæœ¬çš„æ“ä½œä¹‹ä¸€äº†ã€‚

å¯ä»¥è¯´è¿™ä¸ªå‡½æ•°æ˜¯è¿™ç¯‡åšå®¢å½“ä¸­æœ€é‡è¦çš„ä»£ç ä¹Ÿä¸ä¸ºè¿‡ï¼Œæ‰€ä»¥å¤§å®¶ä¸€å®šè¦æŒæ¡è¿™ä¸ªå‡½æ•°ï¼ˆæµç¨‹åŸç†+ä»£ç ï¼‰ã€‚

æ›´å¤šå…³äºnmsè¯·å‚é˜…ï¼š[ã€Šnmsã€‹](https://blog.csdn.net/qq_38253797/article/details/117920079)

non_max_suppressionå‡½æ•°ä»£ç ï¼š


```python
def non_max_suppression(
    prediction, #  [batch, num_anchors(3ä¸ªyoloé¢„æµ‹å±‚), (x+y+w+h+1+num_classes)] = [1, 18900, 25]  
    # 3ä¸ªanchorçš„é¢„æµ‹ç»“æœæ€»å’Œ
    conf_thres=0.25, # å…ˆè¿›è¡Œä¸€è½®ç­›é€‰ï¼Œå°†åˆ†æ•°è¿‡ä½çš„é¢„æµ‹æ¡†ï¼ˆ<conf_thresï¼‰åˆ é™¤ï¼ˆåˆ†æ•°ç½®0ï¼‰
    iou_thres=0.45, # ioué˜ˆå€¼, å¦‚æœå…¶ä½™é¢„æµ‹æ¡†ä¸targetçš„iou>iou_thres, å°±å°†é‚£ä¸ªé¢„æµ‹æ¡†ç½®0
    classes=None, # æ˜¯å¦nmsååªä¿ç•™ç‰¹å®šçš„ç±»åˆ« é»˜è®¤ä¸ºNone
    agnostic=False,# è¿›è¡Œnmsæ˜¯å¦ä¹Ÿå»é™¤ä¸åŒç±»åˆ«ä¹‹é—´çš„æ¡† é»˜è®¤False
    multi_label=False, # æ˜¯å¦æ˜¯å¤šæ ‡ç­¾  nc>1  ä¸€èˆ¬æ˜¯True
    labels=(), # æ ‡ç­¾
    max_det=300, # æ¯å¼ å›¾ç‰‡çš„æœ€å¤§ç›®æ ‡ä¸ªæ•° é»˜è®¤1000
):
    """Non-Maximum Suppression (NMS) on inference results to reject overlapping bounding boxes
    Returns:
         list of detections, on (n,6) tensor per image [xyxy, conf, cls]
    """
    if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)
        prediction = prediction[0]  # select only inference output
    # Settings  è®¾ç½®ä¸€äº›å˜é‡
    bs = prediction.shape[0]  # batch size
    nc = prediction.shape[2] - 5  # number of classes
    xc = prediction[..., 4] > conf_thres  # candidates

    # Checks
    assert 0 <= conf_thres <= 1, f"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0"
    assert 0 <= iou_thres <= 1, f"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0"

    # Settings
    # min_wh = 2  # (pixels) minimum box width and height
    # (pixels) é¢„æµ‹ç‰©ä½“å®½åº¦å’Œé«˜åº¦çš„å¤§å°èŒƒå›´ [min_wh, max_wh]
    max_wh = 7680  # (pixels) maximum box width and height
    # æ¯ä¸ªå›¾åƒæœ€å¤šæ£€æµ‹ç‰©ä½“çš„ä¸ªæ•°  maximum number of boxes into torchvision.ops.nms()
    max_nms = 30000  # maximum number of boxes into flow.nms()
    # nmsæ‰§è¡Œæ—¶é—´é˜ˆå€¼ è¶…è¿‡è¿™ä¸ªæ—¶é—´å°±é€€å‡ºäº† seconds to quit after
    time_limit = 0.3 + 0.03 * bs  # seconds to quit after
    # æ˜¯å¦éœ€è¦å†—ä½™çš„detections require redundant detections
    redundant = True  # require redundant detections
    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)
    merge = False  # use merge-NMS
    
    t = time.time() # è®°å½•å½“å‰æ—¶åˆ»æ—¶é—´
    output = [flow.zeros((0, 6), device=prediction.device)] * bs
    for xi, x in enumerate(prediction):  # image index, image inference
        # Apply constraints
        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height
        x = x[xc[xi]]  # confidence
        # {list: bs} ç¬¬ä¸€å¼ å›¾ç‰‡çš„target[17, 5] ç¬¬äºŒå¼ [1, 5] ç¬¬ä¸‰å¼ [7, 5] ç¬¬å››å¼ [6, 5]
        # Cat apriori labels if autolabelling è‡ªåŠ¨æ ‡æ³¨labelæ—¶è°ƒç”¨  ä¸€èˆ¬ä¸ç”¨
        # è‡ªåŠ¨æ ‡è®°åœ¨éå¸¸é«˜çš„ç½®ä¿¡é˜ˆå€¼ï¼ˆå³ 0.90 ç½®ä¿¡åº¦ï¼‰ä¸‹æ•ˆæœæœ€ä½³,è€Œ mAP è®¡ç®—ä¾èµ–äºéå¸¸ä½çš„ç½®ä¿¡é˜ˆå€¼ï¼ˆå³ 0.001ï¼‰æ¥æ­£ç¡®è¯„ä¼° PR æ›²çº¿ä¸‹çš„åŒºåŸŸã€‚
        # è¿™ä¸ªè‡ªåŠ¨æ ‡æ³¨æˆ‘è§‰å¾—åº”è¯¥æ˜¯ä¸€ä¸ªç±»ä¼¼RNNé‡Œé¢çš„Teacher Forcingçš„è®­ç»ƒæœºåˆ¶ å°±æ˜¯åœ¨è®­ç»ƒçš„æ—¶å€™è·Ÿç€è€å¸ˆ(ground truth)èµ°
        # ä½†æ˜¯è¿™æ ·åˆä¼šé€ æˆä¸€ä¸ªé—®é¢˜: ä¸€ç›´é è€å¸ˆå¸¦çš„å­©å­æ˜¯èµ°ä¸è¿œçš„ è¿™æ ·çš„æ¨¡å‹å› ä¸ºä¾èµ–æ ‡ç­¾æ•°æ®,åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­,æ¨¡å‹ä¼šæœ‰è¾ƒå¥½çš„æ•ˆæœ
        # ä½†æ˜¯åœ¨æµ‹è¯•çš„æ—¶å€™å› ä¸ºä¸èƒ½å¾—åˆ°ground truthçš„æ”¯æŒ, æ‰€ä»¥å¦‚æœç›®å‰ç”Ÿæˆçš„åºåˆ—åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰å¾ˆå¤§ä¸åŒ, æ¨¡å‹å°±ä¼šå˜å¾—è„†å¼±ã€‚
        # æ‰€ä»¥ä¸ªäººè®¤ä¸º(ä¸ªäººè§‚ç‚¹): åº”è¯¥åœ¨ä¸‹é¢ä½¿ç”¨çš„æ—¶å€™æœ‰é€‰æ‹©çš„å¼€å¯è¿™ä¸ªtrick æ¯”å¦‚è®¾ç½®ä¸€ä¸ªæ¦‚ç‡péšæœºå¼€å¯ æˆ–è€…åœ¨è®­ç»ƒçš„å‰nä¸ªepochä½¿ç”¨ åé¢å†å…³é—­

        # Cat apriori labels if autolabelling
        if labels and len(labels[xi]):
            lb = labels[xi]
            v = flow.zeros((len(lb), nc + 5), device=x.device)
            v[:, :4] = lb[:, 1:5]  # box
            v[:, 4] = 1.0  # conf
            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls
            x = flow.cat((x, v), 0)

        # If none remain process next image
        # ç»è¿‡å‰ä¸¤å±‚è¿‡æ»¤åå¦‚æœè¯¥feature mapæ²¡æœ‰ç›®æ ‡æ¡†äº†ï¼Œå°±ç»“æŸè¿™è½®ç›´æ¥è¿›è¡Œä¸‹ä¸€å¼ å›¾
        if not x.shape[0]:
            continue

        # Compute conf è®¡ç®—conf_score
        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf

        # Box (center x, center y, width, height) to (x1, y1, x2, y2) å·¦ä¸Šè§’ å³ä¸‹è§’   [59, 4]
        box = xywh2xyxy(x[:, :4])

        # Detections matrix nx6 (xyxy, conf, cls)
        if multi_label:
            # ç¬¬ä¸‰è½®è¿‡æ»¤:é’ˆå¯¹æ¯ä¸ªç±»åˆ«score(obj_conf * cls_conf) > conf_thres    [59, 6] -> [51, 6]
            # è¿™é‡Œä¸€ä¸ªæ¡†æ˜¯æœ‰å¯èƒ½æœ‰å¤šä¸ªç‰©ä½“çš„ï¼Œæ‰€ä»¥è¦ç­›é€‰
            # nonzero: è·å¾—çŸ©é˜µä¸­çš„é0(True)æ•°æ®çš„ä¸‹æ ‡  a.t(): å°†açŸ©é˜µæ‹†å¼€
            # i: ä¸‹æ ‡ [43]   j: ç±»åˆ«index [43] è¿‡æ»¤äº†ä¸¤ä¸ªscoreå¤ªä½çš„
            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T
            x = flow.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)
        else:  # best class only
            conf, j = x[:, 5:].max(1, keepdim=True) # ä¸€ä¸ªç±»åˆ«ç›´æ¥å–åˆ†æ•°æœ€å¤§ç±»çš„å³å¯
            x = flow.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres] 

        # Filter by class æ˜¯å¦åªä¿ç•™ç‰¹å®šçš„ç±»åˆ«  é»˜è®¤None  ä¸æ‰§è¡Œè¿™é‡Œ
        if classes is not None:
            x = x[(x[:, 5:6] == flow.tensor(classes, device=x.device)).any(1)]

        # Apply finite constraint
        # if not flow.isfinite(x).all():
        #     x = x[flow.isfinite(x).all(1)]

        # Check shape
        n = x.shape[0]  # number of boxes
        if not n:  # no boxes  å¦‚æœç»è¿‡ç¬¬ä¸‰è½®è¿‡æ»¤è¯¥feature mapæ²¡æœ‰ç›®æ ‡æ¡†äº†ï¼Œå°±ç»“æŸè¿™è½®ç›´æ¥è¿›è¡Œä¸‹ä¸€å¼ å›¾
            continue
        elif n > max_nms:  # excess boxes å¦‚æœç»è¿‡ç¬¬ä¸‰è½®è¿‡æ»¤è¯¥feature mapè¿˜è¦å¾ˆå¤šæ¡†(>max_nms)   å°±éœ€è¦æ’åº
            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence

        # Batched NMS
        # ç¬¬4è½®è¿‡æ»¤ Batched NMS   [51, 6] -> [5, 6]
        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes
        # åšä¸ªåˆ‡ç‰‡ å¾—åˆ°boxeså’Œscores   ä¸åŒç±»åˆ«çš„boxä½ç½®ä¿¡æ¯åŠ ä¸Šä¸€ä¸ªå¾ˆå¤§çš„æ•°ä½†åˆä¸åŒçš„æ•°c
        # è¿™æ ·ä½œéæå¤§æŠ‘åˆ¶çš„æ—¶å€™ä¸åŒç±»åˆ«çš„æ¡†å°±ä¸ä¼šæºå’Œåˆ°ä¸€å—äº†  è¿™æ˜¯ä¸€ä¸ªä½œnmsæŒºå·§å¦™çš„æŠ€å·§
        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores
        # è¿”å›nmsè¿‡æ»¤åçš„bounding box(boxes)çš„ç´¢å¼•ï¼ˆé™åºæ’åˆ—ï¼‰
        # i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS
        i = flow.nms(boxes, scores, iou_thres)  # NMS
        if i.shape[0] > max_det:  # limit detections
            i = i[:max_det]
        if merge and (1 < n < 3e3):  # Merge NMS (boxes merged using weighted mean)
            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)
            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix
            weights = iou * scores[None]  # box weights
            # bounding boxåˆå¹¶  å…¶å®å°±æ˜¯æŠŠæƒé‡å’Œæ¡†ç›¸ä¹˜å†é™¤ä»¥æƒé‡ä¹‹å’Œ
            x[i, :4] = flow.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes
            if redundant:
                i = i[iou.sum(1) > 1]  # require redundancy

        output[xi] = x[i] # æœ€ç»ˆè¾“å‡º  [5, 6]
        # çœ‹ä¸‹æ—¶é—´è¶…æ²¡è¶…æ—¶  è¶…æ—¶æ²¡åšå®Œçš„å°±ä¸åšäº†
        if (time.time() - t) > time_limit:
            LOGGER.warning(f"WARNING: NMS time limit {time_limit:.3f}s exceeded")
            break  # time limit exceeded

    return output
```

è¿™ä¸ªå‡½æ•°ä¸€èˆ¬ä¼šç”¨å†detect.pyæˆ–è€…val.pyçš„æ¨¡å‹å‰å‘æ¨ç†ç»“æŸä¹‹åã€‚

æ›´å¤šå…³äºNMSå‡½æ•°æµç¨‹å’Œä»£ç ï¼š[ã€YOLO-V3-SPP æºç è§£è¯»ã€‘ä¸‰ã€é¢„æµ‹æ¨¡å—.](https://blog.csdn.net/qq_38253797/article/details/117920079)


## 25 strip_optimizer

è¿™ä¸ªå‡½æ•°æ˜¯åœ¨æ¨¡å‹è®­ç»ƒå®Œå, strip_optimizerå‡½æ•°å°†optimizerã€training_resultsã€updatesâ€¦

ä»ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ckptä¸­åˆ é™¤ã€‚

strip_optimizerå‡½æ•°ä»£ç ï¼š


```python
def strip_optimizer(f="best", s=""):  # from utils.general import *; strip_optimizer()
    """ç”¨åœ¨train.pyæ¨¡å‹è®­ç»ƒå®Œå 
    å°†optimizerã€training_resultsã€updates...ä»ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶fä¸­åˆ é™¤
    Strip optimizer from 'f' to finalize training, optionally save as 's'
    :params f: ä¼ å…¥çš„åŸå§‹ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
    :params s: åˆ é™¤optimizerç­‰å˜é‡åçš„æ¨¡å‹ä¿å­˜çš„åœ°å€ dir
    """
    # Strip optimizer from 'f' to finalize training, optionally save as 's'
    x = flow.load(f, map_location=flow.device("cpu")) # x: ä¸ºåŠ è½½è®­ç»ƒçš„æ¨¡å‹
    if x.get("ema"):     # å¦‚æœæ¨¡å‹æ˜¯ema replace model with ema
        x["model"] = x["ema"]  # replace model with ema
    # ä»¥ä¸‹æ¨¡å‹è®­ç»ƒæ¶‰åŠåˆ°çš„è‹¥å¹²ä¸ªæŒ‡å®šå˜é‡ç½®ç©º
    for k in "optimizer", "best_fitness", "wandb_id", "ema", "updates":  # keys
        x[k] = None
    x["epoch"] = -1 # æ¨¡å‹epochæ¢å¤åˆå§‹å€¼ -1
    x["model"].half()  # to FP16
    for p in x["model"].parameters():
        p.requires_grad = False
    # ä¿å­˜æ¨¡å‹ x -> s/f
    flow.save(x, s or f)
    mb = os.path.getsize(s or f) / 1e6  # filesize
    LOGGER.info(f"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB")
```

![image.png](general_imgs/picture_15.png)

## 26 print_mutation

è¿™ä¸ªå‡½æ•°ç”¨æ¥æ‰“å°è¿›åŒ–åçš„è¶…å‚ç»“æœå’Œresultsåˆ°evolve.txtå’Œhyp_evolved.yamlä¸­ã€‚

print_mutationå‡½æ•°ä»£ç ï¼š



```python
def print_mutation(results, hyp, save_dir, bucket, prefix=colorstr("evolve: ")):
    """ç”¨åœ¨train.pyçš„è¿›åŒ–è¶…å‚ç»“æŸå  
    æ‰“å°è¿›åŒ–åçš„è¶…å‚ç»“æœå’Œresultsåˆ°evolve.txtå’Œhyp_evolved.yamlä¸­
    Print mutation results to evolve.txt (for use with train.py --evolve)
    :params hyp: è¿›åŒ–åçš„è¶…å‚ dict {28å¯¹ key:value}
    :params results: tuple(7)   (mp, mr, map50, map50:95, box_loss, obj_loss, cls_loss)
    :params yaml_file: è¦ä¿å­˜çš„è¿›åŒ–åçš„è¶…å‚æ–‡ä»¶å  runs\train\evolve\hyp_evolved.yaml
    :params bucket: ''
    """
    evolve_csv = save_dir / "evolve.csv"
    evolve_yaml = save_dir / "hyp_evolve.yaml"
    keys = ("metrics/precision", "metrics/recall", "metrics/mAP_0.5", "metrics/mAP_0.5:0.95", "val/box_loss", "val/obj_loss", "val/cls_loss",) + tuple(
        hyp.keys()
    )  # [results + hyps]
    keys = tuple(x.strip() for x in keys)
    vals = results + tuple(hyp.values())
    n = len(keys)

    # Download (optional)
    if bucket:
        url = f"gs://{bucket}/evolve.csv"
        if gsutil_getsize(url) > (evolve_csv.stat().st_size if evolve_csv.exists() else 0):
            os.system(f"gsutil cp {url} {save_dir}")  # download evolve.csv if larger than local

    # Log to evolve.csv
    s = "" if evolve_csv.exists() else (("%20s," * n % keys).rstrip(",") + "\n")  # add header
    with open(evolve_csv, "a") as f:
        f.write(s + ("%20.5g," * n % vals).rstrip(",") + "\n")

    # Save yaml
    with open(evolve_yaml, "w") as f:
        data = pd.read_csv(evolve_csv)
        data = data.rename(columns=lambda x: x.strip())  # strip keys
        i = np.argmax(fitness(data.values[:, :4]))  #
        generations = len(data)
        f.write(
            "# YOLOv5 Hyperparameter Evolution Results\n"
            + f"# Best generation: {i}\n"
            + f"# Last generation: {generations - 1}\n"
            + "# "
            + ", ".join(f"{x.strip():>20s}" for x in keys[:7])
            + "\n"
            + "# "
            + ", ".join(f"{x:>20.5g}" for x in data.values[i, :7])
            + "\n\n"
        )
        yaml.safe_dump(data.loc[i][7:].to_dict(), f, sort_keys=False)

    # Print to screen
    LOGGER.info(
        prefix + f"{generations} generations finished, current result:\n" + prefix + ", ".join(f"{x.strip():>20s}" for x in keys) + "\n" + prefix + ", ".join(f"{x:20.5g}" for x in vals) + "\n\n"
    )

    if bucket:
        os.system(f"gsutil cp {evolve_csv} {evolve_yaml} gs://{bucket}")  # upload
```

## 27. apply_classifier
è¿™ä¸ªå‡½æ•°å®šä¹‰äº†ä¸€ä¸ªäºŒçº§åˆ†ç±»å™¨æ¥å¤„ç†yoloçš„è¾“å‡ºï¼Œå¯ä»¥å°†å®ƒç”¨åœ¨detect.pyä¸­ã€‚

è¿™é‡Œå†™çš„è¿™ä¸ªå‡½æ•°åªæ˜¯ä¸€ä¸ªæ™®é€šçš„å®ç°ï¼Œä½ ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±çš„ä»»åŠ¡æ”¹å†™è¿™ä¸ªå‡½æ•°ã€‚

ä¸è¿‡è¿™ä¸ªå‡½æ•°æˆ‘ä»¬å‡ ä¹ä¸ä¼šç”¨å®ƒï¼Œå› ä¸ºå®ƒå¾ˆå®¹æ˜“å‡ºé”™ã€‚æˆ‘ä»¬è¿™é‡Œå°±ä¸ä»”ç»†ä»‹ç»äº†ï¼ŒçœŸçš„å¾ˆéš¾ç”¨åˆ°è¿™ä¸ªå‡½æ•°ï¼Œéšä¾¿çœ‹ä¸‹å°±å¥½ã€‚

å‡½æ•°ä»£ç ï¼š


```python
def apply_classifier(x, model, img, im0):
    """ç”¨åœ¨detect.pyæ–‡ä»¶çš„nmsåç»§ç»­å¯¹feature mapé€å…¥model2 è¿›è¡ŒäºŒæ¬¡åˆ†ç±»
    å®šä¹‰äº†ä¸€ä¸ªäºŒçº§åˆ†ç±»å™¨æ¥å¤„ç†yoloçš„è¾“å‡º  å½“å‰å®ç°æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‚è€ƒèµ·ç‚¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒè‡ªè¡Œå®ç°æ­¤é¡¹
    æ¯”å¦‚ä½ æœ‰ç…§ç‰‡ä¸æ±½è½¦ä¸è½¦ç‰Œ, ä½ ç¬¬ä¸€æ¬¡å‰ªåˆ‡è½¦ç‰Œ, å¹¶å°†å…¶å‘é€åˆ°ç¬¬äºŒé˜¶æ®µåˆ†ç±»å™¨, ä»¥æ£€æµ‹å…¶ä¸­çš„å­—ç¬¦
    Apply a second stage classifier to yolo outputs
    https://github.com/ultralytics/yolov5/issues/2700  è¿™ä¸ªå‡½æ•°ä½¿ç”¨èµ·æ¥å¾ˆå®¹æ˜“å‡ºé”™ ä¸æ˜¯å¾ˆæ¨èä½¿ç”¨
    https://github.com/ultralytics/yolov5/issues/1472
    :params x: yoloå±‚çš„è¾“å‡º
    :params model: åˆ†ç±»æ¨¡å‹
    :params img: è¿›è¡Œresize + padä¹‹åçš„å›¾ç‰‡
    :params im0: åŸå°ºå¯¸çš„å›¾ç‰‡
    """
    im0 = [im0] if isinstance(im0, np.ndarray) else im0
    for i, d in enumerate(x):  # per image
        if d is not None and len(d):
            d = d.clone()

            # Reshape and pad cutouts
            b = xyxy2xywh(d[:, :4])  # boxes xyxy -> xywh
            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square
            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad
            d[:, :4] = xywh2xyxy(b).long()  # xywh -> xyxy

            # Rescale boxes from img_size to im0 size
            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)

            # Classes
            pred_cls1 = d[:, 5].long()  # åœ¨ä¹‹å‰çš„yoloæ¨¡å‹é¢„æµ‹çš„ç±»åˆ«
            ims = []
            for j, a in enumerate(d):  # per item
                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]
                im = cv2.resize(cutout, (224, 224))  # BGR
                # cv2.imwrite('test%i.jpg' % j, cutout)

                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416
                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32
                im /= 255.0  # 0 - 255 to 0.0 - 1.0
                ims.append(im)

            # ç”¨modelæ¨¡å‹è¿›è¡Œåˆ†ç±»é¢„æµ‹
            pred_cls2 = model(flow.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction
            # ä¿ç•™é¢„æµ‹ä¸€è‡´çš„ç»“æœ
            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections

    return x
```

## 28. increment_path
ç”¨äºé€’å¢è·¯å¾„ã€‚

æ¯”å¦‚æˆ‘è¾“å…¥è·¯å¾„æ˜¯run/train/expï¼Œä½†æ˜¯å‘ç°æ–‡ä»¶å¤¹é‡Œé¢å·²ç»æœ‰è¿™ä¸ªæ–‡ä»¶äº†ï¼Œ

é‚£ä¹ˆå°±å°†æ–‡ä»¶è·¯å¾„æ‰©å±•å›´ä¸ºï¼šruns/train/exp{sep}0, runs/exp{sep}1 etcã€‚

increment_pathå‡½æ•°ä»£ç ï¼š


```python
def increment_path(path, exist_ok=False, sep="", mkdir=False):
    """è¿™æ˜¯ä¸ªç”¨å¤„ç‰¹åˆ«å¹¿æ³›çš„å‡½æ•° train.pyã€detect.pyã€test.pyç­‰éƒ½ä¼šç”¨åˆ°
    é€’å¢è·¯å¾„ å¦‚ run/train/exp --> runs/train/exp{sep}0, runs/exp{sep}1 etc.
    :params path: window path   run/train/exp
    :params exist_ok: False
    :params sep: expæ–‡ä»¶åçš„åç¼€  é»˜è®¤''
    :params mkdir: æ˜¯å¦åœ¨è¿™é‡Œåˆ›å»ºdir  False
    """
    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.
    path = Path(path)  # os-agnostic
    # å¦‚æœè¯¥æ–‡ä»¶å¤¹å·²ç»å­˜åœ¨ åˆ™å°†è·¯å¾„run/train/expä¿®æ”¹ä¸º runs/train/exp1
    if path.exists() and not exist_ok:
        # path.suffix å¾—åˆ°è·¯å¾„pathçš„åç¼€  ''
        path, suffix = (path.with_suffix(""), path.suffix) if path.is_file() else (path, "")
        
        # Method 1
        for n in range(2, 9999):
            p = f"{path}{sep}{n}{suffix}"  # increment path
            if not os.path.exists(p):  #
                break
        path = Path(p)

        # Method 2 (deprecated)
        # dirs = glob.glob(f"{path}{sep}*")  # similar paths
        # matches = [re.search(rf"{path.stem}{sep}(\d+)", d) for d in dirs]
        # i = [int(m.groups()[0]) for m in matches if m]  # indices
        # n = max(i) + 1 if i else 2  # increment number
        # path = Path(f"{path}{sep}{n}{suffix}")  # increment path

    if mkdir:
        path.mkdir(parents=True, exist_ok=True)  # make directory

    return path
```

## 29. resample_segments
è¿™ä¸ªå‡½æ•°æ˜¯ å¯¹segmenté‡æ–°é‡‡æ ·ï¼Œæ¯”å¦‚è¯´segmentåæ ‡åªæœ‰100ä¸ªï¼Œé€šè¿‡interpå‡½æ•°å°†å…¶é‡‡æ ·ä¸ºnä¸ª(é»˜è®¤1000)ã€‚

resample_segmentså‡½æ•°ä»£ç ï¼š



```python
def resample_segments(segments, n=1000):
    """ç”¨åœ¨augmentations.pyæ–‡ä»¶ä¸­çš„random_perspectiveå‡½æ•°ä¸­
    å¯¹segmenté‡æ–°é‡‡æ ·ï¼Œæ¯”å¦‚è¯´segmentåæ ‡åªæœ‰100ä¸ªï¼Œé€šè¿‡interpå‡½æ•°å°†å…¶é‡‡æ ·ä¸ºnä¸ª(é»˜è®¤1000)
    :params segments: [N, x1x2...]
    :params n: é‡‡æ ·ä¸ªæ•°
    :return segments: [N, n/2, 2]
    """
    # Up-sample an (n,2) segment
    for i, s in enumerate(segments):
        s = np.concatenate((s, s[0:1, :]), axis=0)
        x = np.linspace(0, len(s) - 1, n)
        xp = np.arange(len(s))
        # å¯¹æ‰€æœ‰çš„segmentséƒ½è¿›è¡Œé‡æ–°é‡‡æ · æ¯”å¦‚è¯´segmentåæ ‡åªæœ‰100ä¸ªï¼Œé€šè¿‡interpå‡½æ•°å°†å…¶é‡‡æ ·ä¸ºnä¸ª(é»˜è®¤1000)
        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy0
    return segments
```

![image.png](general_imgs/picture_16.png)

## 30. segment2box

è¿™ä¸ªå‡½æ•°æ˜¯å°†ä¸€ä¸ªå¤šè¾¹å½¢æ ‡ç­¾(ä¸æ˜¯çŸ©å½¢æ ‡ç­¾ åˆ°åº•æ˜¯å‡ è¾¹å½¢æœªçŸ¥)è½¬åŒ–ä¸ºä¸€ä¸ªçŸ©å½¢æ ‡ç­¾ã€‚

segment2boxå‡½æ•°ä»£ç ï¼š


```python
def segment2box(segment, width=640, height=640):
    """ç”¨åœ¨augmentations.pyæ–‡ä»¶ä¸­çš„random_perspectiveå‡½æ•°ä¸­
    å°†ä¸€ä¸ªå¤šè¾¹å½¢æ ‡ç­¾(ä¸æ˜¯çŸ©å½¢æ ‡ç­¾  åˆ°åº•æ˜¯å‡ è¾¹å½¢æœªçŸ¥)è½¬åŒ–ä¸ºä¸€ä¸ªçŸ©å½¢æ ‡ç­¾
    æ–¹æ³•: å¯¹å¤šè¾¹å½¢æ‰€æœ‰çš„ç‚¹x1y1 x2y2...  è·å–å…¶ä¸­çš„(x_min,y_min)å’Œ(x_max,y_max) ä½œä¸ºçŸ©å½¢labelçš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’
    Convert 1 segment label to 1 box label, applying inside-image constraint
    :params segment: ä¸€ä¸ªå¤šè¾¹å½¢æ ‡ç­¾ [n, 2] ä¼ å…¥è¿™ä¸ªå¤šè¾¹å½¢nä¸ªé¡¶ç‚¹çš„åæ ‡
    :params width: è¿™ä¸ªå¤šè¾¹å½¢æ‰€åœ¨å›¾ç‰‡çš„å®½åº¦
    :params height: è¿™ä¸ªå¤šè¾¹å½¢æ‰€åœ¨å›¾ç‰‡çš„é«˜åº¦
    :return çŸ©å½¢æ ‡ç­¾ [1, x_min+y_min+x_max+y_max]
    """
    # åˆ†åˆ«è·å–å½“å‰å¤šè¾¹å½¢ä¸­æ‰€æœ‰å¤šè¾¹å½¢ç‚¹çš„xå’Œyåæ ‡
    x, y = segment.T  # segment xy
    # inside: ç­›é€‰æ¡ä»¶ xyåæ ‡å¿…é¡»å¤§äºç­‰äº0 xåæ ‡å¿…é¡»å°äºç­‰äºå®½åº¦ yåæ ‡å¿…é¡»å°äºç­‰äºé«˜åº¦
    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)
    # è·å–ç­›é€‰åçš„æ‰€æœ‰å¤šè¾¹å½¢ç‚¹çš„xå’Œyåæ ‡
    x, y, = x[inside], y[inside]
    # å–å½“å‰å¤šè¾¹å½¢ä¸­xyåæ ‡çš„æœ€å¤§æœ€å°å€¼ï¼Œå¾—åˆ°è¾¹æ¡†çš„åæ ‡xyxy
    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))

```


![image.png](general_imgs/picture_17.png)

## 31. segments2boxes

è¿™ä¸ªå‡½æ•°æ˜¯å°†å¤šä¸ªå¤šè¾¹å½¢æ ‡ç­¾(ä¸æ˜¯çŸ©å½¢æ ‡ç­¾ åˆ°åº•æ˜¯å‡ è¾¹å½¢æœªçŸ¥)è½¬åŒ–ä¸ºå¤šä¸ªçŸ©å½¢æ ‡ç­¾ã€‚

segments2boxesæ¨¡å—ä»£ç :


```python
def segments2boxes(segments):
    """ç”¨åœ¨dataloaders.pyæ–‡ä»¶ä¸­çš„verify_image_labelå‡½æ•°ä¸­
    å°†å¤šä¸ªå¤šè¾¹å½¢æ ‡ç­¾(ä¸æ˜¯çŸ©å½¢æ ‡ç­¾  åˆ°åº•æ˜¯å‡ è¾¹å½¢æœªçŸ¥)è½¬åŒ–ä¸ºå¤šä¸ªçŸ©å½¢æ ‡ç­¾
    Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)
    :params segments: [N, cls+x1y1+x2y2 ...]
    :return [N, cls+xywh]
    """
    boxes = []
    for s in segments:
        # åˆ†åˆ«è·å–å½“å‰å¤šè¾¹å½¢ä¸­æ‰€æœ‰å¤šè¾¹å½¢ç‚¹çš„xå’Œyåæ ‡
        x, y = s.T
        # å–å½“å‰å¤šè¾¹å½¢ä¸­xå’Œyåæ ‡çš„æœ€å¤§æœ€å°å€¼ï¼Œå¾—åˆ°è¾¹æ¡†çš„åæ ‡xyxy
        boxes.append([x.min(), y.min(), x.max(), y.max()])
    # [N, cls+xywh]
    return xyxy2xywh(np.array(boxes))

```

![image.png](general_imgs/picture_18.png)

## æ€»ç»“
è¿™ä¸ªæ–‡ä»¶çš„ä»£ç ä¸»è¦æ˜¯ä¸€äº›é€šç”¨çš„å·¥å…·å‡½æ•°ï¼Œä¼šå¹¿æ³›çš„åœ¨æ•´ä¸ªé¡¹ç›®çš„æ–‡ä»¶ä¸­ä½¿ç”¨ï¼Œæ‰€ä»¥æ¯”è¾ƒé‡è¦ï¼Œå¸Œæœ›å¤§å®¶éƒ½å¯ä»¥æŒæ¡ã€‚

æ¯”è¾ƒé‡è¦çš„å‡½æ•°æœ‰ï¼šset_loggingã€init_seedsã€get_latest_runã€colorstrã€check_git_statusã€check_requirementsã€make_divisibleã€check_fileã€check_datasetã€one_cycleã€labels_to_class_weightsã€labels_to_image_weightsã€strip_optimizerã€print_mutationã€save_one_boxã€increment_pathã€‚

éå¸¸é‡è¦çš„æœ‰ï¼šclip_coordsã€scale_coordsã€xyxy2xywhã€xywh2xyxyã€xywhn2xyxyã€xyxy2xywhnã€xyn2xyã€non_max_suppressionã€‚


## Reference

- [ã€YOLOV5-5.x æºç è§£è¯»ã€‘general.py](https://blog.csdn.net/qq_38253797/article/details/119348092)