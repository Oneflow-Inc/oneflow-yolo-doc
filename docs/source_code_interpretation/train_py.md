## å‰è¨€

>ğŸ‰ä»£ç ä»“åº“åœ°å€ï¼š<a href="https://github.com/Oneflow-Inc/one-yolov5" target="blank">https://github.com/Oneflow-Inc/one-yolov5</a>
æ¬¢è¿star [one-yolov5é¡¹ç›®](https://github.com/Oneflow-Inc/one-yolov5) è·å–<a href="https://github.com/Oneflow-Inc/one-yolov5/tags" target="blank" >æœ€æ–°çš„åŠ¨æ€ã€‚</a>
<a href="https://github.com/Oneflow-Inc/one-yolov5/issues/new"  target="blank"  >å¦‚æœæ‚¨æœ‰é—®é¢˜ï¼Œæ¬¢è¿åœ¨ä»“åº“ç»™æˆ‘ä»¬æå‡ºå®è´µçš„æ„è§ã€‚ğŸŒŸğŸŒŸğŸŒŸ</a>
<a href="https://github.com/Oneflow-Inc/one-yolov5" target="blank" >
å¦‚æœå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿æ¥ç»™æˆ‘Starå‘€ğŸ˜Š~  </a>


æºç è§£è¯»ï¼š [train.py](https://github.com/Oneflow-Inc/one-yolov5/blob/main/train.py)

> è¿™ä¸ªæ–‡ä»¶æ˜¯yolov5çš„è®­ç»ƒè„šæœ¬ã€‚æ€»ä½“ä¸Šä»£ç æµç¨‹æ¯”è¾ƒç®€å•çš„ï¼ŒæŠ“ä½ æ•°æ® + æ¨¡å‹ + å­¦ä¹ ç‡ + ä¼˜åŒ–å™¨ + è®­ç»ƒè¿™äº”æ­¥å³å¯ã€‚





## 1. å¯¼å…¥éœ€è¦çš„åŒ…å’ŒåŸºæœ¬é…ç½®



```python
import argparse       # è§£æå‘½ä»¤è¡Œå‚æ•°æ¨¡å—
import math           # æ•°å­¦å…¬å¼æ¨¡å—
import os             # ä¸æ“ä½œç³»ç»Ÿè¿›è¡Œäº¤äº’çš„æ¨¡å— åŒ…å«æ–‡ä»¶è·¯å¾„æ“ä½œå’Œè§£æ
import random         # ç”Ÿæˆéšæœºæ•°æ¨¡å—
import sys            # sysç³»ç»Ÿæ¨¡å— åŒ…å«äº†ä¸Pythonè§£é‡Šå™¨å’Œå®ƒçš„ç¯å¢ƒæœ‰å…³çš„å‡½æ•°
import time           # æ—¶é—´æ¨¡å— æ›´åº•å±‚
from copy import deepcopy # æ·±åº¦æ‹·è´æ¨¡å—
from datetime import datetime
from pathlib import Path # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—

import numpy as np       # numpyæ•°ç»„æ“ä½œæ¨¡å—
import oneflow as flow   # OneFlowæ¡†æ¶
import oneflow.distributed as dist # åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å—
import oneflow.nn as nn  # å¯¹oneflow.nn.functionalçš„ç±»çš„å°è£… æœ‰å¾ˆå¤šå’Œoneflow.nn.functionalç›¸åŒçš„å‡½æ•°
import yaml              # æ“ä½œyamlæ–‡ä»¶æ¨¡å—
from oneflow.optim import lr_scheduler  # å­¦ä¹ ç‡æ¨¡å—
from tqdm import tqdm   # è¿›åº¦æ¡æ¨¡å—

import val  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors

# from utils.autobatch import check_train_batch_size
from utils.callbacks import Callbacks
from utils.dataloaders import create_dataloader

from utils.downloads import is_url  # , attempt_download
from utils.general import check_img_size  # check_suffix,
from utils.general import (
    LOGGER,
    check_dataset,
    check_file,
    check_git_status,
    check_requirements,
    check_yaml,
    colorstr,
    get_latest_run,
    increment_path,
    init_seeds,
    intersect_dicts,
    labels_to_class_weights,
    labels_to_image_weights,
    methods,
    one_cycle,
    print_args,
    print_mutation,
    strip_optimizer,
    yaml_save,
    model_save,
)
from utils.loggers import Loggers
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.loss import ComputeLoss
from utils.metrics import fitness
from utils.oneflow_utils import EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer, smart_resume
from utils.plots import plot_evolve, plot_labels
# è¿™ä¸ª Worker æ˜¯è¿™å°æœºå™¨ä¸Šçš„ç¬¬å‡ ä¸ª Worker
LOCAL_RANK = int(os.getenv("LOCAL_RANK", -1))  # https://pytorch.org/docs/stable/elastic/run.html
# è¿™ä¸ª Worker æ˜¯å…¨å±€ç¬¬å‡ ä¸ª Worker
RANK = int(os.getenv("RANK", -1))
# æ€»å…±æœ‰å‡ ä¸ª Worker
WORLD_SIZE = int(os.getenv("WORLD_SIZE", 1))

# Linux ä¸‹ï¼š
# FILE =  'path/to/one-yolov5/train.py'
# å°†'path/to/one-yolov5'åŠ å…¥ç³»ç»Ÿçš„ç¯å¢ƒå˜é‡  è¯¥è„šæœ¬ç»“æŸåå¤±æ•ˆã€‚
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

```

## 2. [parse_opt å‡½æ•°](https://github.com/Oneflow-Inc/one-yolov5/blob/a681bd5ce5853027d366451861241bb09ef6eabd/train.py#L472-L561)


> è¿™ä¸ªå‡½æ•°ç”¨äºè®¾ç½®optå‚æ•°

```
weights: æƒé‡æ–‡ä»¶
cfg: æ¨¡å‹é…ç½®æ–‡ä»¶ åŒ…æ‹¬ncã€depth_multipleã€width_multipleã€anchorsã€backboneã€headç­‰
data: æ•°æ®é›†é…ç½®æ–‡ä»¶ åŒ…æ‹¬pathã€trainã€valã€testã€ncã€namesã€downloadç­‰
hyp: åˆå§‹è¶…å‚æ–‡ä»¶
epochs: è®­ç»ƒè½®æ¬¡
batch-size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
img-size: è¾“å…¥ç½‘ç»œçš„å›¾ç‰‡åˆ†è¾¨ç‡å¤§å°
resume: æ–­ç‚¹ç»­è®­, ä»ä¸Šæ¬¡æ‰“æ–­çš„è®­ç»ƒç»“æœå¤„æ¥ç€è®­ç»ƒ  é»˜è®¤False
nosave: ä¸ä¿å­˜æ¨¡å‹  é»˜è®¤False(ä¿å­˜)      True: only test final epoch
notest: æ˜¯å¦åªæµ‹è¯•æœ€åä¸€è½® é»˜è®¤False  True: åªæµ‹è¯•æœ€åä¸€è½®   False: æ¯è½®è®­ç»ƒå®Œéƒ½æµ‹è¯•mAP
workers: dataloaderä¸­çš„æœ€å¤§workæ•°ï¼ˆçº¿ç¨‹ä¸ªæ•°ï¼‰
device: è®­ç»ƒçš„è®¾å¤‡
single-cls: æ•°æ®é›†æ˜¯å¦åªæœ‰ä¸€ä¸ªç±»åˆ« é»˜è®¤False

rect: è®­ç»ƒé›†æ˜¯å¦é‡‡ç”¨çŸ©å½¢è®­ç»ƒ  é»˜è®¤False
noautoanchor: ä¸è‡ªåŠ¨è°ƒæ•´anchor é»˜è®¤False(è‡ªåŠ¨è°ƒæ•´anchor)
evolve: æ˜¯å¦è¿›è¡Œè¶…å‚è¿›åŒ– é»˜è®¤False
multi-scale: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦è®­ç»ƒ é»˜è®¤False
label-smoothing: æ ‡ç­¾å¹³æ»‘å¢å¼º é»˜è®¤0.0ä¸å¢å¼º  è¦å¢å¼ºä¸€èˆ¬å°±è®¾ä¸º0.1
adam: æ˜¯å¦ä½¿ç”¨adamä¼˜åŒ–å™¨ é»˜è®¤False(ä½¿ç”¨SGD)
sync-bn: æ˜¯å¦ä½¿ç”¨è·¨å¡åŒæ­¥bnæ“ä½œ,å†DDPä¸­ä½¿ç”¨  é»˜è®¤False
linear-lr: æ˜¯å¦ä½¿ç”¨linear lr  çº¿æ€§å­¦ä¹ ç‡  é»˜è®¤False ä½¿ç”¨cosine lr
cache-image: æ˜¯å¦æå‰ç¼“å­˜å›¾ç‰‡åˆ°å†…å­˜cache,ä»¥åŠ é€Ÿè®­ç»ƒ  é»˜è®¤False
image-weights: æ˜¯å¦ä½¿ç”¨å›¾ç‰‡é‡‡ç”¨ç­–ç•¥(selection img to training by class weights) é»˜è®¤False ä¸ä½¿ç”¨
bbox_iou_optim: ä¼˜åŒ– compute_loss ä¸­çš„ bbox_iou å‡½æ•°

bucket: è°·æ­Œäº‘ç›˜bucket ä¸€èˆ¬ç”¨ä¸åˆ°
project: è®­ç»ƒç»“æœä¿å­˜çš„æ ¹ç›®å½• é»˜è®¤æ˜¯runs/train
name: è®­ç»ƒç»“æœä¿å­˜çš„ç›®å½• é»˜è®¤æ˜¯exp  æœ€ç»ˆ: runs/train/exp
exist-ok: å¦‚æœæ–‡ä»¶å­˜åœ¨å°±okä¸å­˜åœ¨å°±æ–°å»ºæˆ–increment name  é»˜è®¤False(é»˜è®¤æ–‡ä»¶éƒ½æ˜¯ä¸å­˜åœ¨çš„)
quad: dataloaderå–æ•°æ®æ—¶, æ˜¯å¦ä½¿ç”¨collate_fn4ä»£æ›¿collate_fn  é»˜è®¤False
save_period: Log model after every "save_period" epoch    é»˜è®¤-1 ä¸éœ€è¦log model ä¿¡æ¯
artifact_alias: which version of dataset artifact to be stripped  é»˜è®¤lastest  è²Œä¼¼æ²¡ç”¨åˆ°è¿™ä¸ªå‚æ•°ï¼Ÿ
local_rank: rankä¸ºè¿›ç¨‹ç¼–å·  -1ä¸”gpu=1æ—¶ä¸è¿›è¡Œåˆ†å¸ƒå¼  -1ä¸”å¤šå—gpuä½¿ç”¨DataParallelæ¨¡å¼

entity: wandb entity é»˜è®¤None
upload_dataset: æ˜¯å¦ä¸Šä¼ datasetåˆ°wandb tabel(å°†æ•°æ®é›†ä½œä¸ºäº¤äº’å¼ dsvizè¡¨ åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ã€æŸ¥è¯¢ã€ç­›é€‰å’Œåˆ†ææ•°æ®é›†) é»˜è®¤False
bbox_interval: è®¾ç½®å¸¦è¾¹ç•Œæ¡†å›¾åƒè®°å½•é—´éš” Set bounding-box image logging interval for W&B é»˜è®¤-1   opt.epochs // 10
```


æ›´å¤šç»†èŠ‚[è¯·ç‚¹è¿™](https://start.oneflow.org/oneflow-yolo-doc/tutorials/03_chapter/quick_start.html#_18)

## 3 mainå‡½æ•°

### 3.1 [Checks](https://github.com/Oneflow-Inc/one-yolov5/blob/a681bd5ce5853027d366451861241bb09ef6eabd/train.py#L566-L569)


```python
def main(opt, callbacks=Callbacks()):
    # Checks
    if RANK in {-1, 0}:
        # è¾“å‡ºæ‰€æœ‰è®­ç»ƒoptå‚æ•°  train: ...
        print_args(vars(opt))
        # æ£€æŸ¥ä»£ç ç‰ˆæœ¬æ˜¯å¦æ˜¯æœ€æ–°çš„  github: ...
        check_git_status()
        # æ£€æŸ¥requirements.txtæ‰€éœ€åŒ…æ˜¯å¦éƒ½æ»¡è¶³ requirements: ...
        check_requirements(exclude=["thop"])
```

## 3.2 [Resume](https://github.com/Oneflow-Inc/one-yolov5/blob/a681bd5ce5853027d366451861241bb09ef6eabd/train.py#L571-L603)
> åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ–­ç‚¹ç»­è®­resume, è¯»å–å‚æ•°

ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»`path/to/last`ä¸­è¯»å–ç›¸å…³å‚æ•°ï¼›ä¸ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»æ–‡ä»¶ä¸­è¯»å–ç›¸å…³å‚æ•°


```python
# 2ã€åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ–­ç‚¹ç»­è®­resume, è¯»å–å‚æ•°
if opt.resume and not (check_wandb_resume(opt) or opt.evolve):  # resume from specified or most recent last
    # ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»last.ptä¸­è¯»å–ç›¸å…³å‚æ•°
    # å¦‚æœresumeæ˜¯strï¼Œåˆ™è¡¨ç¤ºä¼ å…¥çš„æ˜¯æ¨¡å‹çš„è·¯å¾„åœ°å€
    # å¦‚æœresumeæ˜¯Trueï¼Œåˆ™é€šè¿‡get_lastest_run()å‡½æ•°æ‰¾åˆ°runsä¸ºæ–‡ä»¶å¤¹ä¸­æœ€è¿‘çš„æƒé‡æ–‡ä»¶last
    last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())
    opt_yaml = last.parent.parent / "opt.yaml"  # train options yaml
    opt_data = opt.data  # original dataset
    if opt_yaml.is_file():
        # ç›¸å…³çš„optå‚æ•°ä¹Ÿè¦æ›¿æ¢æˆlast.ptä¸­çš„optå‚æ•°
        with open(opt_yaml, errors="ignore") as f:
            d = yaml.safe_load(f)
    else:
        d = flow.load(last, map_location="cpu")["opt"]
    opt = argparse.Namespace(**d)  # replace
    opt.cfg, opt.weights, opt.resume = "", str(last), True  # reinstate
    if is_url(opt_data):
        opt.data = check_file(opt_data)  # avoid HUB resume auth timeout
else:
    # ä¸ä½¿ç”¨æ–­ç‚¹ç»­è®­ å°±ä»æ–‡ä»¶ä¸­è¯»å–ç›¸å…³å‚æ•°
    # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml')
    opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = (
        check_file(opt.data),
        check_yaml(opt.cfg),
        check_yaml(opt.hyp),
        str(opt.weights),
        str(opt.project),
    )  # checks
    assert len(opt.cfg) or len(opt.weights), "either --cfg or --weights must be specified"
    # å°†opt.img_sizeæ‰©å±•ä¸º[train_img_size, test_img_size]
    if opt.evolve:
        if opt.project == str(ROOT / "runs/train"):  # if default project name, rename to runs/evolve
            opt.project = str(ROOT / "runs/evolve")
        opt.exist_ok, opt.resume = (
            opt.resume,
            False,
        )  # pass resume to exist_ok and disable resume
    if opt.name == "cfg":
        opt.name = Path(opt.cfg).stem  # use model.yaml as name
    # æ ¹æ®opt.projectç”Ÿæˆç›®å½•  å¦‚: runs/train/exp18
    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))
```

## 3.3  DDP mode
> DDP modeè®¾ç½®


```python
# 3ã€DDP modeè®¾ç½®
# é€‰æ‹©è®¾å¤‡  cpu/cuda:0
device = select_device(opt.device, batch_size=opt.batch_size)
if LOCAL_RANK != -1:
    msg = "is not compatible with YOLOv5 Multi-GPU DDP training"
    assert not opt.image_weights, f"--image-weights {msg}"
    assert not opt.evolve, f"--evolve {msg}"
    assert opt.batch_size != -1, f"AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size"
    assert opt.batch_size % WORLD_SIZE == 0, f"--batch-size {opt.batch_size} must be multiple of WORLD_SIZE"
    assert flow.cuda.device_count() > LOCAL_RANK, "insufficient CUDA devices for DDP command"
    flow.cuda.set_device(LOCAL_RANK)
    device = flow.device("cuda", LOCAL_RANK)
```

### 3.4Train
> ä¸ä½¿ç”¨[è¿›åŒ–ç®—æ³•](https://so.csdn.net/so/search?q=%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95&spm=1001.2101.3001.7020) æ­£å¸¸Train


```python
# Train
if not opt.evolve:
    # å¦‚æœä¸è¿›è¡Œè¶…å‚è¿›åŒ– é‚£ä¹ˆå°±ç›´æ¥è°ƒç”¨train()å‡½æ•°ï¼Œå¼€å§‹è®­ç»ƒ
    train(opt.hyp, opt, device, callbacks)

```

### 3.5 Evolve hyperparameters (optional)
> [é—ä¼ è¿›åŒ–ç®—æ³•ï¼Œè¾¹è¿›åŒ–è¾¹è®­ç»ƒ](https://github.com/Oneflow-Inc/one-yolov5/blob/a681bd5ce5853027d366451861241bb09ef6eabd/train.py#L625-L713)



```python
# å¦åˆ™ä½¿ç”¨è¶…å‚è¿›åŒ–ç®—æ³•(é—ä¼ ç®—æ³•) æ±‚å‡ºæœ€ä½³è¶…å‚ å†è¿›è¡Œè®­ç»ƒ
else:
    # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)
    # è¶…å‚è¿›åŒ–åˆ—è¡¨ (çªå˜è§„æ¨¡, æœ€å°å€¼, æœ€å¤§å€¼)
    meta = {
        "lr0": (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)
        "lrf": (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)
        "momentum": (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1
        "weight_decay": (1, 0.0, 0.001),  # optimizer weight decay
        "warmup_epochs": (1, 0.0, 5.0),  # warmup epochs (fractions ok)
        "warmup_momentum": (1, 0.0, 0.95),  # warmup initial momentum
        "warmup_bias_lr": (1, 0.0, 0.2),  # warmup initial bias lr
        "box": (1, 0.02, 0.2),  # box loss gain
        "cls": (1, 0.2, 4.0),  # cls loss gain
        "cls_pw": (1, 0.5, 2.0),  # cls BCELoss positive_weight
        "obj": (1, 0.2, 4.0),  # obj loss gain (scale with pixels)
        "obj_pw": (1, 0.5, 2.0),  # obj BCELoss positive_weight
        "iou_t": (0, 0.1, 0.7),  # IoU training threshold
        "anchor_t": (1, 2.0, 8.0),  # anchor-multiple threshold
        "anchors": (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)
        "fl_gamma": (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)
        "hsv_h": (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)
        "hsv_s": (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)
        "hsv_v": (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)
        "degrees": (1, 0.0, 45.0),  # image rotation (+/- deg)
        "translate": (1, 0.0, 0.9),  # image translation (+/- fraction)
        "scale": (1, 0.0, 0.9),  # image scale (+/- gain)
        "shear": (1, 0.0, 10.0),  # image shear (+/- deg)
        "perspective": (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001
        "flipud": (1, 0.0, 1.0),  # image flip up-down (probability)
        "fliplr": (0, 0.0, 1.0),  # image flip left-right (probability)
        "mosaic": (1, 0.0, 1.0),  # image mixup (probability)
        "mixup": (1, 0.0, 1.0),  # image mixup (probability)
        "copy_paste": (1, 0.0, 1.0),
    }  # segment copy-paste (probability)

    with open(opt.hyp, errors="ignore") as f: # è½½å…¥åˆå§‹è¶…å‚
        hyp = yaml.safe_load(f)  # load hyps dict
        if "anchors" not in hyp:  # anchors commented in hyp.yaml
            hyp["anchors"] = 3
    opt.noval, opt.nosave, save_dir = (
        True,
        True,
        Path(opt.save_dir),
    )  # only val/save final epoch
    # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices
    # evolve_yaml è¶…å‚è¿›åŒ–åæ–‡ä»¶ä¿å­˜åœ°å€
    evolve_yaml, evolve_csv = save_dir / "hyp_evolve.yaml", save_dir / "evolve.csv"
    if opt.bucket:
        os.system(f"gsutil cp gs://{opt.bucket}/evolve.csv {evolve_csv}")  # download evolve.csv if exists
    
    """
    ä½¿ç”¨é—ä¼ ç®—æ³•è¿›è¡Œå‚æ•°è¿›åŒ– é»˜è®¤æ˜¯è¿›åŒ–300ä»£
    è¿™é‡Œçš„è¿›åŒ–ç®—æ³•æ˜¯ï¼šæ ¹æ®ä¹‹å‰è®­ç»ƒæ—¶çš„hypæ¥ç¡®å®šä¸€ä¸ªbase hypå†è¿›è¡Œçªå˜ï¼›
    å¦‚ä½•æ ¹æ®ï¼Ÿé€šè¿‡ä¹‹å‰æ¯æ¬¡è¿›åŒ–å¾—åˆ°çš„resultsæ¥ç¡®å®šä¹‹å‰æ¯ä¸ªhypçš„æƒé‡
    æœ‰äº†æ¯ä¸ªhypå’Œæ¯ä¸ªhypçš„æƒé‡ä¹‹åæœ‰ä¸¤ç§è¿›åŒ–æ–¹å¼ï¼›
    1.æ ¹æ®æ¯ä¸ªhypçš„æƒé‡éšæœºé€‰æ‹©ä¸€ä¸ªä¹‹å‰çš„hypä½œä¸ºbase hypï¼Œrandom.choices(range(n), weights=w)
    2.æ ¹æ®æ¯ä¸ªhypçš„æƒé‡å¯¹ä¹‹å‰æ‰€æœ‰çš„hypè¿›è¡Œèåˆè·å¾—ä¸€ä¸ªbase hypï¼Œ(x * w.reshape(n, 1)).sum(0) / w.sum()
    evolve.txtä¼šè®°å½•æ¯æ¬¡è¿›åŒ–ä¹‹åçš„results+hyp
    æ¯æ¬¡è¿›åŒ–æ—¶ï¼Œhypä¼šæ ¹æ®ä¹‹å‰çš„resultsè¿›è¡Œä»å¤§åˆ°å°çš„æ’åºï¼›
    å†æ ¹æ®fitnesså‡½æ•°è®¡ç®—ä¹‹å‰æ¯æ¬¡è¿›åŒ–å¾—åˆ°çš„hypçš„æƒé‡
    å†ç¡®å®šå“ªä¸€ç§è¿›åŒ–æ–¹å¼ï¼Œä»è€Œè¿›è¡Œè¿›åŒ–
    """
    for _ in range(opt.evolve):  # generations to evolve
        if evolve_csv.exists():  # if evolve.csv exists: select best hyps and mutate
            # Select parent(s)
            parent = "single"  # parent selection method: 'single' or 'weighted'
            x = np.loadtxt(evolve_csv, ndmin=2, delimiter=",", skiprows=1)
            n = min(5, len(x))  # number of previous results to consider
            x = x[np.argsort(-fitness(x))][:n]  # top n mutations
            w = fitness(x) - fitness(x).min() + 1e-6  # weights (sum > 0)
            if parent == "single" or len(x) == 1:
                # x = x[random.randint(0, n - 1)]  # random selection
                x = x[random.choices(range(n), weights=w)[0]]  # weighted selection
            elif parent == "weighted":
                x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination

            # Mutate
            mp, s = 0.8, 0.2  # mutation probability, sigma
            npr = np.random
            npr.seed(int(time.time()))
            g = np.array([meta[k][0] for k in hyp.keys()])  # gains 0-1
            ng = len(meta)
            v = np.ones(ng)
            while all(v == 1):  # mutate until a change occurs (prevent duplicates)
                v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)
            for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)
                hyp[k] = float(x[i + 7] * v[i])  # mutate

        # Constrain to limits
        for k, v in meta.items():
            hyp[k] = max(hyp[k], v[1])  # lower limit
            hyp[k] = min(hyp[k], v[2])  # upper limit
            hyp[k] = round(hyp[k], 5)  # significant digits

        # Train mutation
        results = train(hyp.copy(), opt, device, callbacks)
        callbacks = Callbacks()
        # Write mutation results
        print_mutation(results, hyp.copy(), save_dir, opt.bucket)

    # Plot results
    plot_evolve(evolve_csv)
    LOGGER.info(f"Hyperparameter evolution finished {opt.evolve} generations\n" f"Results saved to {colorstr('bold', save_dir)}\n" f"Usage example: $ python train.py --hyp {evolve_yaml}")

```

## 4 def train(hyp, opt, device, callbacks):  



### 4.1 è½½å…¥å‚æ•°



```python
"""
:params hyp: data/hyps/hyp.scratch.yaml   hyp dictionary
:params opt: mainä¸­optå‚æ•°
:params device: å½“å‰è®¾å¤‡
"""
def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary
    (save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, bbox_iou_optim) = (
        Path(opt.save_dir),
        opt.epochs,
        opt.batch_size,
        opt.weights,
        opt.single_cls,
        opt.evolve,
        opt.data,
        opt.cfg,
        opt.resume,
        opt.noval,
        opt.nosave,
        opt.workers,
        opt.freeze,
        opt.bbox_iou_optim,
    )

```

### 4.2 åˆå§‹åŒ–å‚æ•°å’Œé…ç½®ä¿¡æ¯



```python
# ä¿å­˜æƒé‡è·¯å¾„ å¦‚runs/train/exp18/weights
w = save_dir / "weights"  # weights dir
(w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
last, best = w / "last", w / "best"
```


```python
callbacks.run("on_pretrain_routine_start")

# Directories
w = save_dir / "weights"  # weights dir
(w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
last, best = w / "last", w / "best"

# Hyperparameters è¶…å‚
if isinstance(hyp, str):
    with open(hyp, errors="ignore") as f:
        # load hyps dict  åŠ è½½è¶…å‚ä¿¡æ¯
        hyp = yaml.safe_load(f)  # load hyps dict
# æ—¥å¿—è¾“å‡ºè¶…å‚ä¿¡æ¯ hyperparameters: ...
LOGGER.info(colorstr("hyperparameters: ") + ", ".join(f"{k}={v}" for k, v in hyp.items()))
opt.hyp = hyp.copy()  # for saving hyps to checkpoints

# Save run settings
if not evolve:
    yaml_save(save_dir / "hyp.yaml", hyp)
    yaml_save(save_dir / "opt.yaml", vars(opt))

# Loggers
data_dict = None
if RANK in {-1, 0}:
    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance

    # Register actions
    for k in methods(loggers):
        callbacks.register_action(k, callback=getattr(loggers, k))

# Config
# æ˜¯å¦éœ€è¦ç”»å›¾: æ‰€æœ‰çš„labelsä¿¡æ¯ã€å‰ä¸‰æ¬¡è¿­ä»£çš„barchã€è®­ç»ƒç»“æœç­‰
plots = not evolve and not opt.noplots  # create plots
cuda = device.type != "cpu"

init_seeds(opt.seed + 1 + RANK, deterministic=True)

data_dict = data_dict or check_dataset(data)  # check if None

train_path, val_path = data_dict["train"], data_dict["val"]
# nc: number of classes  æ•°æ®é›†æœ‰å¤šå°‘ç§ç±»åˆ«
nc = 1 if single_cls else int(data_dict["nc"])  # number of classes
names = ["item"] if single_cls and len(data_dict["names"]) != 1 else data_dict["names"]  # class names
assert len(names) == nc, f"{len(names)} names found for nc={nc} dataset in {data}"  # check
# å½“å‰æ•°æ®é›†æ˜¯å¦æ˜¯cocoæ•°æ®é›†(80ä¸ªç±»åˆ«) 
is_coco = isinstance(val_path, str) and val_path.endswith("coco/val2017.txt")  # COCO dataset

```

### 4.3 model 


```python
pretrained = os.path.exists(weights)
# è½½å…¥æ¨¡å‹
if pretrained:
    # ä½¿ç”¨é¢„è®­ç»ƒ
    # ---------------------------------------------------------#
    # åŠ è½½æ¨¡å‹åŠå‚æ•°
    ckpt = flow.load(weights, map_location="cpu")  # load checkpoint to CPU to avoid CUDA memory leak
    # è¿™é‡ŒåŠ è½½æ¨¡å‹æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯é€šè¿‡opt.cfg å¦ä¸€ç§æ˜¯é€šè¿‡ckpt['model'].yaml
    # åŒºåˆ«åœ¨äºæ˜¯å¦ä½¿ç”¨resume å¦‚æœä½¿ç”¨resumeä¼šå°†opt.cfgè®¾ä¸ºç©ºï¼ŒæŒ‰ç…§ckpt['model'].yamlæ¥åˆ›å»ºæ¨¡å‹
    # è¿™ä¹Ÿå½±å“äº†ä¸‹é¢æ˜¯å¦é™¤å»anchorçš„key(ä¹Ÿå°±æ˜¯ä¸åŠ è½½anchor), å¦‚æœresumeåˆ™ä¸åŠ è½½anchor
    # åŸå› : ä¿å­˜çš„æ¨¡å‹ä¼šä¿å­˜anchorsï¼Œæœ‰æ—¶å€™ç”¨æˆ·è‡ªå®šä¹‰äº†anchorä¹‹åï¼Œå†resumeï¼Œåˆ™åŸæ¥åŸºäºcocoæ•°æ®é›†çš„anchorä¼šè‡ªå·±è¦†ç›–è‡ªå·±è®¾å®šçš„anchor
    # è¯¦æƒ…å‚è€ƒ: https://github.com/ultralytics/yolov5/issues/459
    # æ‰€ä»¥ä¸‹é¢è®¾ç½®intersect_dicts()å°±æ˜¯å¿½ç•¥exclude
    model = Model(cfg or ckpt["model"].yaml, ch=3, nc=nc, anchors=hyp.get("anchors")).to(device)  # create
    exclude = ["anchor"] if (cfg or hyp.get("anchors")) and not resume else []  # exclude keys
    csd = ckpt["model"].float().state_dict()  # checkpoint state_dict as FP32
    # ç­›é€‰å­—å…¸ä¸­çš„é”®å€¼å¯¹  æŠŠexcludeåˆ é™¤
    csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect
    # è½½å…¥æ¨¡å‹æƒé‡
    model.load_state_dict(csd, strict=False)  # load
    LOGGER.info(f"Transferred {len(csd)}/{len(model.state_dict())} items from {weights}")  # report
else:
    # ä¸ä½¿ç”¨é¢„è®­ç»ƒ
    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get("anchors")).to(device)  # create
# amp = check_amp(model)  # check AMP
amp = False

# Freeze
# å†»ç»“æƒé‡å±‚
# è¿™é‡Œåªæ˜¯ç»™äº†å†»ç»“æƒé‡å±‚çš„ä¸€ä¸ªä¾‹å­, ä½†æ˜¯ä½œè€…å¹¶ä¸å»ºè®®å†»ç»“æƒé‡å±‚, è®­ç»ƒå…¨éƒ¨å±‚å‚æ•°, å¯ä»¥å¾—åˆ°æ›´å¥½çš„æ€§èƒ½, å½“ç„¶ä¹Ÿä¼šæ›´æ…¢
freeze = [f"model.{x}." for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze
for k, v in model.named_parameters():
    v.requires_grad = True  # train all layers
    # NaN to 0 (commented for erratic training results)
    # v.register_hook(lambda x: torch.nan_to_num(x))
    if any(x in k for x in freeze):
        LOGGER.info(f"freezing {k}")
        v.requires_grad = False
```

### 4.4 Optimizer
> é€‰æ‹©ä¼˜åŒ–å™¨


```python
# Optimizer
nbs = 64  # nominal batch size
accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing
hyp["weight_decay"] *= batch_size * accumulate / nbs  # scale weight_decay
optimizer = smart_optimizer(model, opt.optimizer, hyp["lr0"], hyp["momentum"], hyp["weight_decay"])
```

### 4.5 å­¦ä¹ ç‡


```python
# Scheduler
if opt.cos_lr:
    # ä½¿ç”¨one cycle å­¦ä¹ ç‡  https://arxiv.org/pdf/1803.09820.pdf
    lf = one_cycle(1, hyp["lrf"], epochs)  # cosine 1->hyp['lrf']
else:
    # ä½¿ç”¨çº¿æ€§å­¦ä¹ ç‡
    def f(x):
        return (1 - x / epochs) * (1.0 - hyp["lrf"]) + hyp["lrf"]

    lf = f  # linear
# å®ä¾‹åŒ– scheduler
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)

```

### 4.6 EMA 
> å•å¡è®­ç»ƒ: ä½¿ç”¨EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰å¯¹æ¨¡å‹çš„å‚æ•°åšå¹³å‡, ä¸€ç§ç»™äºˆè¿‘æœŸæ•°æ®æ›´é«˜æƒé‡çš„å¹³å‡æ–¹æ³•, ä»¥æ±‚æé«˜æµ‹è¯•æŒ‡æ ‡å¹¶å¢åŠ æ¨¡å‹é²æ£’ã€‚



```python
# EMA
ema = ModelEMA(model) if RANK in {-1, 0} else None
```

### 4.7 Resume


```python
# Resume
best_fitness, start_epoch = 0.0, 0
if pretrained:
    if resume:
        best_fitness, start_epoch, epochs = smart_resume(ckpt, optimizer, ema, weights, epochs, resume)
    del ckpt, csd
```

### 4.8 SyncBatchNorm
> [SyncBatchNorm](https://start.oneflow.org/oneflow-yolo-doc/tutorials/03_chapter/quick_start.html#syncbatchnorm)å¯ä»¥æé«˜å¤šgpuè®­ç»ƒçš„å‡†ç¡®æ€§ï¼Œä½†ä¼šæ˜¾è‘—é™ä½è®­ç»ƒé€Ÿåº¦ã€‚å®ƒä»…é€‚ç”¨äºå¤šGPU DistributedDataParallel è®­ç»ƒã€‚


```python
# SyncBatchNorm
if opt.sync_bn and cuda and RANK != -1:
    model = flow.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
    LOGGER.info("Using SyncBatchNorm()")
```

### 4.9 æ•°æ®åŠ è½½


```python
# Trainloader
train_loader, dataset = create_dataloader(
    train_path,
    imgsz,
    batch_size // WORLD_SIZE,
    gs,
    single_cls,
    hyp=hyp,
    augment=True,
    cache=None if opt.cache == "val" else opt.cache,
    rect=opt.rect,
    rank=LOCAL_RANK,
    workers=workers,
    image_weights=opt.image_weights,
    quad=opt.quad,
    prefix=colorstr("train: "),
    shuffle=True,
)
labels = np.concatenate(dataset.labels, 0)
# è·å–æ ‡ç­¾ä¸­æœ€å¤§ç±»åˆ«å€¼ï¼Œä¸ç±»åˆ«æ•°ä½œæ¯”è¾ƒï¼Œå¦‚æœå°äºç±»åˆ«æ•°åˆ™è¡¨ç¤ºæœ‰é—®é¢˜
mlc = int(labels[:, 0].max())  # max label class
assert mlc < nc, f"Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}"

# Process 0
if RANK in {-1, 0}:
    val_loader = create_dataloader(
        val_path,
        imgsz,
        batch_size // WORLD_SIZE * 2,
        gs,
        single_cls,
        hyp=hyp,
        cache=None if noval else opt.cache,
        rect=True,
        rank=-1,
        workers=workers * 2,
        pad=0.5,
        prefix=colorstr("val: "),
    )[0]
    # å¦‚æœä¸ä½¿ç”¨æ–­ç‚¹ç»­è®­
    if not resume:
        if plots:
            plot_labels(labels, names, save_dir)
        # Anchors
        # è®¡ç®—é»˜è®¤é”šæ¡†anchorä¸æ•°æ®é›†æ ‡ç­¾æ¡†çš„é«˜å®½æ¯”
        # æ ‡ç­¾çš„é«˜hå®½wä¸anchorçš„é«˜h_aå®½h_bçš„æ¯”å€¼ å³h/h_a, w/w_aéƒ½è¦åœ¨(1/hyp['anchor_t'], hyp['anchor_t'])æ˜¯å¯ä»¥æ¥å—çš„
        # å¦‚æœbprå°äº98%ï¼Œåˆ™æ ¹æ®k-meanç®—æ³•èšç±»æ–°çš„é”šæ¡†
        if not opt.noautoanchor:
            check_anchors(dataset, model=model, thr=hyp["anchor_t"], imgsz=imgsz)
        model.half().float()  # pre-reduce anchor precision

    callbacks.run("on_pretrain_routine_end")
```

### 4.10 DDP mode


```python
# DDP mode
if cuda and RANK != -1:
    model = smart_DDP(model)
```

### 4.11 é™„åŠ model attributes


```python
# Model attributes
nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)
hyp["box"] *= 3 / nl  # scale to layers
hyp["cls"] *= nc / 80 * 3 / nl  # scale to classes and layers
hyp["obj"] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers
hyp["label_smoothing"] = opt.label_smoothing
model.nc = nc  # attach number of classes to model
model.hyp = hyp  # attach hyperparameters to model
# ä»è®­ç»ƒæ ·æœ¬æ ‡ç­¾å¾—åˆ°ç±»åˆ«æƒé‡ï¼ˆå’Œç±»åˆ«ä¸­çš„ç›®æ ‡æ•°å³ç±»åˆ«é¢‘ç‡æˆåæ¯”ï¼‰
model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights
model.names = names # è·å–ç±»åˆ«å
```

### 4.12 Start training


```python
# Start training
t0 = time.time()
nb = len(train_loader)  # number of batches
# è·å–çƒ­èº«è¿­ä»£çš„æ¬¡æ•°iterations  # number of warmup iterations, max(3 epochs, 1k iterations)
nw = max(round(hyp["warmup_epochs"] * nb), 100)  # number of warmup iterations, max(3 epochs, 100 iterations)
# nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training
last_opt_step = -1
# åˆå§‹åŒ–maps(æ¯ä¸ªç±»åˆ«çš„map)å’Œresults
maps = np.zeros(nc)  # mAP per class
results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)
# è®¾ç½®å­¦ä¹ ç‡è¡°å‡æ‰€è¿›è¡Œåˆ°çš„è½®æ¬¡ï¼Œå³ä½¿æ‰“æ–­è®­ç»ƒï¼Œä½¿ç”¨resumeæ¥ç€è®­ç»ƒä¹Ÿèƒ½æ­£å¸¸è¡”æ¥ä¹‹å‰çš„è®­ç»ƒè¿›è¡Œå­¦ä¹ ç‡è¡°å‡
scheduler.last_epoch = start_epoch - 1  # do not move
# scaler = flow.cuda.amp.GradScaler(enabled=amp)

stopper, _ = EarlyStopping(patience=opt.patience), False
# åˆå§‹åŒ–æŸå¤±å‡½æ•°
compute_loss = ComputeLoss(model, bbox_iou_optim=bbox_iou_optim)  # init loss class
callbacks.run("on_train_start")
# æ‰“å°æ—¥å¿—ä¿¡æ¯
LOGGER.info(
    f"Image sizes {imgsz} train, {imgsz} val\n"
    f"Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\n"
    f"Logging results to {colorstr('bold', save_dir)}\n"
    f"Starting training for {epochs} epochs..."
)

for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------
    callbacks.run("on_train_epoch_start")
    model.train()

    # Update image weights (optional, single-GPU only)
    # Update image weights (optional)  å¹¶ä¸ä¸€å®šå¥½  é»˜è®¤æ˜¯Falseçš„
    # å¦‚æœä¸ºTrue è¿›è¡Œå›¾ç‰‡é‡‡æ ·ç­–ç•¥(æŒ‰æ•°æ®é›†å„ç±»åˆ«æƒé‡é‡‡æ ·)
    if opt.image_weights:
        # æ ¹æ®å‰é¢åˆå§‹åŒ–çš„å›¾ç‰‡é‡‡æ ·æƒé‡model.class_weightsï¼ˆæ¯ä¸ªç±»åˆ«çš„æƒé‡ é¢‘ç‡é«˜çš„æƒé‡å°ï¼‰ä»¥åŠmapsé…åˆæ¯å¼ å›¾ç‰‡åŒ…å«çš„ç±»åˆ«æ•°
        # é€šè¿‡rando.choicesç”Ÿæˆå›¾ç‰‡ç´¢å¼•indicesä»è€Œè¿›è¡Œé‡‡ç”¨ ï¼ˆä½œè€…è‡ªå·±å†™çš„é‡‡æ ·ç­–ç•¥ï¼Œæ•ˆæœä¸ä¸€å®šokï¼‰
        cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights
        iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights
        dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx
    # åˆå§‹åŒ–è®­ç»ƒæ—¶æ‰“å°çš„å¹³å‡æŸå¤±ä¿¡æ¯
    mloss = flow.zeros(3, device=device)  # mean losses

    if RANK != -1:
        # DDPæ¨¡å¼æ‰“ä¹±æ•°æ®ï¼Œå¹¶ä¸”dpp.samplerçš„éšæœºé‡‡æ ·æ•°æ®æ˜¯åŸºäºepoch+seedä½œä¸ºéšæœºç§å­ï¼Œæ¯æ¬¡epochä¸åŒï¼Œéšæœºç§å­ä¸åŒ
        train_loader.sampler.set_epoch(epoch)
    
    # è¿›åº¦æ¡ï¼Œæ–¹ä¾¿å±•ç¤ºä¿¡æ¯
    pbar = enumerate(train_loader)

    LOGGER.info(("\n" + "%10s" * 7) % ("Epoch", "gpu_mem", "box", "obj", "cls", "labels", "img_size"))
    if RANK in {-1, 0}:
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(pbar, total=nb, bar_format="{l_bar}{bar:10}{r_bar}{bar:-10b}")  # progress bar
    
    # æ¢¯åº¦æ¸…é›¶
    optimizer.zero_grad()

    for i, (
        imgs,
        targets,
        paths,
        _,
    ) in pbar:  # batch -------------------------------------------------------------
        callbacks.run("on_train_batch_start")
        # ni: è®¡ç®—å½“å‰è¿­ä»£æ¬¡æ•° iteration
        ni = i + nb * epoch  # number integrated batches (since train start)
        imgs = imgs.to(device).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0

        # Warmup
        # çƒ­èº«è®­ç»ƒï¼ˆå‰nwæ¬¡è¿­ä»£ï¼‰çƒ­èº«è®­ç»ƒè¿­ä»£çš„æ¬¡æ•°iterationèŒƒå›´[1:nw]  é€‰å–è¾ƒå°çš„accumulateï¼Œå­¦ä¹ ç‡ä»¥åŠmomentum,æ…¢æ…¢çš„è®­ç»ƒ
        if ni <= nw:
            xi = [0, nw]  # x interp
            # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)
            accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
            for j, x in enumerate(optimizer.param_groups):
                # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                x["lr"] = np.interp(
                    ni,
                    xi,
                    [hyp["warmup_bias_lr"] if j == 0 else 0.0, x["initial_lr"] * lf(epoch)],
                )
                if "momentum" in x:
                    x["momentum"] = np.interp(ni, xi, [hyp["warmup_momentum"], hyp["momentum"]])

        # Multi-scale
        # Multi-scale å¤šå°ºåº¦è®­ç»ƒ   ä»[imgsz*0.5, imgsz*1.5+gs]é—´éšæœºé€‰å–ä¸€ä¸ªå°ºå¯¸(32çš„å€æ•°)ä½œä¸ºå½“å‰batchçš„å°ºå¯¸é€å…¥æ¨¡å‹å¼€å§‹è®­ç»ƒ
        # imgsz: é»˜è®¤è®­ç»ƒå°ºå¯¸   gs: æ¨¡å‹æœ€å¤§stride=32   [32 16 8]
        if opt.multi_scale:
            sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size
            sf = sz / max(imgs.shape[2:])  # scale factor
            if sf != 1:
                ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)
                # ä¸‹é‡‡æ ·
                imgs = nn.functional.interpolate(imgs, size=ns, mode="bilinear", align_corners=False)

        # Forward
        # imgs = flow.FloatTensor(np.ones([16,3,640,640])).cuda()

        pred = model(imgs)  # forward

        loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size

        if RANK != -1:
            loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode
        if opt.quad:
            loss *= 4.0

        # Backward
        # scaler.scale(loss).backward()
        # Backward  åå‘ä¼ æ’­  
        loss.backward()

        # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
        # æ¨¡å‹åå‘ä¼ æ’­accumulateæ¬¡ï¼ˆiterationsï¼‰åå†æ ¹æ®ç´¯è®¡çš„æ¢¯åº¦æ›´æ–°ä¸€æ¬¡å‚æ•°
        if ni - last_opt_step >= accumulate:
            # optimizer.step   å‚æ•°æ›´æ–°
            optimizer.step()
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            if ema:
                # å½“å‰epochè®­ç»ƒç»“æŸ  æ›´æ–°ema
                ema.update(model)
            last_opt_step = ni

        # Log
        # æ‰“å°Printä¸€äº›ä¿¡æ¯ åŒ…æ‹¬å½“å‰epochã€æ˜¾å­˜ã€æŸå¤±(boxã€objã€clsã€total)ã€å½“å‰batchçš„targetçš„æ•°é‡å’Œå›¾ç‰‡çš„sizeç­‰ä¿¡æ¯
        if RANK in {-1, 0}:
            mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses
            pbar.set_description(("%10s" * 1 + "%10.4g" * 5) % (f"{epoch}/{epochs - 1}", *mloss, targets.shape[0], imgs.shape[-1]))

        # end batch ----------------------------------------------------------------

    # Scheduler
    lr = [x["lr"] for x in optimizer.param_groups]  # for loggers
    scheduler.step()

    if RANK in {-1, 0}:
        # mAP
        callbacks.run("on_train_epoch_end", epoch=epoch)
        ema.update_attr(model, include=["yaml", "nc", "hyp", "names", "stride", "class_weights"])
        final_epoch = (epoch + 1 == epochs) or stopper.possible_stop

        if not noval or final_epoch:  # Calculate mAP
            results, maps, _ = val.run(
                data_dict,
                batch_size=batch_size // WORLD_SIZE * 2,
                imgsz=imgsz,
                half=amp,
                model=ema.ema,
                single_cls=single_cls,
                dataloader=val_loader,
                save_dir=save_dir,
                plots=False,
                callbacks=callbacks,
                compute_loss=compute_loss,
            )
        # Update best mAP
        fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]
        # stop = stopper(epoch=epoch, fitness=fi)  # early stop check
        if fi > best_fitness:
            best_fitness = fi
        log_vals = list(mloss) + list(results) + lr
        callbacks.run("on_fit_epoch_end", log_vals, epoch, best_fitness, fi)

        # Save model
        if (not nosave) or (final_epoch and not evolve):  # if save
            # æµ‹è¯•ä½¿ç”¨çš„æ˜¯emaï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ å¯¹æ¨¡å‹çš„å‚æ•°åšå¹³å‡ï¼‰çš„æ¨¡å‹              
            # results: [1] Precision æ‰€æœ‰ç±»åˆ«çš„å¹³å‡precision(æœ€å¤§f1æ—¶)
            #          [1] Recall æ‰€æœ‰ç±»åˆ«çš„å¹³å‡recall
            #          [1] map@0.5 æ‰€æœ‰ç±»åˆ«çš„å¹³å‡mAP@0.5
            #          [1] map@0.5:0.95 æ‰€æœ‰ç±»åˆ«çš„å¹³å‡mAP@0.5:0.95
            #          [1] box_loss éªŒè¯é›†å›å½’æŸå¤±, obj_loss éªŒè¯é›†ç½®ä¿¡åº¦æŸå¤±, cls_loss éªŒè¯é›†åˆ†ç±»æŸå¤±
            # maps: [80] æ‰€æœ‰ç±»åˆ«çš„mAP@0.5:0.95
            ckpt = {
                "epoch": epoch,
                "best_fitness": best_fitness,
                "model": deepcopy(de_parallel(model)).half(),
                "ema": deepcopy(ema.ema).half(),
                "updates": ema.updates,
                "optimizer": optimizer.state_dict(),
                "wandb_id": loggers.wandb.wandb_run.id if loggers.wandb else None,
                "opt": vars(opt),
                "date": datetime.now().isoformat(),
            }

            # Save last, best and delete
            model_save(ckpt, last)  # flow.save(ckpt, last)
            if best_fitness == fi:
                model_save(ckpt, best)  # flow.save(ckpt, best)

            if opt.save_period > 0 and epoch % opt.save_period == 0:
                print("is ok")
                model_save(ckpt, w / f"epoch{epoch}")  # flow.save(ckpt, w / f"epoch{epoch}")
            del ckpt
            # Write  å°†æµ‹è¯•ç»“æœå†™å…¥result.txtä¸­
            callbacks.run("on_model_save", last, epoch, final_epoch, best_fitness, fi)

    # end epoch --------------------------------------------------------------------------
# end training ---------------------------------------------------------------------------
```


```python
### 4.13 End 

æ‰“å°ä¸€äº›ä¿¡æ¯
(æ—¥å¿—: æ‰“å°è®­ç»ƒæ—¶é—´ã€plotså¯è§†åŒ–è®­ç»ƒç»“æœresults1.pngã€confusion_matrix.png ä»¥åŠ(â€˜F1â€™, â€˜PRâ€™, â€˜Pâ€™, â€˜Râ€™)æ›²çº¿å˜åŒ– ã€æ—¥å¿—ä¿¡æ¯)
+ cocoè¯„ä»·(åªåœ¨cocoæ•°æ®é›†æ‰ä¼šè¿è¡Œ) + é‡Šæ”¾æ˜¾å­˜
return 

```


```python
if RANK in {-1, 0}:
    LOGGER.info(f"\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours")
    for f in last, best:
        if f.exists():
            strip_optimizer(f)  # strip optimizers
            if f is best:
                LOGGER.info(f"\nValidating {f}...")
                results, _, _ = val.run(
                    data_dict,
                    batch_size=batch_size // WORLD_SIZE * 2,
                    imgsz=imgsz,
                    model=attempt_load(f, device).half(),
                    iou_thres=0.65 if is_coco else 0.60,  # best pycocotools results at 0.65
                    single_cls=single_cls,
                    dataloader=val_loader,
                    save_dir=save_dir,
                    save_json=is_coco,
                    verbose=True,
                    plots=plots,
                    callbacks=callbacks,
                    compute_loss=compute_loss,
                )  # val best model with plots

    callbacks.run("on_train_end", last, best, plots, epoch, results)

flow.cuda.empty_cache()
return 
```

## 4 runå‡½æ•°
> æ”¯æŒæŒ‡ä»¤æ‰§è¡Œè¿™ä¸ªè„šæœ¬   å°è£…trainæ¥å£


```python
def run(**kwargs):
    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m')
    opt = parse_opt(True)
    for k, v in kwargs.items():
        setattr(opt, k, v)
    main(opt)
    return opt
```

## Reference

- [ã€YOLOV5-5.x æºç è§£è¯»ã€‘train.py](https://blog.csdn.net/qq_38253797/article/details/119733964)

- [Github: Laughing-q/yolov5_annotations](https://github.com/Laughing-q/yolov5_annotations/blob/master/train.py)

- [CSDN Liaojiajia-2020: YOLOv5ä»£ç è¯¦è§£ï¼ˆtrain.pyéƒ¨åˆ†ï¼‰](https://blog.csdn.net/mary_0830/article/details/107076617?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162910543316780264030293%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=162910543316780264030293&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-107076617.pc_search_result_control_group&utm_term=yolov5+train.py&spm=1018.2226.3001.4187)